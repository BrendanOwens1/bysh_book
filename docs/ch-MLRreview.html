<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Broadening Your Statistical Horizons</title>
  <meta name="description" content="Test.">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="Broadening Your Statistical Horizons" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Test." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Broadening Your Statistical Horizons" />
  
  <meta name="twitter:description" content="Test." />
  

<meta name="author" content="J. Legler and P. Roback">


<meta name="date" content="2018-02-02">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="index.html">
<link rel="next" href="ch-beyondmost.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Broadening Your Statistical Horizons</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="ch-MLRreview.html"><a href="ch-MLRreview.html"><i class="fa fa-check"></i><b>1</b> Review of Multiple Linear Regression</a><ul>
<li class="chapter" data-level="1.1" data-path="ch-MLRreview.html"><a href="ch-MLRreview.html#learning-objectives"><i class="fa fa-check"></i><b>1.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="1.2" data-path="ch-MLRreview.html"><a href="ch-MLRreview.html#introduction"><i class="fa fa-check"></i><b>1.2</b> Introduction</a></li>
<li class="chapter" data-level="1.3" data-path="ch-MLRreview.html"><a href="ch-MLRreview.html#ordinary-least-squares-ols-assumptions"><i class="fa fa-check"></i><b>1.3</b> Ordinary Least Squares (OLS) Assumptions</a><ul>
<li class="chapter" data-level="1.3.1" data-path="ch-MLRreview.html"><a href="ch-MLRreview.html#cases-that-do-not-violate-the-ols-assumptions-for-inference"><i class="fa fa-check"></i><b>1.3.1</b> Cases that do not violate the OLS assumptions for inference</a></li>
<li class="chapter" data-level="1.3.2" data-path="ch-MLRreview.html"><a href="ch-MLRreview.html#cases-where-the-ols-assumptions-for-inference-are-violated"><i class="fa fa-check"></i><b>1.3.2</b> Cases where the OLS assumptions for inference are violated</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="ch-MLRreview.html"><a href="ch-MLRreview.html#cs:derby"><i class="fa fa-check"></i><b>1.4</b> Case Study: Kentucky Derby</a></li>
<li class="chapter" data-level="1.5" data-path="ch-MLRreview.html"><a href="ch-MLRreview.html#explore"><i class="fa fa-check"></i><b>1.5</b> Initial Exploratory Analyses</a><ul>
<li class="chapter" data-level="1.5.1" data-path="ch-MLRreview.html"><a href="ch-MLRreview.html#data-organization"><i class="fa fa-check"></i><b>1.5.1</b> Data Organization</a></li>
<li class="chapter" data-level="1.5.2" data-path="ch-MLRreview.html"><a href="ch-MLRreview.html#univariate-summaries"><i class="fa fa-check"></i><b>1.5.2</b> Univariate Summaries</a></li>
<li class="chapter" data-level="1.5.3" data-path="ch-MLRreview.html"><a href="ch-MLRreview.html#bivariate-summaries"><i class="fa fa-check"></i><b>1.5.3</b> Bivariate Summaries</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="ch-MLRreview.html"><a href="ch-MLRreview.html#multreg"><i class="fa fa-check"></i><b>1.6</b> Multiple linear regression modeling</a><ul>
<li class="chapter" data-level="1.6.1" data-path="ch-MLRreview.html"><a href="ch-MLRreview.html#SLRcontinuous"><i class="fa fa-check"></i><b>1.6.1</b> Simple linear regression with a continuous predictor</a></li>
<li class="chapter" data-level="1.6.2" data-path="ch-MLRreview.html"><a href="ch-MLRreview.html#simple-linear-regression-with-a-binary-predictor"><i class="fa fa-check"></i><b>1.6.2</b> Simple linear regression with a binary predictor</a></li>
<li class="chapter" data-level="1.6.3" data-path="ch-MLRreview.html"><a href="ch-MLRreview.html#multiple-linear-regression-with-two-predictors"><i class="fa fa-check"></i><b>1.6.3</b> Multiple linear regression with two predictors</a></li>
<li class="chapter" data-level="1.6.4" data-path="ch-MLRreview.html"><a href="ch-MLRreview.html#multreg-inference"><i class="fa fa-check"></i><b>1.6.4</b> Inference in multiple linear regression</a></li>
<li class="chapter" data-level="1.6.5" data-path="ch-MLRreview.html"><a href="ch-MLRreview.html#multiple-linear-regression-with-an-interaction-term"><i class="fa fa-check"></i><b>1.6.5</b> Multiple linear regression with an interaction term</a></li>
<li class="chapter" data-level="1.6.6" data-path="ch-MLRreview.html"><a href="ch-MLRreview.html#multreg_build"><i class="fa fa-check"></i><b>1.6.6</b> Building a multiple linear regression model</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="ch-MLRreview.html"><a href="ch-MLRreview.html#preview"><i class="fa fa-check"></i><b>1.7</b> Preview</a><ul>
<li class="chapter" data-level="1.7.1" data-path="ch-MLRreview.html"><a href="ch-MLRreview.html#soccer"><i class="fa fa-check"></i><b>1.7.1</b> Soccer</a></li>
<li class="chapter" data-level="1.7.2" data-path="ch-MLRreview.html"><a href="ch-MLRreview.html#elephant-mating"><i class="fa fa-check"></i><b>1.7.2</b> Elephant Mating</a></li>
<li class="chapter" data-level="1.7.3" data-path="ch-MLRreview.html"><a href="ch-MLRreview.html#parenting-and-gang-activity"><i class="fa fa-check"></i><b>1.7.3</b> Parenting and Gang Activity</a></li>
<li class="chapter" data-level="1.7.4" data-path="ch-MLRreview.html"><a href="ch-MLRreview.html#crime"><i class="fa fa-check"></i><b>1.7.4</b> Crime</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="ch-MLRreview.html"><a href="ch-MLRreview.html#exercises"><i class="fa fa-check"></i><b>1.8</b> Exercises</a><ul>
<li class="chapter" data-level="1.8.1" data-path="ch-MLRreview.html"><a href="ch-MLRreview.html#conceptual-exercises"><i class="fa fa-check"></i><b>1.8.1</b> Conceptual Exercises</a></li>
<li class="chapter" data-level="1.8.2" data-path="ch-MLRreview.html"><a href="ch-MLRreview.html#guided-exercise"><i class="fa fa-check"></i><b>1.8.2</b> Guided Exercise</a></li>
<li class="chapter" data-level="1.8.3" data-path="ch-MLRreview.html"><a href="ch-MLRreview.html#open-ended-exercises"><i class="fa fa-check"></i><b>1.8.3</b> Open-ended Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-beyondmost.html"><a href="ch-beyondmost.html"><i class="fa fa-check"></i><b>2</b> Beyond Least Squares: Using Likelihoods to Fit and Compare Models</a><ul>
<li class="chapter" data-level="2.1" data-path="ch-beyondmost.html"><a href="ch-beyondmost.html#learning-objectives-1"><i class="fa fa-check"></i><b>2.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="2.2" data-path="ch-beyondmost.html"><a href="ch-beyondmost.html#case-study-does-sex-run-in-families"><i class="fa fa-check"></i><b>2.2</b> Case Study: Does sex run in families?</a><ul>
<li class="chapter" data-level="2.2.1" data-path="ch-beyondmost.html"><a href="ch-beyondmost.html#research-questions"><i class="fa fa-check"></i><b>2.2.1</b> Research Questions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="ch-beyondmost.html"><a href="ch-beyondmost.html#model-0-sex-unconditional-model-equal-probabilities-independence"><i class="fa fa-check"></i><b>2.3</b> Model 0: Sex Unconditional Model (Equal probabilities, Independence)</a></li>
<li class="chapter" data-level="2.4" data-path="ch-beyondmost.html"><a href="ch-beyondmost.html#sex_unconditional_model"><i class="fa fa-check"></i><b>2.4</b> Model 1: Sex Unconditional Model (Any Probability, Independence) and the Principle of Maximum Likelihood</a><ul>
<li class="chapter" data-level="2.4.1" data-path="ch-beyondmost.html"><a href="ch-beyondmost.html#what-is-a-likelihood"><i class="fa fa-check"></i><b>2.4.1</b> What is a likelihood?</a></li>
<li class="chapter" data-level="2.4.2" data-path="ch-beyondmost.html"><a href="ch-beyondmost.html#findMLE.sec"><i class="fa fa-check"></i><b>2.4.2</b> Finding MLEs</a></li>
<li class="chapter" data-level="2.4.3" data-path="ch-beyondmost.html"><a href="ch-beyondmost.html#summary"><i class="fa fa-check"></i><b>2.4.3</b> Summary</a></li>
<li class="chapter" data-level="2.4.4" data-path="ch-beyondmost.html"><a href="ch-beyondmost.html#is-a-likelihood-a-probability-function-optional"><i class="fa fa-check"></i><b>2.4.4</b> Is a likelihood a probability function? (Optional)</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="ch-beyondmost.html"><a href="ch-beyondmost.html#sex_conditional.sec"><i class="fa fa-check"></i><b>2.5</b> Model 2: A Sex Conditional Model (Sex Bias) and Model Specification</a><ul>
<li class="chapter" data-level="2.5.1" data-path="ch-beyondmost.html"><a href="ch-beyondmost.html#model-specification"><i class="fa fa-check"></i><b>2.5.1</b> Model Specification</a></li>
<li class="chapter" data-level="2.5.2" data-path="ch-beyondmost.html"><a href="ch-beyondmost.html#application-to-hypothetical-data"><i class="fa fa-check"></i><b>2.5.2</b> Application to Hypothetical Data</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="ch-beyondmost.html"><a href="ch-beyondmost.html#case-study-analysis-of-the-nlsy-data"><i class="fa fa-check"></i><b>2.6</b> Case Study: Analysis of the NLSY data</a><ul>
<li class="chapter" data-level="2.6.1" data-path="ch-beyondmost.html"><a href="ch-beyondmost.html#model-building-plan"><i class="fa fa-check"></i><b>2.6.1</b> Model Building Plan</a></li>
<li class="chapter" data-level="2.6.2" data-path="ch-beyondmost.html"><a href="ch-beyondmost.html#EDA.sec"><i class="fa fa-check"></i><b>2.6.2</b> Family Composition of Boys and Girls, NLSY: Exploratory Data Analysis</a></li>
<li class="chapter" data-level="2.6.3" data-path="ch-beyondmost.html"><a href="ch-beyondmost.html#likelihood-for-the-sex-unconditional-model-the-nlsy-data"><i class="fa fa-check"></i><b>2.6.3</b> Likelihood for the Sex Unconditional Model: the NLSY data</a></li>
<li class="chapter" data-level="2.6.4" data-path="ch-beyondmost.html"><a href="ch-beyondmost.html#sex_cond_lik.sec"><i class="fa fa-check"></i><b>2.6.4</b> Likelihood for the Sex Conditional Model</a></li>
<li class="chapter" data-level="2.6.5" data-path="ch-beyondmost.html"><a href="ch-beyondmost.html#comparing-the-sex-unconditional-to-the-sex-conditional-model"><i class="fa fa-check"></i><b>2.6.5</b> Comparing the Sex Unconditional to the Sex Conditional Model</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="ch-beyondmost.html"><a href="ch-beyondmost.html#model-3-stopping-rule-model-waiting-for-a-boy"><i class="fa fa-check"></i><b>2.7</b> Model 3: Stopping Rule Model (Waiting for a boy)</a><ul>
<li class="chapter" data-level="2.7.1" data-path="ch-beyondmost.html"><a href="ch-beyondmost.html#non-nested-models"><i class="fa fa-check"></i><b>2.7.1</b> Non-nested Models</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="ch-beyondmost.html"><a href="ch-beyondmost.html#summary-of-model-building"><i class="fa fa-check"></i><b>2.8</b> Summary of Model Building</a></li>
<li class="chapter" data-level="2.9" data-path="ch-beyondmost.html"><a href="ch-beyondmost.html#likelihood-based-methods"><i class="fa fa-check"></i><b>2.9</b> Likelihood-based Methods</a></li>
<li class="chapter" data-level="2.10" data-path="ch-beyondmost.html"><a href="ch-beyondmost.html#likelihoods-and-this-course"><i class="fa fa-check"></i><b>2.10</b> Likelihoods and this Course</a></li>
<li class="chapter" data-level="2.11" data-path="ch-beyondmost.html"><a href="ch-beyondmost.html#exercises-1"><i class="fa fa-check"></i><b>2.11</b> Exercises</a><ul>
<li class="chapter" data-level="2.11.1" data-path="ch-beyondmost.html"><a href="ch-beyondmost.html#conceptual-exercises-1"><i class="fa fa-check"></i><b>2.11.1</b> Conceptual Exercises</a></li>
<li class="chapter" data-level="2.11.2" data-path="ch-beyondmost.html"><a href="ch-beyondmost.html#guided-exercise-1"><i class="fa fa-check"></i><b>2.11.2</b> Guided Exercise</a></li>
<li class="chapter" data-level="2.11.3" data-path="ch-beyondmost.html"><a href="ch-beyondmost.html#open-ended-exercise"><i class="fa fa-check"></i><b>2.11.3</b> Open-ended Exercise</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ch-distthry.html"><a href="ch-distthry.html"><i class="fa fa-check"></i><b>3</b> Distribution Theory</a><ul>
<li class="chapter" data-level="3.1" data-path="ch-distthry.html"><a href="ch-distthry.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a><ul>
<li class="chapter" data-level="3.1.1" data-path="ch-distthry.html"><a href="ch-distthry.html#characteristics-of-random-variables"><i class="fa fa-check"></i><b>3.1.1</b> Characteristics of Random Variables</a></li>
<li class="chapter" data-level="3.1.2" data-path="ch-distthry.html"><a href="ch-distthry.html#location-scale-and-shape-parameters"><i class="fa fa-check"></i><b>3.1.2</b> Location, scale and shape parameters</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="ch-distthry.html"><a href="ch-distthry.html#modeling-responses"><i class="fa fa-check"></i><b>3.2</b> Modeling Responses</a><ul>
<li class="chapter" data-level="3.2.1" data-path="ch-distthry.html"><a href="ch-distthry.html#discrete-random-variables"><i class="fa fa-check"></i><b>3.2.1</b> Discrete Random Variables</a></li>
<li class="chapter" data-level="3.2.2" data-path="ch-distthry.html"><a href="ch-distthry.html#bernoulli-process"><i class="fa fa-check"></i><b>3.2.2</b> Bernoulli Process</a></li>
<li class="chapter" data-level="3.2.3" data-path="ch-distthry.html"><a href="ch-distthry.html#poisson-process"><i class="fa fa-check"></i><b>3.2.3</b> Poisson Process</a></li>
<li class="chapter" data-level="3.2.4" data-path="ch-distthry.html"><a href="ch-distthry.html#continuous-random-variables"><i class="fa fa-check"></i><b>3.2.4</b> Continuous Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ch-distthry.html"><a href="ch-distthry.html#distributions-used-in-testing"><i class="fa fa-check"></i><b>3.3</b> Distributions used in Testing</a><ul>
<li class="chapter" data-level="3.3.1" data-path="ch-distthry.html"><a href="ch-distthry.html#chi2"><i class="fa fa-check"></i><b>3.3.1</b> <span class="math inline">\(\chi^2\)</span></a></li>
<li class="chapter" data-level="3.3.2" data-path="ch-distthry.html"><a href="ch-distthry.html#f"><i class="fa fa-check"></i><b>3.3.2</b> F</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="ch-distthry.html"><a href="ch-distthry.html#mixtures"><i class="fa fa-check"></i><b>3.4</b> Mixtures</a><ul>
<li class="chapter" data-level="3.4.1" data-path="ch-distthry.html"><a href="ch-distthry.html#zero-inflated-poisson"><i class="fa fa-check"></i><b>3.4.1</b> Zero-inflated Poisson</a></li>
<li class="chapter" data-level="3.4.2" data-path="ch-distthry.html"><a href="ch-distthry.html#mixture-of-two-normal-distributions"><i class="fa fa-check"></i><b>3.4.2</b> Mixture of Two Normal Distributions</a></li>
<li class="chapter" data-level="3.4.3" data-path="ch-distthry.html"><a href="ch-distthry.html#beta-binomial"><i class="fa fa-check"></i><b>3.4.3</b> Beta-Binomial</a></li>
<li class="chapter" data-level="3.4.4" data-path="ch-distthry.html"><a href="ch-distthry.html#negative-binomial-1"><i class="fa fa-check"></i><b>3.4.4</b> Negative Binomial</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ch-distthry.html"><a href="ch-distthry.html#exercises-2"><i class="fa fa-check"></i><b>3.5</b> Exercises</a><ul>
<li class="chapter" data-level="3.5.1" data-path="ch-distthry.html"><a href="ch-distthry.html#conceptual-exercises-2"><i class="fa fa-check"></i><b>3.5.1</b> Conceptual Exercises</a></li>
<li class="chapter" data-level="3.5.2" data-path="ch-distthry.html"><a href="ch-distthry.html#guided-exercises"><i class="fa fa-check"></i><b>3.5.2</b> Guided Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html"><i class="fa fa-check"></i><b>4</b> Poisson Regression</a><ul>
<li class="chapter" data-level="4.1" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#learning-objectives-2"><i class="fa fa-check"></i><b>4.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="4.2" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#preface-1"><i class="fa fa-check"></i><b>4.2</b> Preface</a></li>
<li class="chapter" data-level="4.3" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#poisintrosec"><i class="fa fa-check"></i><b>4.3</b> Introduction to Poisson Regression</a><ul>
<li class="chapter" data-level="4.3.1" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#initial-examples"><i class="fa fa-check"></i><b>4.3.1</b> Initial Examples</a></li>
<li class="chapter" data-level="4.3.2" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#sec-Prop"><i class="fa fa-check"></i><b>4.3.2</b> Poisson Random Variables</a></li>
<li class="chapter" data-level="4.3.3" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#modeling-with-poisson-variables"><i class="fa fa-check"></i><b>4.3.3</b> Modeling with Poisson variables</a></li>
<li class="chapter" data-level="4.3.4" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#a-graphical-look-at-poisson-regression"><i class="fa fa-check"></i><b>4.3.4</b> A Graphical Look at Poisson Regression</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#case-studies-overview"><i class="fa fa-check"></i><b>4.4</b> Case Studies Overview</a></li>
<li class="chapter" data-level="4.5" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#case-study-household-size-in-haiti"><i class="fa fa-check"></i><b>4.5</b> Case Study: Household Size in Haiti</a><ul>
<li class="chapter" data-level="4.5.1" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#research-question"><i class="fa fa-check"></i><b>4.5.1</b> Research Question</a></li>
<li class="chapter" data-level="4.5.2" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#data-collection"><i class="fa fa-check"></i><b>4.5.2</b> Data Collection</a></li>
<li class="chapter" data-level="4.5.3" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#data-organization-1"><i class="fa fa-check"></i><b>4.5.3</b> Data Organization</a></li>
<li class="chapter" data-level="4.5.4" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>4.5.4</b> Exploratory Data Analysis</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#likelihood.sec"><i class="fa fa-check"></i><b>4.6</b> Using Likelihoods to fit Poisson Regression Models (Optional)</a></li>
<li class="chapter" data-level="4.7" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#modeling"><i class="fa fa-check"></i><b>4.7</b> Modeling</a><ul>
<li class="chapter" data-level="4.7.1" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#first-order-model"><i class="fa fa-check"></i><b>4.7.1</b> First Order Model</a></li>
<li class="chapter" data-level="4.7.2" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#sec-PoisInference"><i class="fa fa-check"></i><b>4.7.2</b> Estimation and Inference</a></li>
<li class="chapter" data-level="4.7.3" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#using-deviances-to-compare-models"><i class="fa fa-check"></i><b>4.7.3</b> Using Deviances to Compare Models</a></li>
<li class="chapter" data-level="4.7.4" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#poisson-regression-assumptions-1"><i class="fa fa-check"></i><b>4.7.4</b> Poisson Regression Assumptions</a></li>
<li class="chapter" data-level="4.7.5" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#residual-plot"><i class="fa fa-check"></i><b>4.7.5</b> Residual Plot</a></li>
<li class="chapter" data-level="4.7.6" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#residuals-for-poisson-models-optional"><i class="fa fa-check"></i><b>4.7.6</b> Residuals for Poisson Models (Optional)</a></li>
<li class="chapter" data-level="4.7.7" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#second-order-model"><i class="fa fa-check"></i><b>4.7.7</b> Second Order Model</a></li>
<li class="chapter" data-level="4.7.8" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#finding-the-age-where-the-number-in-the-house-is-a-maximum"><i class="fa fa-check"></i><b>4.7.8</b> Finding the age where the number in the house is a maximum</a></li>
<li class="chapter" data-level="4.7.9" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#adding-a-covariate"><i class="fa fa-check"></i><b>4.7.9</b> Adding a covariate</a></li>
<li class="chapter" data-level="4.7.10" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#sec-PoisGOF"><i class="fa fa-check"></i><b>4.7.10</b> Goodness-of-fit</a></li>
<li class="chapter" data-level="4.7.11" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#least-squares-regression-vs.poisson-regression"><i class="fa fa-check"></i><b>4.7.11</b> Least Squares Regression vs. Poisson Regression</a></li>
<li class="chapter" data-level="4.7.12" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#optional-topics-to-be-developed"><i class="fa fa-check"></i><b>4.7.12</b> Optional topics to be developed</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#case-study-campus-crime"><i class="fa fa-check"></i><b>4.8</b> Case Study: Campus Crime</a></li>
<li class="chapter" data-level="4.9" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#analysis-for-crime-data"><i class="fa fa-check"></i><b>4.9</b> Analysis for crime data</a><ul>
<li class="chapter" data-level="4.9.1" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#research-question-1"><i class="fa fa-check"></i><b>4.9.1</b> Research Question</a></li>
<li class="chapter" data-level="4.9.2" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#data-organization-2"><i class="fa fa-check"></i><b>4.9.2</b> Data Organization</a></li>
<li class="chapter" data-level="4.9.3" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#exploratory-data-analysis-1"><i class="fa fa-check"></i><b>4.9.3</b> Exploratory Data Analysis</a></li>
<li class="chapter" data-level="4.9.4" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#accounting-for-enrollment"><i class="fa fa-check"></i><b>4.9.4</b> Accounting for Enrollment</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#checking-assumptions"><i class="fa fa-check"></i><b>4.10</b> Checking Assumptions</a><ul>
<li class="chapter" data-level="4.10.1" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#is-the-mean-equal-to-the-variance"><i class="fa fa-check"></i><b>4.10.1</b> Is the Mean equal to the Variance?</a></li>
</ul></li>
<li class="chapter" data-level="4.11" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#modeling-1"><i class="fa fa-check"></i><b>4.11</b> Modeling</a></li>
<li class="chapter" data-level="4.12" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#sec-overdispPois"><i class="fa fa-check"></i><b>4.12</b> Overdispersion</a><ul>
<li class="chapter" data-level="4.12.1" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#dispersion-parameter-adjustment"><i class="fa fa-check"></i><b>4.12.1</b> Dispersion parameter adjustment</a></li>
<li class="chapter" data-level="4.12.2" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#negative-binomial-modeling"><i class="fa fa-check"></i><b>4.12.2</b> Negative binomial modeling</a></li>
</ul></li>
<li class="chapter" data-level="4.13" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#case-study-weekend-drinking-csdrinking"><i class="fa fa-check"></i><b>4.13</b> Case Study: Weekend drinking {cs:drinking}</a><ul>
<li class="chapter" data-level="4.13.1" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#research-question-2"><i class="fa fa-check"></i><b>4.13.1</b> Research Question</a></li>
<li class="chapter" data-level="4.13.2" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#data-organization-3"><i class="fa fa-check"></i><b>4.13.2</b> Data Organization</a></li>
<li class="chapter" data-level="4.13.3" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#exploratory-data-analysis-2"><i class="fa fa-check"></i><b>4.13.3</b> Exploratory Data Analysis</a></li>
<li class="chapter" data-level="4.13.4" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#modeling-2"><i class="fa fa-check"></i><b>4.13.4</b> Modeling</a></li>
<li class="chapter" data-level="4.13.5" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#fitting-a-zip-model"><i class="fa fa-check"></i><b>4.13.5</b> Fitting a ZIP Model</a></li>
<li class="chapter" data-level="4.13.6" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#comparing-the-ordinary-poisson-regression-model-to-the-zip-model"><i class="fa fa-check"></i><b>4.13.6</b> Comparing the ordinary Poisson regression model to the ZIP model</a></li>
<li class="chapter" data-level="4.13.7" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#residual-plot-1"><i class="fa fa-check"></i><b>4.13.7</b> Residual Plot</a></li>
<li class="chapter" data-level="4.13.8" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#caveats-and-extensions"><i class="fa fa-check"></i><b>4.13.8</b> Caveats and Extensions</a></li>
</ul></li>
<li class="chapter" data-level="4.14" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#references"><i class="fa fa-check"></i><b>4.14</b> References</a></li>
<li class="chapter" data-level="4.15" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#exercises-3"><i class="fa fa-check"></i><b>4.15</b> Exercises</a><ul>
<li class="chapter" data-level="4.15.1" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#exer:concept"><i class="fa fa-check"></i><b>4.15.1</b> Conceptual Exercises</a></li>
<li class="chapter" data-level="4.15.2" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#guided-exercise-2"><i class="fa fa-check"></i><b>4.15.2</b> Guided Exercise</a></li>
<li class="chapter" data-level="4.15.3" data-path="ch-poissonreg.html"><a href="ch-poissonreg.html#open-ended"><i class="fa fa-check"></i><b>4.15.3</b> Open-ended</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-glms.html"><a href="ch-glms.html"><i class="fa fa-check"></i><b>5</b> Generalized Linear Models (GLMs): A Unifying Theory</a><ul>
<li class="chapter" data-level="5.1" data-path="ch-glms.html"><a href="ch-glms.html#learning-objectives-3"><i class="fa fa-check"></i><b>5.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="5.2" data-path="ch-glms.html"><a href="ch-glms.html#one-parameter-exponential-families"><i class="fa fa-check"></i><b>5.2</b> One parameter exponential families</a><ul>
<li class="chapter" data-level="5.2.1" data-path="ch-glms.html"><a href="ch-glms.html#one-parameter-exponential-family-possion"><i class="fa fa-check"></i><b>5.2.1</b> One Parameter Exponential Family: Possion</a></li>
<li class="chapter" data-level="5.2.2" data-path="ch-glms.html"><a href="ch-glms.html#one-parameter-exponential-family-normal"><i class="fa fa-check"></i><b>5.2.2</b> One parameter exponential family: Normal</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ch-glms.html"><a href="ch-glms.html#generalized-linear-modeling"><i class="fa fa-check"></i><b>5.3</b> Generalized Linear Modeling</a></li>
<li class="chapter" data-level="5.4" data-path="ch-glms.html"><a href="ch-glms.html#exercises-4"><i class="fa fa-check"></i><b>5.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-logreg.html"><a href="ch-logreg.html"><i class="fa fa-check"></i><b>6</b> Logistic Regression</a><ul>
<li class="chapter" data-level="6.1" data-path="ch-logreg.html"><a href="ch-logreg.html#learning-objectives-4"><i class="fa fa-check"></i><b>6.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="6.2" data-path="ch-logreg.html"><a href="ch-logreg.html#introduction-2"><i class="fa fa-check"></i><b>6.2</b> Introduction</a><ul>
<li class="chapter" data-level="6.2.1" data-path="ch-logreg.html"><a href="ch-logreg.html#binary-responses"><i class="fa fa-check"></i><b>6.2.1</b> Binary Responses</a></li>
<li class="chapter" data-level="6.2.2" data-path="ch-logreg.html"><a href="ch-logreg.html#binomial-responses-sums-of-binary-responses"><i class="fa fa-check"></i><b>6.2.2</b> Binomial Responses: sums of binary responses</a></li>
<li class="chapter" data-level="6.2.3" data-path="ch-logreg.html"><a href="ch-logreg.html#an-example-binge-drinking"><i class="fa fa-check"></i><b>6.2.3</b> An Example: Binge Drinking</a></li>
<li class="chapter" data-level="6.2.4" data-path="ch-logreg.html"><a href="ch-logreg.html#a-graphical-look-at-binomial-regression"><i class="fa fa-check"></i><b>6.2.4</b> A Graphical Look at Binomial Regression</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="ch-logreg.html"><a href="ch-logreg.html#sec-modelframework"><i class="fa fa-check"></i><b>6.3</b> A Modeling Framework</a></li>
<li class="chapter" data-level="6.4" data-path="ch-logreg.html"><a href="ch-logreg.html#case-studies-overview-1"><i class="fa fa-check"></i><b>6.4</b> Case Studies Overview</a></li>
<li class="chapter" data-level="6.5" data-path="ch-logreg.html"><a href="ch-logreg.html#glm-theory-for-binomial-outcomes"><i class="fa fa-check"></i><b>6.5</b> GLM Theory for Binomial Outcomes</a><ul>
<li class="chapter" data-level="6.5.1" data-path="ch-logreg.html"><a href="ch-logreg.html#case-study-soccer-goalkeeper-saves"><i class="fa fa-check"></i><b>6.5.1</b> Case Study: Soccer Goalkeeper Saves</a></li>
<li class="chapter" data-level="6.5.2" data-path="ch-logreg.html"><a href="ch-logreg.html#research-question-3"><i class="fa fa-check"></i><b>6.5.2</b> Research Question</a></li>
<li class="chapter" data-level="6.5.3" data-path="ch-logreg.html"><a href="ch-logreg.html#data-organization-4"><i class="fa fa-check"></i><b>6.5.3</b> Data Organization</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="ch-logreg.html"><a href="ch-logreg.html#case-study-reconstructing-alabama"><i class="fa fa-check"></i><b>6.6</b> Case Study: Reconstructing Alabama</a><ul>
<li class="chapter" data-level="6.6.1" data-path="ch-logreg.html"><a href="ch-logreg.html#research-question-4"><i class="fa fa-check"></i><b>6.6.1</b> Research Question</a></li>
<li class="chapter" data-level="6.6.2" data-path="ch-logreg.html"><a href="ch-logreg.html#data-organization-5"><i class="fa fa-check"></i><b>6.6.2</b> Data Organization</a></li>
<li class="chapter" data-level="6.6.3" data-path="ch-logreg.html"><a href="ch-logreg.html#exploratory-data-analysis-3"><i class="fa fa-check"></i><b>6.6.3</b> Exploratory Data Analysis</a></li>
<li class="chapter" data-level="6.6.4" data-path="ch-logreg.html"><a href="ch-logreg.html#modeling-overview"><i class="fa fa-check"></i><b>6.6.4</b> Modeling Overview</a></li>
<li class="chapter" data-level="6.6.5" data-path="ch-logreg.html"><a href="ch-logreg.html#results"><i class="fa fa-check"></i><b>6.6.5</b> Results</a></li>
<li class="chapter" data-level="6.6.6" data-path="ch-logreg.html"><a href="ch-logreg.html#least-squares-regression-vs.logistic-regression"><i class="fa fa-check"></i><b>6.6.6</b> Least Squares Regression vs. Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="ch-logreg.html"><a href="ch-logreg.html#case-study-who-wants-to-lose-weight-sex-media-sports-and-bmi."><i class="fa fa-check"></i><b>6.7</b> Case Study: Who wants to lose weight? Sex, Media, Sports, and BMI.</a><ul>
<li class="chapter" data-level="6.7.1" data-path="ch-logreg.html"><a href="ch-logreg.html#background"><i class="fa fa-check"></i><b>6.7.1</b> Background</a></li>
<li class="chapter" data-level="6.7.2" data-path="ch-logreg.html"><a href="ch-logreg.html#research-questions-1"><i class="fa fa-check"></i><b>6.7.2</b> Research Questions</a></li>
<li class="chapter" data-level="6.7.3" data-path="ch-logreg.html"><a href="ch-logreg.html#data-collection-1"><i class="fa fa-check"></i><b>6.7.3</b> Data Collection</a></li>
<li class="chapter" data-level="6.7.4" data-path="ch-logreg.html"><a href="ch-logreg.html#data-organization-6"><i class="fa fa-check"></i><b>6.7.4</b> Data Organization</a></li>
<li class="chapter" data-level="6.7.5" data-path="ch-logreg.html"><a href="ch-logreg.html#exploratory-data-analysis-4"><i class="fa fa-check"></i><b>6.7.5</b> Exploratory Data Analysis</a></li>
<li class="chapter" data-level="6.7.6" data-path="ch-logreg.html"><a href="ch-logreg.html#modeling-3"><i class="fa fa-check"></i><b>6.7.6</b> Modeling</a></li>
<li class="chapter" data-level="6.7.7" data-path="ch-logreg.html"><a href="ch-logreg.html#discussion"><i class="fa fa-check"></i><b>6.7.7</b> Discussion</a></li>
<li class="chapter" data-level="6.7.8" data-path="ch-logreg.html"><a href="ch-logreg.html#optional-topics-to-be-developed-1"><i class="fa fa-check"></i><b>6.7.8</b> Optional topics to be developed</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="ch-logreg.html"><a href="ch-logreg.html#references-1"><i class="fa fa-check"></i><b>6.8</b> References</a></li>
<li class="chapter" data-level="6.9" data-path="ch-logreg.html"><a href="ch-logreg.html#exercises-5"><i class="fa fa-check"></i><b>6.9</b> Exercises</a><ul>
<li class="chapter" data-level="6.9.1" data-path="ch-logreg.html"><a href="ch-logreg.html#interpret-article-abstracts"><i class="fa fa-check"></i><b>6.9.1</b> Interpret article abstracts</a></li>
<li class="chapter" data-level="6.9.2" data-path="ch-logreg.html"><a href="ch-logreg.html#guided-exercises-1"><i class="fa fa-check"></i><b>6.9.2</b> Guided Exercises</a></li>
<li class="chapter" data-level="6.9.3" data-path="ch-logreg.html"><a href="ch-logreg.html#open-ended-exercises-1"><i class="fa fa-check"></i><b>6.9.3</b> Open-ended Exercises</a></li>
<li class="chapter" data-level="6.9.4" data-path="ch-logreg.html"><a href="ch-logreg.html#project-ideas"><i class="fa fa-check"></i><b>6.9.4</b> Project Ideas</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-corrdata.html"><a href="ch-corrdata.html"><i class="fa fa-check"></i><b>7</b> Correlated Data</a><ul>
<li class="chapter" data-level="7.1" data-path="ch-corrdata.html"><a href="ch-corrdata.html#learning-objectives-5"><i class="fa fa-check"></i><b>7.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="7.2" data-path="ch-corrdata.html"><a href="ch-corrdata.html#recognizing-correlation"><i class="fa fa-check"></i><b>7.2</b> Recognizing correlation</a></li>
<li class="chapter" data-level="7.3" data-path="ch-corrdata.html"><a href="ch-corrdata.html#case-study-dams-and-pups-correlated-binary-outcomes"><i class="fa fa-check"></i><b>7.3</b> Case Study: Dams and pups, Correlated Binary Outcomes</a><ul>
<li class="chapter" data-level="7.3.1" data-path="ch-corrdata.html"><a href="ch-corrdata.html#sources-of-variability"><i class="fa fa-check"></i><b>7.3.1</b> Sources of Variability</a></li>
<li class="chapter" data-level="7.3.2" data-path="ch-corrdata.html"><a href="ch-corrdata.html#analyzing-a-control-group"><i class="fa fa-check"></i><b>7.3.2</b> Analyzing a control group</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="ch-corrdata.html"><a href="ch-corrdata.html#under-construction"><i class="fa fa-check"></i><b>7.4</b> Under Construction…</a><ul>
<li class="chapter" data-level="7.4.1" data-path="ch-corrdata.html"><a href="ch-corrdata.html#correlated-data-simulation"><i class="fa fa-check"></i><b>7.4.1</b> Correlated Data Simulation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ch-multilevelintro.html"><a href="ch-multilevelintro.html"><i class="fa fa-check"></i><b>8</b> Introduction to Multilevel Models</a><ul>
<li class="chapter" data-level="8.1" data-path="ch-multilevelintro.html"><a href="ch-multilevelintro.html#learning-objectives-6"><i class="fa fa-check"></i><b>8.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="8.2" data-path="ch-multilevelintro.html"><a href="ch-multilevelintro.html#cs:music"><i class="fa fa-check"></i><b>8.2</b> Case Study: Music Performance Anxiety</a></li>
<li class="chapter" data-level="8.3" data-path="ch-MLRreview.html"><a href="ch-MLRreview.html#explore"><i class="fa fa-check"></i><b>8.3</b> Initial Exploratory Analyses</a><ul>
<li class="chapter" data-level="8.3.1" data-path="ch-multilevelintro.html"><a href="ch-multilevelintro.html#organizedata1"><i class="fa fa-check"></i><b>8.3.1</b> Data Organization</a></li>
<li class="chapter" data-level="8.3.2" data-path="ch-multilevelintro.html"><a href="ch-multilevelintro.html#explore1"><i class="fa fa-check"></i><b>8.3.2</b> Exploratory Analyses: Univariate Summaries</a></li>
<li class="chapter" data-level="8.3.3" data-path="ch-multilevelintro.html"><a href="ch-multilevelintro.html#explore2"><i class="fa fa-check"></i><b>8.3.3</b> Exploratory Analyses: Bivariate Summaries</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="ch-multilevelintro.html"><a href="ch-multilevelintro.html#twolevelmodeling"><i class="fa fa-check"></i><b>8.4</b> Two level modeling: preliminary considerations</a><ul>
<li class="chapter" data-level="8.4.1" data-path="ch-multilevelintro.html"><a href="ch-multilevelintro.html#multregr"><i class="fa fa-check"></i><b>8.4.1</b> Ignoring the two level structure (not recommended)</a></li>
<li class="chapter" data-level="8.4.2" data-path="ch-multilevelintro.html"><a href="ch-multilevelintro.html#twostage"><i class="fa fa-check"></i><b>8.4.2</b> A two-stage modeling approach (better but imperfect)</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="ch-multilevelintro.html"><a href="ch-multilevelintro.html#twolevelmodelingunified"><i class="fa fa-check"></i><b>8.5</b> Two level modeling: a unified approach</a><ul>
<li class="chapter" data-level="8.5.1" data-path="ch-multilevelintro.html"><a href="ch-multilevelintro.html#ourframework"><i class="fa fa-check"></i><b>8.5.1</b> Our framework</a></li>
<li class="chapter" data-level="8.5.2" data-path="ch-multilevelintro.html"><a href="ch-multilevelintro.html#random-vs.fixed-effects"><i class="fa fa-check"></i><b>8.5.2</b> Random vs. fixed effects</a></li>
<li class="chapter" data-level="8.5.3" data-path="ch-multilevelintro.html"><a href="ch-multilevelintro.html#MVN"><i class="fa fa-check"></i><b>8.5.3</b> Distribution of errors: the multivariate normal distribution</a></li>
<li class="chapter" data-level="8.5.4" data-path="ch-multilevelintro.html"><a href="ch-multilevelintro.html#multileveltechnical"><i class="fa fa-check"></i><b>8.5.4</b> Technical issues when estimating and testing parameters (Optional)</a></li>
<li class="chapter" data-level="8.5.5" data-path="ch-multilevelintro.html"><a href="ch-multilevelintro.html#initialmodel"><i class="fa fa-check"></i><b>8.5.5</b> An initial model with parameter interpretations</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="ch-multilevelintro.html"><a href="ch-multilevelintro.html#sec:buildmodel"><i class="fa fa-check"></i><b>8.6</b> Building a multilevel model</a><ul>
<li class="chapter" data-level="8.6.1" data-path="ch-multilevelintro.html"><a href="ch-multilevelintro.html#buildstrategy"><i class="fa fa-check"></i><b>8.6.1</b> Model building strategy</a></li>
<li class="chapter" data-level="8.6.2" data-path="ch-multilevelintro.html"><a href="ch-multilevelintro.html#modela"><i class="fa fa-check"></i><b>8.6.2</b> An initial model: unconditional means or random intercepts</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="ch-multilevelintro.html"><a href="ch-multilevelintro.html#modelb"><i class="fa fa-check"></i><b>8.7</b> Binary covariates at Level One and Level Two</a><ul>
<li class="chapter" data-level="8.7.1" data-path="ch-multilevelintro.html"><a href="ch-multilevelintro.html#randomslopeandint"><i class="fa fa-check"></i><b>8.7.1</b> Random slopes and intercepts model</a></li>
<li class="chapter" data-level="8.7.2" data-path="ch-multilevelintro.html"><a href="ch-multilevelintro.html#pseudoR2"><i class="fa fa-check"></i><b>8.7.2</b> Pseudo <span class="math inline">\(R^2\)</span> values</a></li>
<li class="chapter" data-level="8.7.3" data-path="ch-multilevelintro.html"><a href="ch-multilevelintro.html#modelc"><i class="fa fa-check"></i><b>8.7.3</b> Adding a covariate at Level Two</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="ch-multilevelintro.html"><a href="ch-multilevelintro.html#sec:modeld"><i class="fa fa-check"></i><b>8.8</b> Additional covariates: model comparison and interpretability</a><ul>
<li class="chapter" data-level="8.8.1" data-path="ch-multilevelintro.html"><a href="ch-multilevelintro.html#interp:modeld"><i class="fa fa-check"></i><b>8.8.1</b> Interpretation of parameter estimates</a></li>
<li class="chapter" data-level="8.8.2" data-path="ch-multilevelintro.html"><a href="ch-multilevelintro.html#compare:modeld"><i class="fa fa-check"></i><b>8.8.2</b> Model comparisons</a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="ch-multilevelintro.html"><a href="ch-multilevelintro.html#sec:modele"><i class="fa fa-check"></i><b>8.9</b> Center covariates</a></li>
<li class="chapter" data-level="8.10" data-path="ch-multilevelintro.html"><a href="ch-multilevelintro.html#modelf"><i class="fa fa-check"></i><b>8.10</b> A potential final model for music performance anxiety</a></li>
<li class="chapter" data-level="8.11" data-path="ch-multilevelintro.html"><a href="ch-multilevelintro.html#multinecessary"><i class="fa fa-check"></i><b>8.11</b> Modeling the multilevel structure: is it really necessary?</a></li>
<li class="chapter" data-level="8.12" data-path="ch-multilevelintro.html"><a href="ch-multilevelintro.html#notesr8"><i class="fa fa-check"></i><b>8.12</b> Notes on Using R (Optional)</a></li>
<li class="chapter" data-level="8.13" data-path="ch-multilevelintro.html"><a href="ch-multilevelintro.html#exercises-6"><i class="fa fa-check"></i><b>8.13</b> Exercises</a><ul>
<li class="chapter" data-level="8.13.1" data-path="ch-multilevelintro.html"><a href="ch-multilevelintro.html#conceptual-exercises-3"><i class="fa fa-check"></i><b>8.13.1</b> Conceptual Exercises</a></li>
<li class="chapter" data-level="8.13.2" data-path="ch-multilevelintro.html"><a href="ch-multilevelintro.html#guided-exercise-3"><i class="fa fa-check"></i><b>8.13.2</b> Guided Exercise</a></li>
<li class="chapter" data-level="8.13.3" data-path="ch-multilevelintro.html"><a href="ch-multilevelintro.html#open-ended-exercises-2"><i class="fa fa-check"></i><b>8.13.3</b> Open-ended Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ch-lon.html"><a href="ch-lon.html"><i class="fa fa-check"></i><b>9</b> Two Level Longitudinal Data</a><ul>
<li class="chapter" data-level="9.1" data-path="ch-lon.html"><a href="ch-lon.html#learning-objectives-7"><i class="fa fa-check"></i><b>9.1</b> Learning objectives</a></li>
<li class="chapter" data-level="9.2" data-path="ch-lon.html"><a href="ch-lon.html#cs:charter"><i class="fa fa-check"></i><b>9.2</b> Case study: Charter schools</a></li>
<li class="chapter" data-level="9.3" data-path="ch-lon.html"><a href="ch-lon.html#exploratoryanalysis"><i class="fa fa-check"></i><b>9.3</b> Initial Exploratory Analyses</a><ul>
<li class="chapter" data-level="9.3.1" data-path="ch-lon.html"><a href="ch-lon.html#data"><i class="fa fa-check"></i><b>9.3.1</b> Data organization</a></li>
<li class="chapter" data-level="9.3.2" data-path="ch-lon.html"><a href="ch-lon.html#missing"><i class="fa fa-check"></i><b>9.3.2</b> Missing data</a></li>
<li class="chapter" data-level="9.3.3" data-path="ch-lon.html"><a href="ch-lon.html#generalanalyses"><i class="fa fa-check"></i><b>9.3.3</b> Exploratory analyses for general multilevel models</a></li>
<li class="chapter" data-level="9.3.4" data-path="ch-lon.html"><a href="ch-lon.html#longitudinalanalyses"><i class="fa fa-check"></i><b>9.3.4</b> Exploratory analyses for longitudinal data</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="ch-lon.html"><a href="ch-lon.html#twostage9"><i class="fa fa-check"></i><b>9.4</b> Preliminary two-stage modeling</a><ul>
<li class="chapter" data-level="9.4.1" data-path="ch-lon.html"><a href="ch-lon.html#lineartwostage"><i class="fa fa-check"></i><b>9.4.1</b> Linear trends within schools</a></li>
<li class="chapter" data-level="9.4.2" data-path="ch-lon.html"><a href="ch-lon.html#lineartwostageL2effects"><i class="fa fa-check"></i><b>9.4.2</b> Effects of level two covariates on linear time trends</a></li>
<li class="chapter" data-level="9.4.3" data-path="ch-lon.html"><a href="ch-lon.html#lineartwostageerror"><i class="fa fa-check"></i><b>9.4.3</b> Error structure within schools</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="ch-lon.html"><a href="ch-lon.html#lineartwostageerror"><i class="fa fa-check"></i><b>9.5</b> Initial models</a><ul>
<li class="chapter" data-level="9.5.1" data-path="ch-multilevelintro.html"><a href="ch-multilevelintro.html#modela"><i class="fa fa-check"></i><b>9.5.1</b> Unconditional means model</a></li>
<li class="chapter" data-level="9.5.2" data-path="ch-multilevelintro.html"><a href="ch-multilevelintro.html#modelb"><i class="fa fa-check"></i><b>9.5.2</b> Unconditional growth model</a></li>
<li class="chapter" data-level="9.5.3" data-path="ch-lon.html"><a href="ch-lon.html#othertimetrends"><i class="fa fa-check"></i><b>9.5.3</b> Modeling other trends over time</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="ch-lon.html"><a href="ch-lon.html#finalmodel"><i class="fa fa-check"></i><b>9.6</b> Building to a final model</a><ul>
<li class="chapter" data-level="9.6.1" data-path="ch-lon.html"><a href="ch-lon.html#modelc9"><i class="fa fa-check"></i><b>9.6.1</b> Uncontrolled effects of school type</a></li>
<li class="chapter" data-level="9.6.2" data-path="ch-lon.html"><a href="ch-lon.html#modeld"><i class="fa fa-check"></i><b>9.6.2</b> Add percent free and reduced lunch as a covariate</a></li>
<li class="chapter" data-level="9.6.3" data-path="ch-lon.html"><a href="ch-lon.html#modelf9"><i class="fa fa-check"></i><b>9.6.3</b> A potential final model with three Level Two covariates</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="ch-lon.html"><a href="ch-lon.html#errorcovariance"><i class="fa fa-check"></i><b>9.7</b> Covariance structure among observations</a><ul>
<li class="chapter" data-level="9.7.1" data-path="ch-lon.html"><a href="ch-lon.html#standarderror"><i class="fa fa-check"></i><b>9.7.1</b> Standard covariance structure</a></li>
<li class="chapter" data-level="9.7.2" data-path="ch-lon.html"><a href="ch-lon.html#alternateerror"><i class="fa fa-check"></i><b>9.7.2</b> Alternative covariance structures</a></li>
<li class="chapter" data-level="9.7.3" data-path="ch-lon.html"><a href="ch-lon.html#covariance-structure-in-non-longitudinal-multilevel-models"><i class="fa fa-check"></i><b>9.7.3</b> Covariance structure in non-longitudinal multilevel models</a></li>
<li class="chapter" data-level="9.7.4" data-path="ch-lon.html"><a href="ch-lon.html#final-thoughts-regarding-covariance-structures"><i class="fa fa-check"></i><b>9.7.4</b> Final thoughts regarding covariance structures</a></li>
<li class="chapter" data-level="9.7.5" data-path="ch-lon.html"><a href="ch-lon.html#optionalcov"><i class="fa fa-check"></i><b>9.7.5</b> Details of covariance structures (Optional)</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="ch-lon.html"><a href="ch-lon.html#notesr9"><i class="fa fa-check"></i><b>9.8</b> Notes on Using R (Optional)</a></li>
<li class="chapter" data-level="9.9" data-path="ch-lon.html"><a href="ch-lon.html#exercises-7"><i class="fa fa-check"></i><b>9.9</b> Exercises</a><ul>
<li class="chapter" data-level="9.9.1" data-path="ch-lon.html"><a href="ch-lon.html#conceptual-exercises-4"><i class="fa fa-check"></i><b>9.9.1</b> Conceptual Exercises</a></li>
<li class="chapter" data-level="9.9.2" data-path="ch-lon.html"><a href="ch-lon.html#guided-exercise-4"><i class="fa fa-check"></i><b>9.9.2</b> Guided Exercise</a></li>
<li class="chapter" data-level="9.9.3" data-path="ch-lon.html"><a href="ch-lon.html#open-ended-exercises-3"><i class="fa fa-check"></i><b>9.9.3</b> Open-ended Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ch-3level.html"><a href="ch-3level.html"><i class="fa fa-check"></i><b>10</b> Multilevel Data With More Than Two Levels</a><ul>
<li class="chapter" data-level="10.1" data-path="ch-3level.html"><a href="ch-3level.html#learning-objectives-8"><i class="fa fa-check"></i><b>10.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="10.2" data-path="ch-3level.html"><a href="ch-3level.html#cs:seeds"><i class="fa fa-check"></i><b>10.2</b> Case Studies: Seed Germination</a></li>
<li class="chapter" data-level="10.3" data-path="ch-3level.html"><a href="ch-3level.html#explore3"><i class="fa fa-check"></i><b>10.3</b> Initial Exploratory Analyses</a><ul>
<li class="chapter" data-level="10.3.1" data-path="ch-3level.html"><a href="ch-3level.html#organizedata3"><i class="fa fa-check"></i><b>10.3.1</b> Data Organization</a></li>
<li class="chapter" data-level="10.3.2" data-path="ch-3level.html"><a href="ch-3level.html#explore3v2"><i class="fa fa-check"></i><b>10.3.2</b> Exploratory Analyses</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="ch-3level.html"><a href="ch-3level.html#initialmodels-3level"><i class="fa fa-check"></i><b>10.4</b> Initial models: unconditional means and unconditional growth</a></li>
<li class="chapter" data-level="10.5" data-path="ch-3level.html"><a href="ch-3level.html#sec:boundary"><i class="fa fa-check"></i><b>10.5</b> Encountering boundary constraints</a></li>
<li class="chapter" data-level="10.6" data-path="ch-3level.html"><a href="ch-3level.html#threelevel-paraboot"><i class="fa fa-check"></i><b>10.6</b> Parametric bootstrap testing</a></li>
<li class="chapter" data-level="10.7" data-path="ch-3level.html"><a href="ch-3level.html#sec:explodingvarcomps"><i class="fa fa-check"></i><b>10.7</b> Exploding variance components</a></li>
<li class="chapter" data-level="10.8" data-path="ch-3level.html"><a href="ch-3level.html#modelsDEF"><i class="fa fa-check"></i><b>10.8</b> Building to a final model</a></li>
<li class="chapter" data-level="10.9" data-path="ch-3level.html"><a href="ch-3level.html#error-3level"><i class="fa fa-check"></i><b>10.9</b> Covariance structure (Optional)</a><ul>
<li class="chapter" data-level="10.9.1" data-path="ch-3level.html"><a href="ch-3level.html#optionalerror"><i class="fa fa-check"></i><b>10.9.1</b> Details of covariance structures</a></li>
</ul></li>
<li class="chapter" data-level="10.10" data-path="ch-3level.html"><a href="ch-3level.html#usingR3"><i class="fa fa-check"></i><b>10.10</b> Notes on Using R (Optional)</a></li>
<li class="chapter" data-level="10.11" data-path="ch-3level.html"><a href="ch-3level.html#exercises-8"><i class="fa fa-check"></i><b>10.11</b> Exercises</a><ul>
<li class="chapter" data-level="10.11.1" data-path="ch-3level.html"><a href="ch-3level.html#conceptual-exercises-5"><i class="fa fa-check"></i><b>10.11.1</b> Conceptual Exercises</a></li>
<li class="chapter" data-level="10.11.2" data-path="ch-3level.html"><a href="ch-3level.html#guided-exercise-5"><i class="fa fa-check"></i><b>10.11.2</b> Guided Exercise</a></li>
<li class="chapter" data-level="10.11.3" data-path="ch-3level.html"><a href="ch-3level.html#open-ended-exercises-4"><i class="fa fa-check"></i><b>10.11.3</b> Open-ended Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ch-GLMM.html"><a href="ch-GLMM.html"><i class="fa fa-check"></i><b>11</b> Generalized Linear Multilevel Models</a><ul>
<li class="chapter" data-level="11.1" data-path="ch-GLMM.html"><a href="ch-GLMM.html#objectives"><i class="fa fa-check"></i><b>11.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="11.2" data-path="ch-GLMM.html"><a href="ch-GLMM.html#cs:refs"><i class="fa fa-check"></i><b>11.2</b> Case Study: College Basketball Referees</a></li>
<li class="chapter" data-level="11.3" data-path="ch-GLMM.html"><a href="ch-GLMM.html#explore-glmm"><i class="fa fa-check"></i><b>11.3</b> Initial Exploratory Analyses</a><ul>
<li class="chapter" data-level="11.3.1" data-path="ch-GLMM.html"><a href="ch-GLMM.html#data-organization-7"><i class="fa fa-check"></i><b>11.3.1</b> Data organization</a></li>
<li class="chapter" data-level="11.3.2" data-path="ch-GLMM.html"><a href="ch-GLMM.html#glmm-eda"><i class="fa fa-check"></i><b>11.3.2</b> Exploratory analyses</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="ch-GLMM.html"><a href="ch-GLMM.html#twolevelmodeling-glmm"><i class="fa fa-check"></i><b>11.4</b> Two level Modeling with a Generalized Response</a><ul>
<li class="chapter" data-level="11.4.1" data-path="ch-GLMM.html"><a href="ch-GLMM.html#multregr-glmm"><i class="fa fa-check"></i><b>11.4.1</b> A multiple generalized linear model approach (correlation not accounted for)</a></li>
<li class="chapter" data-level="11.4.2" data-path="ch-GLMM.html"><a href="ch-GLMM.html#twostage-glmm"><i class="fa fa-check"></i><b>11.4.2</b> A two-stage modeling approach (provides the basic idea for multilevel modeling)</a></li>
<li class="chapter" data-level="11.4.3" data-path="ch-GLMM.html"><a href="ch-GLMM.html#unified-glmm"><i class="fa fa-check"></i><b>11.4.3</b> A unified multilevel approach (the framework we’ll use)</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="ch-GLMM.html"><a href="ch-GLMM.html#crossedre"><i class="fa fa-check"></i><b>11.5</b> Crossed Random Effects</a></li>
<li class="chapter" data-level="11.6" data-path="ch-GLMM.html"><a href="ch-GLMM.html#glmm-paraboot"><i class="fa fa-check"></i><b>11.6</b> Model Comparisons Using the Parametric Bootstrap</a></li>
<li class="chapter" data-level="11.7" data-path="ch-GLMM.html"><a href="ch-GLMM.html#sec:finalmodel-glmm"><i class="fa fa-check"></i><b>11.7</b> A Potential Final Model for Examining Referee Bias</a></li>
<li class="chapter" data-level="11.8" data-path="ch-GLMM.html"><a href="ch-GLMM.html#estimatedRE"><i class="fa fa-check"></i><b>11.8</b> Estimated Random Effects</a></li>
<li class="chapter" data-level="11.9" data-path="ch-GLMM.html"><a href="ch-GLMM.html#usingR-glmm"><i class="fa fa-check"></i><b>11.9</b> Notes on Using R (Optional)</a></li>
<li class="chapter" data-level="11.10" data-path="ch-GLMM.html"><a href="ch-GLMM.html#exercises-9"><i class="fa fa-check"></i><b>11.10</b> Exercises</a></li>
<li class="chapter" data-level="11.11" data-path="ch-GLMM.html"><a href="ch-GLMM.html#conceptual-exercises-6"><i class="fa fa-check"></i><b>11.11</b> Conceptual Exercises</a><ul>
<li class="chapter" data-level="11.11.1" data-path="ch-GLMM.html"><a href="ch-GLMM.html#guided-exercise-6"><i class="fa fa-check"></i><b>11.11.1</b> Guided Exercise</a></li>
<li class="chapter" data-level="11.11.2" data-path="ch-GLMM.html"><a href="ch-GLMM.html#open-ended-exercises-5"><i class="fa fa-check"></i><b>11.11.2</b> Open-ended Exercises</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Broadening Your Statistical Horizons</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-MLRreview" class="section level1">
<h1><span class="header-section-number">Chapter 1</span> Review of Multiple Linear Regression</h1>
<div id="learning-objectives" class="section level2">
<h2><span class="header-section-number">1.1</span> Learning Objectives</h2>
<p>After finishing this chapter, you should be able to:</p>
<ul>
<li>Identify cases where ordinary least squares (OLS) assumptions are violated.</li>
<li>Generate exploratory data analysis plots and summary statistics.</li>
<li>Use residual diagnostics to examine OLS assumptions.</li>
<li>Interpret parameters and associated tests and intervals from multiple regression models.</li>
<li>Understand the basic ideas behind bootstrapped confidence intervals.</li>
</ul>
</div>
<div id="introduction" class="section level2">
<h2><span class="header-section-number">1.2</span> Introduction</h2>
<p>Ecologists count species, criminologists count arrests, and cancer specialists count cases. Political scientists seek to explain who is a Democrat, pre-med students are curious about who gets in to medical school, and sociologists study which people get tattoos. In the first case, ecologists, criminologists and cancer specialists are concerned about outcomes which are counts. The political scientists’, pre-med students’ and sociologists’ interest centers on binary responses: Democrat or not, accepted or not, and tattooed or not. We can model these non-Gaussian (non-normal) responses in a more natural way by fitting <strong>generalized linear models (GLMs)</strong> as opposed to using to <strong>ordinary least squares (OLS)</strong> models.</p>
<p>When models are fit to data using OLS, inferences are possible using traditional statistical theory under certain conditions: if we can assume that there is a linear relationship between the response (Y) and an explanatory variable (X), the observations are independent of one another, the responses are approximately normal for each level of the X, and the variation in the responses is the same for each level of X. If we intend to make inferences using GLMs, necessary assumptions are different. First, we will not be constrained by the normality assumption. When conditions are met, GLMs can accommodate non-normal responses such as the counts and binary data in our preceding examples. While the observations must still be independent of one another, the variance in Y at each level of X need not be equal nor does the assumption of linearity between Y and X need to be plausible.</p>
<p>However GLMs cannot be used for models in the following circumstances: medical researchers collect data on patients in clinical trials weekly for 6 months; rat dams are injected with teratogenic substances and their offspring are monitored for defects; and, musicians’ performance anxiety is recorded for several performances. Each of these examples involves correlated data: the same patient’s outcomes are more likely to be similar from week-to-week than outcomes from different patients; litter mates are more likely to suffer defects at similar rates in contrast to unrelated rat pups; and, a musician’s anxiety is more similar from performance to performance than it is with other musicians. Each of these examples violate the independence assumption of simpler linear models for OLS or GLM inference.</p>
<p>The <strong>Generalized Linear Models</strong> in the book’s title extends OLS methods you may have seen in linear regression to handle responses that are non-normal in addition to normal outcomes. The <strong>Multilevel Methods</strong> will allow us to create models for situations where the observations are not independent of one another. Overall, these approaches will permit us to get much more out of data and may be more faithful to the actual data structure than models based on ordinary least squares. These models will <em>broaden your statistical horizons</em>.</p>
<p>In order to understand the motivation for handling violations of assumptions, it is helpful to be able to recognize the model assumptions for inference with OLS in the context of different studies. While linearity is sufficient for fitting an OLS model, in order to make inferences and predictions the observations must also be independent, the responses should be approximately normal at each level of the predictors, and the standard deviation of the responses at each level of the predictors should be approximately equal. After examining circumstances where inference with OLS modeling is appropriate, we will look for violations of these assumptions in other sets of circumstances. These are settings where we may be able to use the methods of this text. We’ve kept the examples in the exposition simple to fix ideas. There are exercises which describe more realistic and complex studies.</p>
</div>
<div id="ordinary-least-squares-ols-assumptions" class="section level2">
<h2><span class="header-section-number">1.3</span> Ordinary Least Squares (OLS) Assumptions</h2>
<div class="figure" style="text-align: center"><span id="fig:OLSassumptions"></span>
<img src="bookdown-bysh_files/figure-html/OLSassumptions-1.png" alt="Ordinary least squares assumptions" width="60%" />
<p class="caption">
Figure 1.1: Ordinary least squares assumptions
</p>
</div>
<p>Recall that making inferences or predictions with models fit using ordinary least squares (OLS) requires that the following assumptions be tenable. The acronym LINE can be used to recall the assumptions required for making inferences and predictions with models based on OLS. If we consider a simple linear regression, with just a single predictor X, then:</p>
<ul>
<li>(<em>L</em>) there is a linear relationship between the mean response (Y) and the explanatory variable (X),</li>
<li>(<em>I</em>) the errors are independent—there’s no connection between how far any two points lie from the regression line,</li>
<li>(<em>N</em>) the responses are normally distributed at each level of X, and</li>
<li>(<em>E</em>) the variance or, equivalently, the standard deviation of the responses is equal for all levels of X.</li>
</ul>
<p>These assumptions are depicted in Figure <a href="ch-MLRreview.html#fig:OLSassumptions">1.1</a>.</p>
<ul>
<li>(<em>L</em>) The mean value for Y at each level of X falls on the regression line.<br />
</li>
<li>(<em>I</em>) We’ll need to check the design of the study to determine if the errors (distances from the line) are independent of one another.<br />
</li>
<li>(<em>N</em>) At each level of X, the values for Y are normally distributed.</li>
<li>(<em>E</em>) The spread in the Y’s for each level of X is the same.</li>
</ul>
<div id="cases-that-do-not-violate-the-ols-assumptions-for-inference" class="section level3">
<h3><span class="header-section-number">1.3.1</span> Cases that do not violate the OLS assumptions for inference</h3>
<p>It can be argued that the following studies do not violate the OLS assumptions for inference. We begin by identifying the response and the explanatory variables followed by describing each of the LINE assumptions in the context of the study, commenting on possible problems with the assumptions.</p>
<ol style="list-style-type: decimal">
<li><strong>Reaction times and car radios.</strong> A researcher suspects that loud music can affect how quickly drivers react. She randomly selects drivers to drive the same stretch of road with varying levels of music volume. Stopping distances for each driver are measured along with the decibel level of the music on their car radio.</li>
</ol>
<ul>
<li><em>Response variable:</em> Reaction time</li>
<li><em>Explanatory variable:</em> Decibel level of music</li>
</ul>
<p>The OLS assumptions for inference would apply if:</p>
<ul>
<li><strong>L:</strong> The mean reaction time is linearly related to decibel level of the music.</li>
<li><strong>I:</strong> Stopping distances are independent. The random selection of drivers should assure independence.</li>
<li><strong>N:</strong> The stopping distances for a given decibel level of music vary and are normally distributed.</li>
<li><strong>E:</strong> The variation in stopping distances should be approximately the same for each decibel level of music.</li>
</ul>
<p>There are potential problems with the linearity and equal standard deviation assumptions. For example, if there is threshold for the volume of music where the effect on reaction times remains the same, mean reaction times would not be a linear function of music. Another problem may occur if a few subjects at each decibel level took a really long time to react. In this case, reaction times would be right skewed and the normality assumption would be violated. Often we can think of circumstances where the OLS assumptions may be suspect. Later in this chapter we will describe plots which can help diagnose issues with OLS assumptions.</p>
<ol start="2" style="list-style-type: decimal">
<li><strong>Crop yield and rainfall.</strong> The yield of wheat per acre for the month of July is thought to be related to the rainfall. A researcher randomly selects acres of wheat and records the rainfall and bushels of wheat per acre.</li>
</ol>
<ul>
<li><em>Response variable:</em> Yield of wheat measured in bushels per acre for July</li>
<li><p><em>Explanatory variable:</em> Rainfall measured in inches for July</p></li>
<li><strong>L:</strong> The mean yield per acre is linearly related to rainfall.</li>
<li><strong>I:</strong> Fields’ yields are independent; knowing one (X, Y) pair does not provide information about another.</li>
<li><strong>N:</strong> The yields for a given amount of rainfall are normally distributed.</li>
<li><p><strong>E:</strong> The standard deviation of yields is approximately the same for each rainfall level.</p></li>
</ul>
<p>Again we may encounter problems with the linearity assumption if mean yields increase initially as the amount of rainfall increases after which excess rainfall begins to ruin crop yield. The random selection of fields should assure independence if fields are not close to one another.</p>
<ol start="3" style="list-style-type: decimal">
<li><strong>Heights of sons and fathers.</strong> Galton suspected that a son’s height could be predicted using the father’s height. He collected observations on heights of fathers and their firstborn sons.</li>
</ol>
<ul>
<li><em>Response variable:</em> Height of the firstborn son</li>
<li><p><em>Explanatory variable:</em> Height of the father</p></li>
<li><strong>L:</strong> The mean height of firstborn sons is linearly related to heights of fathers.</li>
<li><strong>I:</strong> The height of one firstborn son is independent of the height of other firstborn sons in the study. This would be the case if firstborn sons were randomly selected.</li>
<li><strong>N:</strong> The heights of firstborn sons for a given fathers’ height are normally distributed.</li>
<li><p><strong>E:</strong> The standard deviation of firstborn sons’ heights at a given father’s height are the same.</p></li>
</ul>
<p>Heights and other similar measurements are often normally distributed. There would be a problem with the independence assumption if multiple sons from the same family were selected. Or, there would be a problem with equal variance if sons of tall fathers had much more variety in their heights than sons of shorter fathers.</p>
</div>
<div id="cases-where-the-ols-assumptions-for-inference-are-violated" class="section level3">
<h3><span class="header-section-number">1.3.2</span> Cases where the OLS assumptions for inference are violated</h3>
<ol style="list-style-type: decimal">
<li><strong>Grades and studying.</strong> Is the time spent studying predictive of success on an exam? The time spent studying for an exam, in hours, and success, measured as Pass or Fail, are recorded for randomly selected students.</li>
</ol>
<ul>
<li><em>Response variable:</em> Exam outcome (Pass or Fail)</li>
<li><em>Explanatory variable:</em> Time spent studying (in hours)</li>
</ul>
<p>Here the response is a binary outcome which violates the OLS assumption of a normally distribution response at each level of X. In Chapter <a href="ch-logreg.html#ch-logreg">6</a>, we will see logistic regression which is more suitable for models with binary responses.</p>
<ol start="2" style="list-style-type: decimal">
<li><strong>Income and family size.</strong> Do wealthy families tend to have fewer children compared to lower income families? Annual income and family size are recorded for a random sample of families.</li>
</ol>
<ul>
<li><em>Response variable:</em> Family size, number of children</li>
<li><em>Explanatory variable:</em> Annual income, in dollars</li>
</ul>
<p>Family size is a count taking on integer values from 0 to (technically) no upper bound. The normality assumption may be problematic again because the distribution of family size is likely to be skewed, with more families having one or two children and only a few with a much larger number of children. Both of these concerns lead us to question the validity of the normality assumption. Study design should also specify that families are done adding children to their family.</p>
<ol start="3" style="list-style-type: decimal">
<li><strong>Exercise, weight, and sex.</strong> Investigators collected the weight, sex, and amount of exercise for a random sample of college students.</li>
</ol>
<ul>
<li><em>Response variable:</em> Weight</li>
<li><em>Explanatory variables:</em> Sex and hours spent exercising in a typical week</li>
</ul>
<p>With two predictors, the assumptions now apply to the combination of sex and exercise. For example, the linearity assumption implies that there is a linear relationship in mean weight and amount of exercise for males and, similarly, a linear relationship in mean weight and amount of exercise for females. This data may not be appropriate for OLS modeling because the standard deviation in weight for students who do not exercise for each sex is likely to be considerably greater than the standard deviation in weight for students who follow an exercise regime. We can assess this potential problem by plotting weight by amount of exercise for males and females separately. There may also be a problem with the independence assumption because there is no indication that the subjects were randomly selected. There may be subgroups of subjects likely to be more similar, e.g. selecting students at a gym and others in a TV lounge.</p>
<ol start="4" style="list-style-type: decimal">
<li><strong>Surgery Outcome and Patient Age.</strong> Medical researchers investigated the outcome of a particular surgery for patients with comparable stages of disease but different ages. The ten hospitals in the study had at least two surgeons performing the surgery of interest. Patients were randomly selected for each surgeon at each hospital. The surgery outcome was recorded on a scale of one to ten.</li>
</ol>
<ul>
<li><em>Response variable:</em> Surgery outcome, scale 1-10</li>
<li><em>Explanatory variable:</em> Patient age, in years</li>
</ul>
<p>Outcomes for patients operated on by the same surgeon are more likely to be similar and have similar results. For example, if surgeons’ skills differ or if their criteria for selecting patients for surgery vary, individual surgeons may tend to have better or worse outcomes, and patient outcomes will be dependent on surgeon. Furthermore, outcomes at one hospital may be more similar possibly due to factors associated with different patient populations. The very structure of this data suggests that the independence assumption will be violated. Multilevel models will explicitly take this structure into account for a proper analysis of this study’s results.</p>
<p>While we identified possible violations of OLS assumptions for inference for each of the examples in this section, there may be violations of the other assumptions that we have not pointed out. Prior to reading this book, you have presumably learned some ways to handle these violations such as applying variance stabilizing transformations or logging responses, but you will discover other models in this text that may be more appropriate for the violations we have presented.</p>
</div>
</div>
<div id="cs:derby" class="section level2">
<h2><span class="header-section-number">1.4</span> Case Study: Kentucky Derby</h2>
<p>Before diving into generalized linear models and multilevel modeling, we review key ideas from multiple linear regression using an example from horse racing. The Kentucky Derby is a 1.25 mile horse race held annually at the Churchill Downs race track in Louisville, Kentucky. Our data set contains the <code>year</code> of the race, the winning horse (<code>winner</code>), the <code>condition</code> of the track, the average <code>speed</code> (in feet per second) of the winner, and the number of <code>starters</code> (field size, or horses who raced) for the years 1896-2017. The track <code>condition</code> has been grouped into three categories: fast, good (which includes the official designations “good” and “dusty”), and slow (which includes the designations “slow”, “heavy”, “muddy”, and “sloppy”). We would like to use OLS linear regression techniques to model the speed of the winning horse as a function of track condition, field size, and trends over time.</p>
</div>
<div id="explore" class="section level2">
<h2><span class="header-section-number">1.5</span> Initial Exploratory Analyses</h2>
<div id="data-organization" class="section level3">
<h3><span class="header-section-number">1.5.1</span> Data Organization</h3>
<p>The first five and last five rows from our data set are illustrated in Table <a href="ch-MLRreview.html#tab:introtable1">1.1</a>. Note that, in certain cases, we created new variables from existing ones:</p>
<ul>
<li><code>fast</code> is an <strong>indicator variable</strong>, taking the value 1 for races run on fast tracks, and 0 for races run under other conditions,</li>
<li><code>good</code> is another indicator variable, taking the value 1 for races run under good conditions, and 0 for races run under other conditions,</li>
<li><code>yearnew</code> is a <strong>centered variable</strong>, where we measure the number of years since 1896, and</li>
<li><code>fastfactor</code> replaces <code>fast</code> = 0 with the description “not fast”, and <code>fast</code> = 1 with the description “fast”. Changing a numeric categorical variable to descriptive phrases can make plot legends more meaningful.</li>
</ul>
<table>
<caption><span id="tab:introtable1">Table 1.1: </span>The first five and the last five observations from the Kentucky Derby case study.</caption>
<thead>
<tr class="header">
<th align="right">year</th>
<th align="left">winner</th>
<th align="left">condition</th>
<th align="right">speed</th>
<th align="right">starters</th>
<th align="right">fast</th>
<th align="right">good</th>
<th align="right">yearnew</th>
<th align="left">fastfactor</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1896</td>
<td align="left">Ben Brush</td>
<td align="left">good</td>
<td align="right">51.66</td>
<td align="right">8</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="left">not fast</td>
</tr>
<tr class="even">
<td align="right">1897</td>
<td align="left">Typhoon II</td>
<td align="left">slow</td>
<td align="right">49.81</td>
<td align="right">6</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left">not fast</td>
</tr>
<tr class="odd">
<td align="right">1898</td>
<td align="left">Plaudit</td>
<td align="left">good</td>
<td align="right">51.16</td>
<td align="right">4</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">2</td>
<td align="left">not fast</td>
</tr>
<tr class="even">
<td align="right">1899</td>
<td align="left">Manuel</td>
<td align="left">fast</td>
<td align="right">50.00</td>
<td align="right">5</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="left">fast</td>
</tr>
<tr class="odd">
<td align="right">1900</td>
<td align="left">Lieut. Gibson</td>
<td align="left">fast</td>
<td align="right">52.28</td>
<td align="right">7</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">4</td>
<td align="left">fast</td>
</tr>
<tr class="even">
<td align="right">2013</td>
<td align="left">Orb</td>
<td align="left">slow</td>
<td align="right">53.71</td>
<td align="right">19</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">117</td>
<td align="left">not fast</td>
</tr>
<tr class="odd">
<td align="right">2014</td>
<td align="left">California Chrome</td>
<td align="left">fast</td>
<td align="right">53.37</td>
<td align="right">19</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">118</td>
<td align="left">fast</td>
</tr>
<tr class="even">
<td align="right">2015</td>
<td align="left">American Pharoah</td>
<td align="left">fast</td>
<td align="right">53.65</td>
<td align="right">18</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">119</td>
<td align="left">fast</td>
</tr>
<tr class="odd">
<td align="right">2016</td>
<td align="left">Nyquist</td>
<td align="left">fast</td>
<td align="right">54.41</td>
<td align="right">20</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">120</td>
<td align="left">fast</td>
</tr>
<tr class="even">
<td align="right">2017</td>
<td align="left">Always Dreaming</td>
<td align="left">fast</td>
<td align="right">53.40</td>
<td align="right">20</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">121</td>
<td align="left">fast</td>
</tr>
</tbody>
</table>
</div>
<div id="univariate-summaries" class="section level3">
<h3><span class="header-section-number">1.5.2</span> Univariate Summaries</h3>
<p>With any statistical analysis, our first task is to explore the data, examining distributions of individual responses and predictors using graphical and numerical summaries, and beginning to discover relationships between variables. This should <em>always</em> be done <em>before</em> any model fitting! We must understand our data thoroughly before doing anything else.</p>
<p>First, we will examine each response variable and potential covariate individually. Continuous variables can be summarized using histograms and statistics indicating center and spread; categorical variables can be summarized with tables and possibly bar charts.</p>
<div class="figure" style="text-align: center"><span id="fig:twohist"></span>
<img src="bookdown-bysh_files/figure-html/twohist-1.png" alt="Histograms of key continuous variables.  Plot (a) shows winning speeds, while plot (b) shows the number of starters." width="60%" />
<p class="caption">
Figure 1.2: Histograms of key continuous variables. Plot (a) shows winning speeds, while plot (b) shows the number of starters.
</p>
</div>
<p>In Figure <a href="ch-MLRreview.html#fig:twohist">1.2</a>(a), we see that the primary response, winning speed, follows a distribution with a slight left skew, with a large number of horses winning with speeds between 53-55 feet per second. Plot (b) shows that the number of starters is mainly distributed between 5 and 20, with the largest number of races having between 15 and 20 starters.</p>
<p>The primary categorical explanatory variable is track condition, where 88 (72%) of the 122 races were run under fast conditions, 10 (8%) under good conditions, and 24 (20%) under slow conditions.</p>
</div>
<div id="bivariate-summaries" class="section level3">
<h3><span class="header-section-number">1.5.3</span> Bivariate Summaries</h3>
<p>The next step in an initial exploratory analysis is the examination of numerical and graphical summaries of relationships between model covariates and responses. Figure <a href="ch-MLRreview.html#fig:bivariate">1.3</a> is densely packed with illustrations of bivariate relationships. The relationship between two continuous variables is depicted with scatterplots below the diagonal and correlation coefficients above the diagonal. Here, we see that higher winning speeds are associated with more recent years, while the relationship between winning speed and number of starters is less clear cut. We also see a somewhat strong correlation between year and number of starters—we should be aware of highly correlated explanatory variables whose contributions might overlap too much.</p>
<div class="figure" style="text-align: center"><span id="fig:bivariate"></span>
<img src="bookdown-bysh_files/figure-html/bivariate-1.png" alt="Relationships between pairs of variables in the Kentucky Derby data set" width="60%" />
<p class="caption">
Figure 1.3: Relationships between pairs of variables in the Kentucky Derby data set
</p>
</div>
<p>Relationships between categorical variables like track condition and continuous variables can be illustrated with side-by-side boxplots as in the top row, or with stacked histograms as in the first column. As expected, we see evidence of higher speeds on fast tracks and also a tendency for recent years to have more fast conditions. These observed trends can be supported with summary statistics generated by subgroup. For instance, the mean speed under fast conditions is 53.6 feet per second, compared to 52.7 ft/s under good conditions and 51.7 ft/s under slow conditions. Variability in winning speeds, however, is greatest under slow conditions (SD = 1.36 ft/s) and least under fast conditions (0.94 ft/s).</p>
<p>Finally, notice that the diagonal illustrates the distribution of individual variables, using density curves for continuous variables and a bar chart for categorical variables. Trends observed in the first and third diagonal entries match trends observed in Figure <a href="ch-MLRreview.html#fig:twohist">1.2</a>.</p>
<p>By using shape or color or other attributes, we can incorporate the effect of a third or even fourth variable into the scatterplots of Figure <a href="ch-MLRreview.html#fig:bivariate">1.3</a>. For example, in the <strong>coded scatterplot</strong> of Figure <a href="ch-logreg.html#fig:coded">6.2</a> we see that speeds are generally faster under fast conditions, but the rate of increasing speed over time is greater under good or slow conditions.</p>
<div class="figure" style="text-align: center"><span id="fig:codeds"></span>
<img src="bookdown-bysh_files/figure-html/codeds-1.png" alt="Linear trends in winning speeds over time, presented separately for fast conditions vs. good or slow conditions" width="60%" />
<p class="caption">
Figure 1.4: Linear trends in winning speeds over time, presented separately for fast conditions vs. good or slow conditions
</p>
</div>
<p>Of course, any graphical analysis is exploratory, and any notable trends at this stage should be checked through formal modeling. At this point, a statistician begins to ask familiar questions such as:</p>
<ul>
<li>are winning speeds increasing in a linear fashion?</li>
<li>does the rate of increase in winning speed depend on track condition or number of starters?</li>
<li>after accounting for other explanatory variables, is greater field size (number of starters) associated with faster winning speeds (because more horses in the field means a greater chance one horse will run a very fast time) or slower winning speeds (because horses are more likely to bump into each other or crowd each others’ attempts to run at full gait)?</li>
<li>are any of these associations statistically significant?</li>
<li>how well can we predict the winning speed in the Kentucky Derby?</li>
</ul>
<p>As you might expect, answers to these questions will arise from proper consideration of variability and properly identified statistical models.</p>
</div>
</div>
<div id="multreg" class="section level2">
<h2><span class="header-section-number">1.6</span> Multiple linear regression modeling</h2>
<div id="SLRcontinuous" class="section level3">
<h3><span class="header-section-number">1.6.1</span> Simple linear regression with a continuous predictor</h3>
We will begin by modeling the winning speed as a function of time; for example, have winning speeds increased at a constant rate since 1896? For this initial model, let <span class="math inline">\(Y_{i}\)</span> be the speed of the winning horse in year <span class="math inline">\(i\)</span>. Then, we might consider Model 1:
<span class="math display" id="eq:model1">\[\begin{equation}
Y_{i}=\beta_{0}+\beta_{1}\textstyle{Year}_{i}+\epsilon_{i} \textrm{ where } \epsilon_{i}\sim N(0,\sigma^2).
\tag{1.1}
\end{equation}\]</span>
<p>In this case, <span class="math inline">\(\beta_{0}\)</span> represents the true intercept—the expected winning speed during Year 0. <span class="math inline">\(\beta_{1}\)</span> represents the true slope—the expected increase in winning speed from one year to the next, assuming the rate of increase is linear (i.e., constant since 1896). Finally, the <strong>error</strong> (<span class="math inline">\(\epsilon_{i}\)</span>) terms represent the deviations of the actual winning speed in Year <span class="math inline">\(i\)</span> (<span class="math inline">\(Y_i\)</span>) from the expected scores under this model (<span class="math inline">\(\beta_{0} + \beta_{1}\textstyle{Year}_{i}\)</span>)—the part of a horse’s winning speed that is not explained by a linear trend over time. The variability in these deviations from the regression model is denoted by <span class="math inline">\(\sigma^2\)</span>.</p>
<p>The parameters in this model (<span class="math inline">\(\beta_{0}\)</span>, <span class="math inline">\(\beta_{1}\)</span>, and <span class="math inline">\(\sigma^2\)</span>) can be estimated through OLS methods; we will use hats to denote estimates of population parameters based on empirical data. Values for <span class="math inline">\(\hat{\beta}_{0}\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span> are selected to minimize the sum of squared residuals, where a <strong>residual</strong> is simply the observed prediction error—the actual winning speed for a given year minus the winning speed predicted by the model. In the notation of this section,</p>
<ul>
<li>Predicted speed: <span class="math inline">\(\hat{Y}_{i}=\hat{\beta}_{0}+\hat{\beta}_{1}\textstyle{Year}_{i}\)</span></li>
<li>Residual (estimated error): <span class="math inline">\(\hat{\epsilon}_{i}=Y_{i} - \hat{Y}_{i}\)</span></li>
<li>Estimated variance of points around the line: <span class="math inline">\(\hat{\sigma}^2 = \sum \hat{\epsilon}^2_{i} / (n-2)\)</span></li>
</ul>
<p>Using Kentucky Derby data, we estimate <span class="math inline">\(\hat{\beta}_{0}=2.05\)</span>, <span class="math inline">\(\hat{\beta}_{1}=0.026\)</span>, and <span class="math inline">\(\hat{\sigma}=0.90\)</span>. Thus, according to our simple linear regression model, winning horses of the Kentucky Derby have an estimated winning speed of 2.05 ft/s in Year 0 (almost 2000 years ago!), and the winning speed improves by an estimated 0.026 ft/s every year. With an <span class="math inline">\(R^2\)</span> of 0.513, the regression model explains a moderate amount (51.3%) of the year-to-year variability in winning speeds, and the trend toward a linear rate of improvement each year is statistically significant at the 0.05 level (t(120) = 11.251, p &lt; .001).</p>
<pre><code>lm(formula = speed ~ year, data = derby.df)

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 2.053473   4.543754   0.452    0.652    
year        0.026126   0.002322  11.251   &lt;2e-16 ***
---
Residual standard error: 0.9032 on 120 degrees of freedom
Multiple R-squared:  0.5134,    Adjusted R-squared:  0.5093 </code></pre>
You may have noticed in Model 1 that the intercept has little meaning in context, since it estimates a winning speed in Year 0, when the first Kentucky Derby run at the current distance (1.25 miles) was in 1896. One way to create more meaningful parameters is through <strong>centering</strong>. In this case, we could create a centered year variable by substracting 1896 from each year for Model 2:
<span class="math display" id="eq:model2">\[\begin{equation}
Y_{i}=\beta_{0}+\beta_{1}\textstyle{Yearnew}_{i}+\epsilon_{i} \textrm{ where } \epsilon_{i}\sim N(0,\sigma^2) \textrm{ and } \textstyle{Yearnew}=\textstyle{Year}-1896.
\tag{1.2}
\end{equation}\]</span>
<p>Note that the only thing that changes from Model 1 to Model 2 is the estimated intercept; <span class="math inline">\(\hat{\beta}_{1}\)</span>, <span class="math inline">\(R^2\)</span>, and <span class="math inline">\(\hat{\sigma}\)</span> all remain exactly the same. Now <span class="math inline">\(\hat{\beta}_{0}\)</span> tells us that the estimated winning speed in 1896 is 51.59 ft/s, but estimates of the linear rate of improvement or the variability explained by the model remain the same. As Figure <a href="ch-MLRreview.html#fig:center">1.5</a> shows, centering year has the effect of shifting the y-axis from year 0 to year 1896, but nothing else changes.</p>
<pre><code>lm(formula = speed ~ yearnew, data = derby.df)

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 51.588393   0.162549  317.37   &lt;2e-16 ***
yearnew      0.026126   0.002322   11.25   &lt;2e-16 ***
---
Residual standard error: 0.9032 on 120 degrees of freedom
Multiple R-squared:  0.5134,    Adjusted R-squared:  0.5093 </code></pre>
<div class="figure" style="text-align: center"><span id="fig:center"></span>
<img src="bookdown-bysh_files/figure-html/center-1.png" alt="Compare Model 1 (with intercept at 0) to Model 2 (with intercept at 1896)" width="60%" />
<p class="caption">
Figure 1.5: Compare Model 1 (with intercept at 0) to Model 2 (with intercept at 1896)
</p>
</div>
<p>We should also attempt to verify that our LINE linear regression model assumptions fit for Model 2 if we want to make inferential statements (hypothesis tests or confidence intervals) about parameters or predictions. Most of these assumptions can be checked graphically using a set of residual plots as in Figure <a href="ch-MLRreview.html#fig:resid2">1.6</a>:</p>
<ul>
<li>The upper left plot, Residuals vs. Fitted, can be used to check the Linearity assumption. Residuals should be patternless around Y = 0; if not, there is a pattern in the data that is currently unaccounted for.<br />
</li>
<li>The upper right plot, Normal Q-Q, can be used to check the Normality assumption. Deviations from a straight line indicate that the distribution of residuals does not conform to a theoretical normal curve.</li>
<li>The lower left plot, Scale-Location, can be used to check the Equal Variance assumption. Positive or negative trends across the fitted values indicate variability that is not constant.</li>
<li>The lower right plot, Residuals vs. Leverage, can be used to check for influential points. Points with high leverage (having unusual values of the predictors) and/or high absolute residuals can have an undue influence on estimates of model parameters.</li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:resid2"></span>
<img src="bookdown-bysh_files/figure-html/resid2-1.png" alt="Residual plots for Model 2" width="60%" />
<p class="caption">
Figure 1.6: Residual plots for Model 2
</p>
</div>
<p>In this case, the Residuals vs. Fitted plot indicates that a quadratic fit might be better than the linear fit of Model 2; other assumptions look reasonable. Influential points would be denoted by high values of Cook’s Distance; they would fall outside cutoff lines in the northeast or southeast section of the Residuals vs. Leverage plot. Since no cutoff lines are even noticeable, there are no potential influential points of concern.</p>
<p>We recommend relying on graphical evidence for identifying regression model assumption violations, looking for highly obvious violations of assumptions before trying corrective actions. While some numerical tests have been devised for issues such as normality and influence, most of these tests are not very reliable, highly influenced by sample size and other factors. There is typically no residual plot, however, to evaluate the Independence assumption; evidence for lack of independence comes from knowing about the study design and methods of data collection. In this case, with a new field of horses each year, the assumption of independence is pretty reasonable.</p>
Based on residual diagnostics, we should test Model 2Q, in which a quadratic term is added to the linear term in Model 2.<br />

<span class="math display" id="eq:model2Q">\[\begin{equation}
Y_{i}=\beta_{0}+\beta_{1}\textstyle{Yearnew}_{i}+\beta_{2}\textstyle{Yearnew}^2_{i}+\epsilon_{i} \textrm{ where } \epsilon_{i}\sim N(0,\sigma^2).
\tag{1.3}
\end{equation}\]</span>
<p>This model could suggest, for example, that the rate of increase in winning speeds is slowing down over time. In fact, there is evidence that the quadratic model improves upon the linear model (see Figure <a href="ch-MLRreview.html#fig:models2and2q">1.7</a>). <span class="math inline">\(R^2\)</span>, the proportion of year-to-year variability in winning speeds explained by the model, has increased from 51.3% to 64.1%, and the pattern in the Residuals vs. Fitted plot of Figure <a href="ch-MLRreview.html#fig:resid2">1.6</a> has disappeared in Figure <a href="ch-MLRreview.html#fig:resid2q">1.8</a>, although normality is a little sketchier in the left tail, and the larger mass of points with fitted values near 54 appears to have slightly lower variability. The significantly negative coefficient for <span class="math inline">\(\beta_{2}\)</span> suggests that the rate of increase is indeed slowing in more recent years.</p>
<div class="figure" style="text-align: center"><span id="fig:models2and2q"></span>
<img src="bookdown-bysh_files/figure-html/models2and2q-1.png" alt="Linear vs. quadratic fit" width="60%" />
<p class="caption">
Figure 1.7: Linear vs. quadratic fit
</p>
</div>
<pre><code>lm(formula = speed ~ yearnew + yearnew2, data = derby.df)

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  5.059e+01  2.082e-01 243.010  &lt; 2e-16 ***
yearnew      7.617e-02  7.950e-03   9.581  &lt; 2e-16 ***
yearnew2    -4.136e-04  6.359e-05  -6.505 1.92e-09 ***
---
Residual standard error: 0.779 on 119 degrees of freedom
Multiple R-squared:  0.641, Adjusted R-squared:  0.635 </code></pre>
<div class="figure" style="text-align: center"><span id="fig:resid2q"></span>
<img src="bookdown-bysh_files/figure-html/resid2q-1.png" alt="Residual plots for Model 2Q" width="60%" />
<p class="caption">
Figure 1.8: Residual plots for Model 2Q
</p>
</div>
</div>
<div id="simple-linear-regression-with-a-binary-predictor" class="section level3">
<h3><span class="header-section-number">1.6.2</span> Simple linear regression with a binary predictor</h3>
We also may want to include track condition as an explanatory variable. We could start by using <code>fast</code> as the lone predictor: Do winning speeds differ for fast and non-fast conditions? <code>fast</code> is considered an <strong>indicator variable</strong>—it takes on only the values 0 and 1, where 1 indicates presence of a certain attribute (like fast racing conditions). Since <code>fast</code> is numeric, we can use simple linear regression techniques to fit Model 3:
<span class="math display" id="eq:model3">\[\begin{equation}
Y_{i}=\beta_{0}+\beta_{1}\textstyle{Fast}_{i}+\epsilon_{i} \textrm{ where } \epsilon_{i}\sim N(0,\sigma^2).
\tag{1.4}
\end{equation}\]</span>
<p>Here, it’s easy to see the meaning of our slope and intercept by writing out separate equations for the two conditions:</p>
<ul>
<li>Good or slow conditions (<code>fast</code> = 0)</li>
</ul>
<span class="math display">\[\begin{equation}
Y_{i} = \beta_{0}+\epsilon_{i}
\end{equation}\]</span>
<ul>
<li>Fast conditions (<code>fast</code> = 1)</li>
</ul>
<span class="math display">\[\begin{equation}
Y_{i} = (\beta_{0}+\beta_{1})+\epsilon_{i}
\end{equation}\]</span>
<p><span class="math inline">\(\beta_{0}\)</span> is the expected winning speed under good or slow conditions, while <span class="math inline">\(\beta_{1}\)</span> is the difference between expected winning speeds under fast conditions vs. non-fast conditions. According to our fitted Model 3, the estimated winning speed under non-fast conditions is 52.0 ft/s, while mean winning speeds under fast conditions are estimated to be 1.6 ft/s higher.</p>
<pre><code>lm(formula = speed ~ fast, data = derby.df)

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  51.9938     0.1826 284.698  &lt; 2e-16 ***
fast          1.6292     0.2150   7.577 8.17e-12 ***
---
Residual standard error: 1.065 on 120 degrees of freedom
Multiple R-squared:  0.3236,    Adjusted R-squared:  0.318 </code></pre>
<p>You might be asking at this point: If we simply wanted to compare mean winning speeds under fast and non-fast conditions, why didn’t we just run a two-sample t-test? The answer is: we did! The t-test corresponding to <span class="math inline">\(\beta_{1}\)</span> is equivalent to an independent-samples t-test with under equal variances. Convince yourself that this is true, and that the equal variance assumption is needed.</p>
</div>
<div id="multiple-linear-regression-with-two-predictors" class="section level3">
<h3><span class="header-section-number">1.6.3</span> Multiple linear regression with two predictors</h3>
<p>The beauty of the linear regression framework is that we can add additional explanatory variables in order to explain more variability in our response, obtain better and more precise predictions, and control for certain covariates while evaluating the effect of others. For example, we could consider adding <code>yearnew</code> to Model 3, which has the indicator variable <code>fast</code> as its only predictor. In this way, we would estimate the difference between winning speeds under fast and non-fast conditions <em>after accounting for the effect of time</em>. As we observed in Figure <a href="ch-MLRreview.html#fig:bivariate">1.3</a>, recent years have tended to have more races under fast conditions, so Model 3 might overstate the effect of fast conditions because winning speeds have also increased over time. A model with terms for both year and track condition will estimate the difference between winning speeds under fast and non-fast conditions <em>for a fixed year</em>; for example, if it had rained in 2016 and turned the track muddy, how much would we have expected the winning speed to decrease?</p>
Our new model (Model 4) can be written:
<span class="math display" id="eq:model4">\[\begin{equation}
Y_{i}=\beta_{0}+\beta_{1}\textstyle{Yearnew}_{i}+\beta_{2}\textstyle{Fast}_{i}+\epsilon_{i} \textrm{ where } \epsilon_{i}\sim N(0,\sigma^2).
\tag{1.5}
\end{equation}\]</span>
<p>and OLS provides the following parameter estimates:</p>
<pre><code>lm(formula = speed ~ yearnew + fast, data = derby.df)

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 50.917822   0.154602  329.35  &lt; 2e-16 ***
yearnew      0.022583   0.001919   11.77  &lt; 2e-16 ***
fast         1.226846   0.150721    8.14 4.39e-13 ***
---
Residual standard error: 0.7269 on 119 degrees of freedom
Multiple R-squared:  0.6874,    Adjusted R-squared:  0.6822 </code></pre>
<p>Our new model estimates that winning speeds are, on average, 1.23 ft/s faster under fast conditions after accounting for time trends, which is down from an estimated 1.63 ft/s without accounting for time. It appears our original model (Model 3) may have overestimated the effect of fast conditions by conflating it with improvements over time. Through our new model, we also estimate that winning speeds increase by 0.023 ft/s per year, after accounting for track condition. This yearly effect is also smaller than the 0.026 ft/s per year we estimated in Model 1, without adjusting for track condition. Based on the <span class="math inline">\(R^2\)</span> value, Model 4 explains 68.7% of the year-to-year variability in winning speeds, a noticeable increase over using either explanatory variable alone.</p>
</div>
<div id="multreg-inference" class="section level3">
<h3><span class="header-section-number">1.6.4</span> Inference in multiple linear regression</h3>
<p>So far we have been using linear regression for descriptive purposes, which is an important task. We are often interested in issues of statistical inference as well—determining if effects are statistically significant, quantifying uncertainty in effect size estimates with confidence intervals, and quantifying uncertainty in model predictions with prediction intervals. Under LINE assumptions, all of these inferential tasks can be completed with the help of the t-distribution and estimated standard errors.</p>
<p>Here are examples of inferential statements based on Model 4:</p>
<ul>
<li>We can be 95% confident that average winning speeds under fast conditions are between 0.93 and 1.53 ft/s higher than under non-fast conditions, after accounting for the effect of year.</li>
<li>Fast conditions lead to significantly faster winning speeds than non-fast conditions (t = 8.14 on 119 df, p &lt; .001), holding year constant.<br />
</li>
<li>Based on our model, we can be 95% confident that the winning speed in 2017 under fast conditions will be between 53.4 and 56.3 ft/s. Note that Always Dreaming’s actual winning speed barely fit within this interval—the 2017 winning speed was a borderline outlier on the slow side.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(model4)</code></pre></div>
<pre><code>                  2.5 %      97.5 %
(Intercept) 50.61169473 51.22394836
yearnew      0.01878324  0.02638227
fast         0.92840273  1.52528902</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">new.data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">yearnew =</span> <span class="dv">2017</span> <span class="op">-</span><span class="st"> </span><span class="dv">1896</span>, <span class="dt">fast =</span> <span class="dv">1</span>) 
<span class="kw">predict</span>(model4, <span class="dt">new =</span> new.data, <span class="dt">interval =</span> <span class="st">&quot;prediction&quot;</span>)</code></pre></div>
<pre><code>       fit     lwr      upr
1 54.87718 53.4143 56.34006</code></pre>
<p>Remember that you must check LINE assumptions using the same residual plots as in Figure <a href="ch-MLRreview.html#fig:resid2">1.6</a> to ensure that these inferential statements are valid. In cases when model assumptions are shaky, one alternative approach to statistical inference is <strong>bootstrapping</strong>; in fact, bootstrapping is a robust approach to statistical inference that we will use frequently throughout this book because of its power and flexibility. In bootstrapping, we use only the data we’ve collected and computing power to estimate the uncertainty surrounding our parameter estimates. Our primary assumption is that our original sample represents the larger population, and then we can learn about uncertainty in our parameter estimates through repeated samples (with replacement) from our original sample.</p>
<p>If we wish to use bootstrapping to obtain confidence intervals for our coefficients in Model 4, we could follow these steps:</p>
<ul>
<li>take a (bootstrap) sample of 122 years of Derby data with replacement, so that some years will get sampled several times and others not at all. This is <strong>case resampling</strong>, so that all information from a given year (winning speed, track condition, number of starters) remains together.</li>
<li>fit Model 4 to the bootstrap sample, saving <span class="math inline">\(\hat{\beta}_0\)</span>, <span class="math inline">\(\hat{\beta}_1\)</span>, and <span class="math inline">\(\hat{\beta}_2\)</span>.</li>
<li>repeat the two steps above a large number of times (say 1000)</li>
<li>the 1000 bootstrap estimates for each parameter can be plotted to show the <strong>bootstrap distribution</strong> (see Figure <a href="ch-MLRreview.html#fig:boot4">1.9</a>)</li>
<li>a 95% confidence interval for each parameter can be found by taking the middle 95% of each bootstrap distribution—i.e., by picking off the 2.5 and 97.5 percentiles. This is called the <strong>percentile method</strong>.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(broom)
bootregr &lt;-<span class="st"> </span>derby.df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">bootstrap</span>(<span class="dv">1000</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">do</span>(<span class="kw">tidy</span>(<span class="kw">lm</span>(speed <span class="op">~</span><span class="st"> </span>yearnew <span class="op">+</span><span class="st"> </span>fast, .)))
bootregr <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">group_by</span>(term) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>dplyr<span class="op">::</span><span class="kw">summarize</span>(<span class="dt">low=</span><span class="kw">quantile</span>(estimate, .<span class="dv">025</span>),
            <span class="dt">high=</span><span class="kw">quantile</span>(estimate, .<span class="dv">975</span>))</code></pre></div>
<pre><code># A tibble: 3 x 3
  term            low    high
  &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;
1 (Intercept) 50.6    51.3   
2 fast         0.929   1.56  
3 yearnew      0.0183  0.0268</code></pre>
<div class="figure" style="text-align: center"><span id="fig:boot4"></span>
<img src="bookdown-bysh_files/figure-html/boot4-1.png" alt="Bootstrapped distributions for Model 4 coefficients" width="60%" />
<p class="caption">
Figure 1.9: Bootstrapped distributions for Model 4 coefficients
</p>
</div>
<p>In this case, we see that 95% bootstrap confidence intervals for <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, and <span class="math inline">\(\beta_2\)</span> are very similar to the normal-theory confidence intervals we found earlier. For example, the normal-theory confidence interval for the effect of fast tracks is 0.93 to 1.53 ft/s, while the analogous bootstrap confidence interval is 0.93 to 1.55 ft/s.</p>
<p>There are many variations on this bootstrap procedure. For example, you could sample residuals rather than cases, or you could conduct a parametric bootstrap in which error terms are randomly chosen from a normal distribution. In addition, researchers have devised other ways of calculating confidence intervals besides the percentile method, including normality, studentized, and bias-corrected and accelerated (Hesterberg 2015; Efron and Tibshirani 1993; Davison and Hinkley 1997). We will focus on case resampling and percentile confidence intervals for now for their understandability and wide applicability.</p>
</div>
<div id="multiple-linear-regression-with-an-interaction-term" class="section level3">
<h3><span class="header-section-number">1.6.5</span> Multiple linear regression with an interaction term</h3>
<p>Adding terms to form a multiple linear regression model as we did in Model 4 is a very powerful modeling tool, allowing us to account for multiple sources of uncertainty and to obtain more precise estimates of effect sizes after accounting for the effect of important covariates. One limitation of Model 4, however, is that we must assume that the effect of track condition has been the same for 122 years, or conversely that the yearly improvements in winning speeds are identical for all track conditions. To expand our modeling capabilities to allow the effect of one predictor to change depending on levels of a second predictor, we need to consider <strong>interaction terms</strong>. Amazingly, if we create a new variable by taking the product of <code>yearnew</code> and <code>fast</code> (i.e., the <strong>interaction</strong> between <code>yearnew</code> and <code>fast</code>), adding that variable into our model will have the desired effect.</p>
Thus, consider Model 5:
<span class="math display" id="eq:model5">\[\begin{equation}
Y_{i}=\beta_{0}+\beta_{1}\textstyle{Yearnew}_{i}+\beta_{2}\textstyle{Fast}_{i}+\beta_{3}\textstyle{Yearnew X Fast}_{i}+\epsilon_{i} \textrm{ where } \epsilon_{i}\sim N(0,\sigma^2).
\tag{1.6}
\end{equation}\]</span>
<p>where OLS provides the following parameter estimates:</p>
<pre><code>lm(formula = speed ~ yearnew + fast + yearnew:fast, data = derby.df)

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  50.528629   0.205072 246.394  &lt; 2e-16 ***
yearnew       0.030751   0.003471   8.859 9.84e-15 ***
fast          1.833523   0.262175   6.994 1.73e-10 ***
yearnew:fast -0.011490   0.004117  -2.791  0.00613 ** 
---
Residual standard error: 0.7071 on 118 degrees of freedom
Multiple R-squared:  0.7068,    Adjusted R-squared:  0.6993 </code></pre>
According to OLS estimates, estimated winning speeds can be found by:
<span class="math display" id="eq:model5est">\[\begin{equation}
\hat{Y}_{i}=50.53+0.031\textstyle{Yearnew}_{i}+1.83\textstyle{Fast}_{i}-0.011\textstyle{Yearnew X Fast}_{i}.
\tag{1.7}
\end{equation}\]</span>
Interpretations of model coefficients are most easily seen by writing out separate equations for fast and non-fast track conditions:
<span class="math display">\[\begin{eqnarray*}
\textstyle{Fast}=0: &amp; &amp; \\
\hat{Y}_{i} &amp; = &amp; 50.53+0.031\textstyle{Yearnew}_{i} \\
\textstyle{Fast}=1: &amp; &amp; \\
\hat{Y}_{i} &amp; = &amp; (50.53+1.83)+(0.031-0.011)\textstyle{Yearnew}_{i}
\end{eqnarray*}\]</span>
<p>leading to the following interpretations for estimated model coefficients:</p>
<ul>
<li><span class="math inline">\(\hat{\beta}_{0} = 50.53\)</span>. The expected winning speed in 1896 under non-fast conditions was 50.53 ft/s.</li>
<li><span class="math inline">\(\hat{\beta}_{1} = 0.031\)</span>. The expected yearly increase in winning speeds under non-fast conditions is 0.031 ft/s.</li>
<li><span class="math inline">\(\hat{\beta}_{2} = 1.83\)</span>. The winning speed in 1896 was expected to be 1.83 ft/s faster under fast conditions compared to non-fast conditions.</li>
<li><span class="math inline">\(\hat{\beta}_{3} = -0.011\)</span>. The expected yearly increase in winning speeds under fast conditions is 0.020 ft/s, which is 0.011 ft/s less than the expected annual increase under non-fast conditions.</li>
</ul>
<p>In fact, using interaction allows us to model the relationships we noticed in Figure <a href="ch-logreg.html#fig:coded">6.2</a>, where both the intercept and slope describing the relationships between <code>speed</code> and <code>year</code> differ depending on whether track conditions were fast or not. Note that we interpret the coefficient for the interaction term by comparing slopes under fast and non-fast conditions; this produces a much more understandable interpretation for a reader than attempting to interpret the -0.011 directly.</p>
</div>
<div id="multreg_build" class="section level3">
<h3><span class="header-section-number">1.6.6</span> Building a multiple linear regression model</h3>
<p>We now begin iterating toward a “final model” for these data, on which we will base conclusions. Typical features of a “final multiple linear regression model” include:</p>
<ul>
<li>explanatory variables allow one to address primary research questions</li>
<li>explanatory variables control for important covariates</li>
<li>potential interactions have been investigated</li>
<li>variables are centered where interpretations can be enhanced</li>
<li>unnecessary terms have been removed</li>
<li>LINE assumptions and the presence of influential points have both been checked using residual plots</li>
<li>the model tells a “persuasive story parsimoniously”</li>
</ul>
<p>Although the process of reporting and writing up research results often demands the selection of a sensible final model, it’s important to realize that (a) statisticians typically will examine and consider an entire taxonomy of models when formulating conclusions, and (b) different statisticians sometimes select different models as their “final model” for the same set of data. Choice of a “final model” depends on many factors, such as primary research questions, purpose of modeling, tradeoff between parsimony and quality of fitted model, underlying assumptions, etc. Modeling decisions should never be automated or made completely on the basis of statistical tests; subject area knowledge should always play a role in the modeling process. You should be able to defend any final model you select, but you should not feel pressured to find the one and only “correct model”, although most good models will lead to similar conclusions.</p>
<p>Several tests and measures of model performance can be used when comparing different models for model building:</p>
<ul>
<li><span class="math inline">\(R^2\)</span> we have seen; it measures the variability in the response variable explained by the model. One problem is that <span class="math inline">\(R^2\)</span> always increases with extra predictors, even if the predictors add very little information.</li>
<li>adjusted <span class="math inline">\(R^2\)</span>. Adds a penalty for model complexity to <span class="math inline">\(R^2\)</span> so that any increase in performance must outweigh the cost of additional complexity. We should ideally favor any model with higher adjusted <span class="math inline">\(R^2\)</span>, regardless of size, but the penalty for model complexity (additional terms) is fairly ad-hoc.</li>
<li>AIC (Akaike Information Criterion). Again attempts to balance model performance with model complexity, with smaller AIC levels being preferable, regardless of model size. The BIC (Bayesian Information Criterion) is similar to the AIC, but with a greater penalty for additional model terms.</li>
<li>extra sum of squares F test. This is a generalization of the t-test for individual model coefficients which can be used to perform significance tests on <strong>nested models</strong>, where one model is a reduced version of the other. For example, we could test whether our final model (below) really needs to adjust for track condition, which is comprised of indicators for both fast condition and good condition (leaving slow condition as the reference level). Our null hypothesis is then <span class="math inline">\(\beta_{3}=\beta_{4}=0\)</span>. We have statistically significant evidence (F = 57.2 on 2 and 116 df, p &lt; .001) that track condition is associated with winning speeds, after accounting for quadratic time trends and number of starters.</li>
</ul>
One potential final model for predicting winning speeds of Kentucky Derby races is:
<span class="math display" id="eq:model0">\[\begin{equation}
Y_{i}=\beta_{0}+\beta_{1}\textstyle{Yearnew}_{i}+\beta_{2}\textstyle{Yearnew}^2_{i}+\beta_{3}\textstyle{Fast}_{i}+\beta_{4}\textstyle{Good}_{i}+\beta_{5}\textstyle{Starters}_{i}+\epsilon_{i} \\
\textrm{ where } \epsilon_{i}\sim N(0,\sigma^2).
\tag{1.8}
\end{equation}\]</span>
<p>and OLS provides the following parameter estimates:</p>
<pre><code>lm(formula = speed ~ yearnew + yearnew2 + fast + good + starters, 
    data = derby.df)

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  5.002e+01  1.946e-01 256.980  &lt; 2e-16 ***
yearnew      7.003e-02  6.130e-03  11.424  &lt; 2e-16 ***
yearnew2    -3.697e-04  4.598e-05  -8.041 8.44e-13 ***
fast         1.393e+00  1.305e-01  10.670  &lt; 2e-16 ***
good         9.157e-01  2.077e-01   4.409 2.33e-05 ***
starters    -2.528e-02  1.360e-02  -1.859   0.0656 .  
---
Residual standard error: 0.5483 on 116 degrees of freedom
Multiple R-squared:  0.8267,    Adjusted R-squared:  0.8192 </code></pre>
<pre><code>Analysis of Variance Table

Model 1: speed ~ yearnew + yearnew2 + starters
Model 2: speed ~ yearnew + yearnew2 + fast + good + starters
  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    
1    118 69.257                                  
2    116 34.870  2    34.386 57.196 &lt; 2.2e-16 ***</code></pre>
<p>This model accounts for the slowing annual increases in winning speed with a negative quadratic term, adjusts for baseline differences stemming from track conditions, and suggests that, for a fixed year and track condition, a larger field is associated with slower winning times (unlike the positive relationship we saw between speed and number of starters in our exploratory analyses). The model explains 82.7% of the year-to-year variability in winning speeds, and residual plots show no serious issues with LINE assumptions. We tested interaction terms for different effects of time or number of starters based on track condition, but we found no significant evidence of interactions.</p>
</div>
</div>
<div id="preview" class="section level2">
<h2><span class="header-section-number">1.7</span> Preview</h2>
<p>Having reviewed key ideas from multiple linear regression, you are now ready to extend those ideas, especially to handle non-normal responses and lack of independence. This section provides a preview of the type of problems you will encounter in the book. For each journal article cited, we provide an abstract in the authors’ words, a description of the type of response and, when applicable, the structure of the data. each of these examples appears later as an exercise, where you can play with the actual data or evaluate the analyses detailed in the articles.</p>
<div id="soccer" class="section level3">
<h3><span class="header-section-number">1.7.1</span> Soccer</h3>
<p>Roskes M, Sligte D, Shalvi S, De Dreu C. (2011). The right side? Under time pressure, approach motivation leads to right-oriented bias. <em>Psychological Science</em> [Online] <strong>22(11)</strong>:1403-7. DOI: 10.1177/0956797611418677, October 2011.</p>
<blockquote>
<p><strong>Abstract:</strong> Approach motivation, a focus on achieving positive outcomes, is related to relative left-hemispheric brain activation, which translates to a variety of right-oriented behavioral biases. [<span class="math inline">\(\ldots\)</span>] In our analysis of all Federation Internationale de Football Association (FIFA) World Cup penalty shoot-outs, we found that goalkeepers were two times more likely to dive to the right than to the left when their team was behind, a situation that we conjecture induces approach motivation. Because penalty takers shot toward the two sides of the goal equally often, the goalkeepers’ right-oriented bias was dysfunctional, allowing more goals to be scored.</p>
</blockquote>
<p>The response for this analysis is the direction of the goalie dive, a binary variable. For example, you could let Y=1 if the dive is to the right and Y=0 if the dive is to the left. This response is clearly not normally distributed. One approach to the analysis is logistic regression as described in Chapter <a href="ch-logreg.html#ch-logreg">6</a>. A binomial random variable could also be created for this application by summing the binary variables for each game so that Y= the number of dives right out of the number of dives the goalie makes during a game. [Thought question: Do you buy the last line of the abstract?]</p>
</div>
<div id="elephant-mating" class="section level3">
<h3><span class="header-section-number">1.7.2</span> Elephant Mating</h3>
<p>Poole J. (1989). Mate guarding, reproductive success and female choice in African elephants. <em>Animal Behavior</em> <strong>37</strong>:842-49.</p>
<blockquote>
<p><strong>Abstract:</strong> Male guarding of females, male mating success and female choice were studied for 8 years among a population of African elephants, Loxodonta africana. Males were not able to compete successfully for access to oestrous female until approximately 25 years of age. Males between 25 and 35 years of age obtained matings during early and late oestrus, but rarely in mid-oestrus. Large musth males over 35 years old guarded females in mid-oestrus. Larger, older males ranked above younger, smaller males and the number of females guarded by males increased rapidly late in life. Body size and longevity are considered important factors in determining the lifetime reproductive success of male elephants…</p>
</blockquote>
<p>Poole and her colleagues recorded, for each male elephant, his age (in years) and the number of matings for a given year. The researchers were interested in how age affects the males’ mating patterns. Specifically, questions concern whether there is a steady increase in mating success as an elephant ages or if there is an optimal age after which the number of matings decline. Because the responses of interest are counts (number of matings for each elephant for a given year), we will consider a Poisson regression (see Chapter <a href="ch-poissonreg.html#ch-poissonreg">4</a>). The general form for Poisson responses is the number of events for a specified time, volume, or space.</p>
</div>
<div id="parenting-and-gang-activity" class="section level3">
<h3><span class="header-section-number">1.7.3</span> Parenting and Gang Activity</h3>
<p>Walker-Barnes C, Mason C. (2001). Ethnic differences in the effect of parenting on gang involvement and gang delinquency: a longitudinal, hierarchical linear modeling perspective. <em>Child Development</em> <strong>72(6)</strong>:1814-31.</p>
<blockquote>
<p><strong>Abstract:</strong> This study examined the relative influence of peer and parenting behavior on changes in adolescent gang involvement and gang-related delinquency. An ethnically diverse sample of 300 ninth-grade students was recruited and assessed on eight occasions during the school year. Analyses were conducted using hierarchical linear modeling. Results indicated that, in general, adolescents decreased their level of gang involvement over the course of the school year, whereas the average level of gang delinquency remained constant over time. As predicted, adolescent gang involvement and gang-related delinquency were most strongly predicted by peer gang involvement and peer gang delinquency, respectively. Nevertheless, parenting behavior continued to significantly predict change in both gang involvement and gang delinquency, even after controlling for peer behavior. A significant interaction between parenting and ethnic and cultural heritage found the effect of parenting to be particularly salient for Black students, for whom higher levels of behavioral control and lower levels of lax parental control were related to better behavioral outcomes over time, whereas higher levels of psychological control predicted worse behavioral outcomes.</p>
</blockquote>
<p>The response for this study is a gang activity measure which ranges from 1 to 100. While it may be reasonable to assume this measure is approximately normal, the structure of this data implies that it is not a simple regression problem. Individual students have measurements made at 8 different points in time. We cannot assume that we have 2400 independent observations because the same measurements on one individual are more likely to be similar than a measurement of another student. Multilevel modeling as discussed in Chapters <a href="ch-3level.html#ch-3level">10</a>, <a href="ch-lon.html#ch-lon">9</a>, and <a href="#ch-glmm"><strong>??</strong></a> can often be used in these situations.</p>
</div>
<div id="crime" class="section level3">
<h3><span class="header-section-number">1.7.4</span> Crime</h3>
<p>Gelman A, Fagan J, Kiss A. (2007). An analysis of the NYPD’s stop-and-frisk policy in the context of claims of racial bias. <em>Journal of the American Statistical Association</em> <strong>102(479)</strong>:813-823.</p>
<blockquote>
<p><strong>Abstract:</strong> Recent studies by police departments and researchers confirm that police stop racial and ethnic minority citizens more often than whites, relative to their proportions in the population. However, it has been argued stop rates more accurately reflect rates of crimes committed by each ethnic group, or that stop rates reflect elevated rates in specific social areas such as neighborhoods or precincts. Most of the research on stop rates and police-citizen interactions has focused on traffic stops, and analyses of pedestrian stops are rare. In this paper, we analyze data from 175,000 pedestrian stops by the New York Police Department over a fifteen-month period. We disaggregate stops by police precinct, and compare stop rates by racial and ethnic group controlling for previous race-specific arrest rates. We use hierarchical multilevel models to adjust for precinct-level variability, thus directly addressing the question of geographic heterogeneity that arises in the analysis of pedestrian stops. We find that persons of African and Hispanic descent were stopped more frequently than whites, even after controlling for precinct variability and race-specific estimates of crime participation.</p>
</blockquote>
<p>This application involves both non-normal data (number of stops by ethnic group can be modeled as a Poisson response) and multilevel data (number of stops within precincts will likely be correlated due to characteristics of the precinct population). This type of analysis will be the last type you encounter, generalized linear multilevel modeling, as addressed in Chapter <a href="#ch-glmm"><strong>??</strong></a>.</p>
</div>
</div>
<div id="exercises" class="section level2">
<h2><span class="header-section-number">1.8</span> Exercises</h2>
<div id="conceptual-exercises" class="section level3">
<h3><span class="header-section-number">1.8.1</span> Conceptual Exercises</h3>
<p><strong>Applications that do not violate the OLS assumptions for inference</strong>. Identify the response and explanatory variable(s) for each problem. Write the OLS assumptions for inference in the context of each study.</p>
<ol style="list-style-type: decimal">
<li>Researchers record the number of cricket chirps per minute and temperature during that time to investigate whether the number of chirps varies with the temperature.</li>
<li>A random selection of women aged 20-24 years are selected and their shoe size is used to predict their height</li>
</ol>
<p><strong>Applications that do violate the OLS assumptions for inference</strong>. All of the examples in this section have at least one violation of the OLS assumptions for inference. Begin by identifying the response and explanatory variables. Write the assumptions in the context of each study as if there were no violations. Then identify which assumptions are likely to be invalid. Explain your answer in the context of the study.</p>
<ol style="list-style-type: decimal">
<li><strong>Fragile Family.</strong> Researchers using the Fragile Family data are attempting to see if socioeconomic status and parental stability are predictive of low birthweight. Note that a newborn is classified as having a low birthweight with potentially hazardous consequences if their birthweight is less than 2500 gm, hence our response is binary: 1 for low birthweight, and 0 when the birthweight is not low.</li>
<li><strong>Computer Mating.</strong> eHarmony.com claims that it is responsible for 2% of the marriages in the U.S. eHarmony.com researchers want to determine which factors predict a successful match.</li>
<li><strong>Clinical Trial I.</strong> A Phase I clinical trial is designed to compare the number of patients getting relief at different dose levels.</li>
<li><strong>Clinical Trial II.</strong> A randomized clinical trial investigated postnatal depression and the use of an estrogen patch (Gregoire et al. 1996). Patients were randomly assigned to either use the patch or not. Depression scores were recorded on 6 different visits.</li>
<li><strong>Schizophrenia.</strong> Thara et al. (1994) investigated the question ``Does the course of schizophrenia differ for patients with early and late onset?&quot; Doctors assess patients’ mood using a 0-100 scale at each of 10 visits.</li>
<li><strong>Pollution and traffic.</strong> Minnesota Pollution Control Agency is interested in using traffic volume data to generate predictions of particulate distributions as measured in counts per cubic feet.</li>
<li><strong>Canoes and zip codes.</strong> For each of over 27,000 overnight permits for the Boundary Water Canoe area, the zip code for the group leader has been translated to the distance traveled and socioeconomic data. This data is used to create a model for the number trips made per zip code.</li>
<li><strong>Factors affecting beginning salary.</strong> As part of a study investigating possible gender discrimination in beginning salaries at a particular company, researchers study the relationship between years of education and beginning salary among company employees.</li>
<li><strong>Elephant mating.</strong> Researchers are interested in how elephant age affects mating patterns among males. In particular, do older elephants have greater mating success, and is there an optimal age for mating among males? Data collected includes, for each elephant, age and number of matings in a given year.</li>
<li><strong>Mandatory holds.</strong> A mandatory hold is a revocation of a patient’s right to leave a psychiatric facility without consent of the monitoring physician. These holds have been classified as “good” or “bad”, and we want to know if certain groups (Native Americans, poor, uninsured) have rates of good holds that differ from others, after controlling for important factors in the hold decision (reason for hold, alcohol/drug use, physical threats, etc.).</li>
<li><strong>Beating the blues.</strong> Can an interactive multimedia program of behavioral therapy help depression patients more than standard care? 167 subjects were randomized into one of two treatment groups, and their depression levels were assessed monthly for 8 months (2 active treatment and 6 follow-up). We want to compare treatments while controlling for baseline depression, concomitant drug therapy, etc.</li>
<li><strong>Basketball referee bias.</strong> Do college basketball referees tend to even out the foul calls on the two teams over the course of a game? For example, if several more fouls have been called on the visitors at a certain point in the game, does it become more likely that the next foul will be called on the home team? And do these chances depend on the score of the game, the size of the crowd, or the referees working the game?</li>
<li><strong>No Child Left Behind.</strong> The Minnesota Department of Education makes all test scores publically available at the classroom level; for each test type (math, science, reading), scores are available by grade within school within district. School boards are interested in mining this data to learn about how their district is performing, relative to other districts, after adjusting for demographic information at the school and district levels (e.g., percent free and reduced lunch, percent non-white, percent special education). They are also interested in quantifying the impact of potential policy changes, such as increased funding, decreased class size, additional charter schools, etc.</li>
</ol>
<p><strong>Kentucky Derby.</strong> The next set of questions is related to the Kentucky Derby case study from this chapter.</p>
<ol style="list-style-type: decimal">
<li>Discuss the pros and cons of using side-by-side boxplots vs. stacked histograms to illustrate the relationships between year and track condition in Figure <a href="ch-MLRreview.html#fig:bivariate">1.3</a>.</li>
<li>Why is a scatterplot more informative than a correlation coefficient to describe the relationship between speed of the winning horse and year in Figure <a href="ch-MLRreview.html#fig:bivariate">1.3</a>.</li>
<li>How might you incorporate a fourth variable, say number of starters, into Figure <a href="ch-logreg.html#fig:coded">6.2</a>?</li>
<li>Explain why <span class="math inline">\(\epsilon_i\)</span> in Equation <a href="ch-MLRreview.html#eq:model1">(1.1)</a> measures the vertical distance from a data point to the regression line.</li>
<li>In the first t-test in Section <a href="ch-MLRreview.html#SLRcontinuous">1.6.1</a> (t = 11.251 for <span class="math inline">\(H_0:\beta_1 = 0\)</span>), notice that <span class="math inline">\(t = \frac{\hat{\beta_1}}{SE(\beta_1)} = \frac{.026}{.0023} = 11.251\)</span>. Why is the t-test based on the ratio of the estimated slope to its standard error?</li>
<li>In Equation <a href="ch-MLRreview.html#eq:model3">(1.4)</a>, explain why the t-test corresponding to <span class="math inline">\(\beta_{1}\)</span> is equivalent to an independent-samples t-test under equal variances. Why is the equal variance assumption needed?</li>
<li>When interpreting <span class="math inline">\(\beta_2\)</span> in Equation <a href="ch-MLRreview.html#eq:model4">(1.5)</a>, why do we have to be careful to say <em>for a fixed year</em> or <em>after adjusting for year</em>? Is it wrong to leave a qualifier like that off?</li>
<li>Interpret in context a 95% confidence interval for <span class="math inline">\(\beta_0\)</span> in Model 4.</li>
<li>State (in context) the result of a t-test for <span class="math inline">\(\beta_1\)</span> in Model 4.</li>
<li>Why is there no <span class="math inline">\(\epsilon_i\)</span> term in Equation <a href="ch-MLRreview.html#eq:model5est">(1.7)</a>?</li>
<li>If you considered the interaction between two continuous variables (like <code>yearnew</code> and <code>starters</code>), how would you interpret the coefficient for that interaction term?<br />
</li>
<li>Interpret (in context) the OLS estimates for <span class="math inline">\(\beta_3\)</span> and <span class="math inline">\(\beta_5\)</span> in Equation <a href="ch-MLRreview.html#eq:model0">(1.8)</a>.</li>
</ol>
<p><strong>Moneyball.</strong> In a 2011 <a href="http://thesportjournal.org/article/a-new-test-of-the-moneyball-hypothesis/">article</a> in <em>The Sport Journal</em>, Farrar and Bruggink attempt to show that Major League Baseball general managers did not immediately embrace the findings of Michael Lewis’s 2003 <em>Moneyball</em> book. They contend that players’ on-base percentage remained relatively undercompensated compared to slugging percentage three years after the book came out. Two regression models are described: a Team Run Production Model and a Player Salary Model.</p>
<ol style="list-style-type: decimal">
<li>Discuss potential concerns (if any) with the LINE assumptions for linear regression in each model.</li>
<li>In Table 3, the authors contend that Model 1 is better than Model 3. Could you argue that Model 3 is actually better? How could you run a formal hypothesis test comparing Model 1 to Model 3?<br />
</li>
<li>If authors had chosen Model 3 in Table 3 with the two interaction terms, how would that affect their final analysis, in which they compare coefficients of slugging and on-base percentage? (Hint: write out interpretations for the two interaction coefficients—the first one should be NL:OBP and the second one should be NL:SLG)</li>
<li>The authors write that “It should also be noted that the runs scored equation fit is better than the one Hakes and Sauer have for their winning equation.” What do you think they mean by this statement? Why might this comparison not be relevant?</li>
<li>In Table 4, Model 1 has a higher adjusted <span class="math inline">\(R^2\)</span> than Model 2, yet the extra term in Model 1 (an indicator value for the National League) is not significant at the 5% level. Explain how this is possible.</li>
<li>What limits does this paper have on providing guidance to baseball decision makers?</li>
</ol>
</div>
<div id="guided-exercise" class="section level3">
<h3><span class="header-section-number">1.8.2</span> Guided Exercise</h3>
<p><strong>Gender discrimination in bank salaries</strong>. In the 1970’s, Harris Trust was sued for gender discrimination in the salaries it paid its employees. One approach to addressing this issue was to examine the starting salaries of all skilled, entry-level clerical workers between 1965 and 1975. The following variables were collected for each worker (Data and scenario from <em>The Statistical Sleuth</em> (Ramsey F, Schafer D; 2013)):</p>
<ul>
<li><code>bsal</code> = beginning salary (annual salary at time of hire)</li>
<li><code>sal77</code> = annual salary in 1977</li>
<li><code>sex</code> = MALE or FEMALE</li>
<li><code>senior</code> = months since hired</li>
<li><code>age</code> = age in months</li>
<li><code>educ</code> = years of education</li>
<li><code>exper</code> = months of prior work experience</li>
</ul>
<p>You may want to start by creating an indicator variable based on <code>sex</code>.</p>
<ol style="list-style-type: decimal">
<li>Identify observational units, the response variable, and explanatory variables.</li>
<li>The mean starting salary of male workers ($5957) was 16% higher than the mean starting salary of female workers ($5139). Confirm these mean salaries. Is this enough evidence to conclude gender discrimination exists? If not, what further evidence would you need?</li>
<li>How would you expect age, experience, and education to be related to starting salary? Generate appropriate exploratory plots; are the relationships as you expected? What implications does this have for modeling?</li>
<li>Why might it be important to control for seniority (number of years with the bank) if we are only concerned with the salary when the worker started?</li>
<li>By referring to exploratory plots and summary statistics, are any explanatory variables (including sex) closely related to each other? What implications does this have for modeling?</li>
<li>Fit a simple linear regression model with starting salary as the response and experience as the sole explanatory variable (Model 1). Interpret the intercept and slope of this model; also interpret the R-squared value. Is there a significant relationship between experience and starting salary?<br />
</li>
<li>Does Model 1 meet all linear regression assumptions? List each assumption and how you decided if it was met or not.</li>
<li>Is a model with all 4 confounding variables (Model 2, with <code>senior</code>, <code>educ</code>, <code>exper</code>, and <code>age</code>) better than a model with just experience (Model 1)? Justify with an appropriate significance test in addition to summary statistics of model performance.</li>
<li>You should have noticed that the term for age was not significant in Model 2. What does this imply about age and about future modeling steps?</li>
<li>Generate an appropriate coded scatterplot to examine a potential age-by-experience interaction. How would you describe the nature of this interaction?</li>
<li>A potential final model (Model 3) would contain terms for seniority, education, and experience in addition to sex. Does this model meet all regression assumptions? State a 95% confidence interval for sex and interpret this interval carefully in the context of the problem.</li>
<li>Based on Model 3, what conclusions can be drawn about gender discrimination at Harris Trust? Do these conclusions have to be qualified at all, or are they pretty clear cut?</li>
<li>Often salary data is logged before analysis. Would you recommend logging starting salary in this study? Support your decision analytically.</li>
<li>Regardless of your answer to the previous question, provide an interpretation for the coefficient for the male coefficient in a modified Model 3 after logging starting salary.</li>
<li>Build your own final model for this study and justify the selection of your final model. You might consider interactions with gender, since those terms could show that discrimination is stronger among certain workers. Based on your final model, do you find evidence of gender discrimination at Harris Trust?</li>
</ol>
</div>
<div id="open-ended-exercises" class="section level3">
<h3><span class="header-section-number">1.8.3</span> Open-ended Exercises</h3>
<ol style="list-style-type: decimal">
<li><strong>Pace of Life</strong>. Some believe that individuals with a constant sense of time urgency (often called type-A behavior) are more susceptible to heart disease than are more relaxed individuals. Although most studies of this issue have focused on individuals, some psychologists have investigated geographical areas. They considered the relationship of city-wide heart disease rates and general measures of the pace of life in the city. For each region of the United States (Northeast, Midwest, South, and West) they selected three large metropolitan areas, three medium-size cities, and three smaller cities. In each city they measured three indicators of the pace of life. The variable <code>WALK</code> is the walking speed of pedestrians over a distance of 60 feet during business hours on a clear summer day along a main downtown street. <code>BANK</code> is the average time a sample of bank clerks takes to make change for two $20 bills or to give $20 bills for change. The variable <code>TALK</code> was obtained by recording responses of postal clerks explaining the difference between regular, certified, and insured mail and dividing the total number of syllables by the time of their response. The researchers also obtained the age-adjusted death rates from ischemic heart disease (a decreased flow of blood to the heart) for each city (<code>HEART</code>). (Data from R. V. Levine, “The Pace of Life,” <em>American Scientist</em> 78 (1990): 450-9 via <em>The Statistical Sleuth</em>) The variables have been standardized on a 0-40 scale, so there are no units of measurement involved. Build a regression model for explaining heart disease rates as a function of pace of life. Can we conclude that type-A individuals are more susceptible to heart disease?</li>
<li><strong>Waitress Tips</strong>. A student collected data from a restaurant where she was a waitress. The student was interested in learning under what conditions a waitress can expect the largest tips—for example: At dinner time or late at night? From younger or older patrons? From patrons receiving free meals? From patrons drinking alcohol? From patrons tipping with cash or credit? And should tip amount be measured as total dollar amount or as a percentage? Here is a quick description of the variables collected:</li>
</ol>
<ul>
<li><code>Day</code> = day of the week</li>
<li><code>Meal</code> = time of day (Lunch, Dinner, Late Night)</li>
<li><code>Payment</code> = how bill was paid (Credit, Cash, Credit with Cash tip)</li>
<li><code>Party</code> = number of people in the party</li>
<li><code>Age</code> = age category of person paying the bill (Yadult, Middle, SenCit)</li>
<li><code>GiftCard</code> = was gift card used?</li>
<li><code>Comps</code> = was part of the meal complimentary?</li>
<li><code>Alcohol</code> = was alcohol purchased?</li>
<li><code>Bday</code> = was a free birthday meal or treat given?</li>
<li><code>Bill</code> = total size of the bill</li>
<li><code>W.tip</code> = total amount paid (bill plus tip)</li>
<li><code>Tip</code> = amount of the tip</li>
<li><code>Tip.Percentage</code> = proportion of the bill represented by the tip</li>
</ul>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch-beyondmost.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
