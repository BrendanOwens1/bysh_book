[
["index.html", "Broadening Your Statistical Horizons Preface", " Broadening Your Statistical Horizons Generalized Linear Models and Multilevel Models J. Legler and P. Roback 2018-02-02 Preface This is the very first part of the book. "],
["ch-MLRreview.html", "Chapter 1 Review of Multiple Linear Regression 1.1 Learning Objectives 1.2 Introduction 1.3 Ordinary Least Squares (OLS) Assumptions 1.4 Case Study: Kentucky Derby 1.5 Initial Exploratory Analyses 1.6 Multiple linear regression modeling 1.7 Preview 1.8 Exercises", " Chapter 1 Review of Multiple Linear Regression 1.1 Learning Objectives After finishing this chapter, you should be able to: Identify cases where ordinary least squares (OLS) assumptions are violated. Generate exploratory data analysis plots and summary statistics. Use residual diagnostics to examine OLS assumptions. Interpret parameters and associated tests and intervals from multiple regression models. Understand the basic ideas behind bootstrapped confidence intervals. 1.2 Introduction Ecologists count species, criminologists count arrests, and cancer specialists count cases. Political scientists seek to explain who is a Democrat, pre-med students are curious about who gets in to medical school, and sociologists study which people get tattoos. In the first case, ecologists, criminologists and cancer specialists are concerned about outcomes which are counts. The political scientists’, pre-med students’ and sociologists’ interest centers on binary responses: Democrat or not, accepted or not, and tattooed or not. We can model these non-Gaussian (non-normal) responses in a more natural way by fitting generalized linear models (GLMs) as opposed to using to ordinary least squares (OLS) models. When models are fit to data using OLS, inferences are possible using traditional statistical theory under certain conditions: if we can assume that there is a linear relationship between the response (Y) and an explanatory variable (X), the observations are independent of one another, the responses are approximately normal for each level of the X, and the variation in the responses is the same for each level of X. If we intend to make inferences using GLMs, necessary assumptions are different. First, we will not be constrained by the normality assumption. When conditions are met, GLMs can accommodate non-normal responses such as the counts and binary data in our preceding examples. While the observations must still be independent of one another, the variance in Y at each level of X need not be equal nor does the assumption of linearity between Y and X need to be plausible. However GLMs cannot be used for models in the following circumstances: medical researchers collect data on patients in clinical trials weekly for 6 months; rat dams are injected with teratogenic substances and their offspring are monitored for defects; and, musicians’ performance anxiety is recorded for several performances. Each of these examples involves correlated data: the same patient’s outcomes are more likely to be similar from week-to-week than outcomes from different patients; litter mates are more likely to suffer defects at similar rates in contrast to unrelated rat pups; and, a musician’s anxiety is more similar from performance to performance than it is with other musicians. Each of these examples violate the independence assumption of simpler linear models for OLS or GLM inference. The Generalized Linear Models in the book’s title extends OLS methods you may have seen in linear regression to handle responses that are non-normal in addition to normal outcomes. The Multilevel Methods will allow us to create models for situations where the observations are not independent of one another. Overall, these approaches will permit us to get much more out of data and may be more faithful to the actual data structure than models based on ordinary least squares. These models will broaden your statistical horizons. In order to understand the motivation for handling violations of assumptions, it is helpful to be able to recognize the model assumptions for inference with OLS in the context of different studies. While linearity is sufficient for fitting an OLS model, in order to make inferences and predictions the observations must also be independent, the responses should be approximately normal at each level of the predictors, and the standard deviation of the responses at each level of the predictors should be approximately equal. After examining circumstances where inference with OLS modeling is appropriate, we will look for violations of these assumptions in other sets of circumstances. These are settings where we may be able to use the methods of this text. We’ve kept the examples in the exposition simple to fix ideas. There are exercises which describe more realistic and complex studies. 1.3 Ordinary Least Squares (OLS) Assumptions Figure 1.1: Ordinary least squares assumptions Recall that making inferences or predictions with models fit using ordinary least squares (OLS) requires that the following assumptions be tenable. The acronym LINE can be used to recall the assumptions required for making inferences and predictions with models based on OLS. If we consider a simple linear regression, with just a single predictor X, then: (L) there is a linear relationship between the mean response (Y) and the explanatory variable (X), (I) the errors are independent—there’s no connection between how far any two points lie from the regression line, (N) the responses are normally distributed at each level of X, and (E) the variance or, equivalently, the standard deviation of the responses is equal for all levels of X. These assumptions are depicted in Figure 1.1. (L) The mean value for Y at each level of X falls on the regression line. (I) We’ll need to check the design of the study to determine if the errors (distances from the line) are independent of one another. (N) At each level of X, the values for Y are normally distributed. (E) The spread in the Y’s for each level of X is the same. 1.3.1 Cases that do not violate the OLS assumptions for inference It can be argued that the following studies do not violate the OLS assumptions for inference. We begin by identifying the response and the explanatory variables followed by describing each of the LINE assumptions in the context of the study, commenting on possible problems with the assumptions. Reaction times and car radios. A researcher suspects that loud music can affect how quickly drivers react. She randomly selects drivers to drive the same stretch of road with varying levels of music volume. Stopping distances for each driver are measured along with the decibel level of the music on their car radio. Response variable: Reaction time Explanatory variable: Decibel level of music The OLS assumptions for inference would apply if: L: The mean reaction time is linearly related to decibel level of the music. I: Stopping distances are independent. The random selection of drivers should assure independence. N: The stopping distances for a given decibel level of music vary and are normally distributed. E: The variation in stopping distances should be approximately the same for each decibel level of music. There are potential problems with the linearity and equal standard deviation assumptions. For example, if there is threshold for the volume of music where the effect on reaction times remains the same, mean reaction times would not be a linear function of music. Another problem may occur if a few subjects at each decibel level took a really long time to react. In this case, reaction times would be right skewed and the normality assumption would be violated. Often we can think of circumstances where the OLS assumptions may be suspect. Later in this chapter we will describe plots which can help diagnose issues with OLS assumptions. Crop yield and rainfall. The yield of wheat per acre for the month of July is thought to be related to the rainfall. A researcher randomly selects acres of wheat and records the rainfall and bushels of wheat per acre. Response variable: Yield of wheat measured in bushels per acre for July Explanatory variable: Rainfall measured in inches for July L: The mean yield per acre is linearly related to rainfall. I: Fields’ yields are independent; knowing one (X, Y) pair does not provide information about another. N: The yields for a given amount of rainfall are normally distributed. E: The standard deviation of yields is approximately the same for each rainfall level. Again we may encounter problems with the linearity assumption if mean yields increase initially as the amount of rainfall increases after which excess rainfall begins to ruin crop yield. The random selection of fields should assure independence if fields are not close to one another. Heights of sons and fathers. Galton suspected that a son’s height could be predicted using the father’s height. He collected observations on heights of fathers and their firstborn sons. Response variable: Height of the firstborn son Explanatory variable: Height of the father L: The mean height of firstborn sons is linearly related to heights of fathers. I: The height of one firstborn son is independent of the height of other firstborn sons in the study. This would be the case if firstborn sons were randomly selected. N: The heights of firstborn sons for a given fathers’ height are normally distributed. E: The standard deviation of firstborn sons’ heights at a given father’s height are the same. Heights and other similar measurements are often normally distributed. There would be a problem with the independence assumption if multiple sons from the same family were selected. Or, there would be a problem with equal variance if sons of tall fathers had much more variety in their heights than sons of shorter fathers. 1.3.2 Cases where the OLS assumptions for inference are violated Grades and studying. Is the time spent studying predictive of success on an exam? The time spent studying for an exam, in hours, and success, measured as Pass or Fail, are recorded for randomly selected students. Response variable: Exam outcome (Pass or Fail) Explanatory variable: Time spent studying (in hours) Here the response is a binary outcome which violates the OLS assumption of a normally distribution response at each level of X. In Chapter 6, we will see logistic regression which is more suitable for models with binary responses. Income and family size. Do wealthy families tend to have fewer children compared to lower income families? Annual income and family size are recorded for a random sample of families. Response variable: Family size, number of children Explanatory variable: Annual income, in dollars Family size is a count taking on integer values from 0 to (technically) no upper bound. The normality assumption may be problematic again because the distribution of family size is likely to be skewed, with more families having one or two children and only a few with a much larger number of children. Both of these concerns lead us to question the validity of the normality assumption. Study design should also specify that families are done adding children to their family. Exercise, weight, and sex. Investigators collected the weight, sex, and amount of exercise for a random sample of college students. Response variable: Weight Explanatory variables: Sex and hours spent exercising in a typical week With two predictors, the assumptions now apply to the combination of sex and exercise. For example, the linearity assumption implies that there is a linear relationship in mean weight and amount of exercise for males and, similarly, a linear relationship in mean weight and amount of exercise for females. This data may not be appropriate for OLS modeling because the standard deviation in weight for students who do not exercise for each sex is likely to be considerably greater than the standard deviation in weight for students who follow an exercise regime. We can assess this potential problem by plotting weight by amount of exercise for males and females separately. There may also be a problem with the independence assumption because there is no indication that the subjects were randomly selected. There may be subgroups of subjects likely to be more similar, e.g. selecting students at a gym and others in a TV lounge. Surgery Outcome and Patient Age. Medical researchers investigated the outcome of a particular surgery for patients with comparable stages of disease but different ages. The ten hospitals in the study had at least two surgeons performing the surgery of interest. Patients were randomly selected for each surgeon at each hospital. The surgery outcome was recorded on a scale of one to ten. Response variable: Surgery outcome, scale 1-10 Explanatory variable: Patient age, in years Outcomes for patients operated on by the same surgeon are more likely to be similar and have similar results. For example, if surgeons’ skills differ or if their criteria for selecting patients for surgery vary, individual surgeons may tend to have better or worse outcomes, and patient outcomes will be dependent on surgeon. Furthermore, outcomes at one hospital may be more similar possibly due to factors associated with different patient populations. The very structure of this data suggests that the independence assumption will be violated. Multilevel models will explicitly take this structure into account for a proper analysis of this study’s results. While we identified possible violations of OLS assumptions for inference for each of the examples in this section, there may be violations of the other assumptions that we have not pointed out. Prior to reading this book, you have presumably learned some ways to handle these violations such as applying variance stabilizing transformations or logging responses, but you will discover other models in this text that may be more appropriate for the violations we have presented. 1.4 Case Study: Kentucky Derby Before diving into generalized linear models and multilevel modeling, we review key ideas from multiple linear regression using an example from horse racing. The Kentucky Derby is a 1.25 mile horse race held annually at the Churchill Downs race track in Louisville, Kentucky. Our data set contains the year of the race, the winning horse (winner), the condition of the track, the average speed (in feet per second) of the winner, and the number of starters (field size, or horses who raced) for the years 1896-2017. The track condition has been grouped into three categories: fast, good (which includes the official designations “good” and “dusty”), and slow (which includes the designations “slow”, “heavy”, “muddy”, and “sloppy”). We would like to use OLS linear regression techniques to model the speed of the winning horse as a function of track condition, field size, and trends over time. 1.5 Initial Exploratory Analyses 1.5.1 Data Organization The first five and last five rows from our data set are illustrated in Table 1.1. Note that, in certain cases, we created new variables from existing ones: fast is an indicator variable, taking the value 1 for races run on fast tracks, and 0 for races run under other conditions, good is another indicator variable, taking the value 1 for races run under good conditions, and 0 for races run under other conditions, yearnew is a centered variable, where we measure the number of years since 1896, and fastfactor replaces fast = 0 with the description “not fast”, and fast = 1 with the description “fast”. Changing a numeric categorical variable to descriptive phrases can make plot legends more meaningful. Table 1.1: The first five and the last five observations from the Kentucky Derby case study. year winner condition speed starters fast good yearnew fastfactor 1896 Ben Brush good 51.66 8 0 1 0 not fast 1897 Typhoon II slow 49.81 6 0 0 1 not fast 1898 Plaudit good 51.16 4 0 1 2 not fast 1899 Manuel fast 50.00 5 1 0 3 fast 1900 Lieut. Gibson fast 52.28 7 1 0 4 fast 2013 Orb slow 53.71 19 0 0 117 not fast 2014 California Chrome fast 53.37 19 1 0 118 fast 2015 American Pharoah fast 53.65 18 1 0 119 fast 2016 Nyquist fast 54.41 20 1 0 120 fast 2017 Always Dreaming fast 53.40 20 1 0 121 fast 1.5.2 Univariate Summaries With any statistical analysis, our first task is to explore the data, examining distributions of individual responses and predictors using graphical and numerical summaries, and beginning to discover relationships between variables. This should always be done before any model fitting! We must understand our data thoroughly before doing anything else. First, we will examine each response variable and potential covariate individually. Continuous variables can be summarized using histograms and statistics indicating center and spread; categorical variables can be summarized with tables and possibly bar charts. Figure 1.2: Histograms of key continuous variables. Plot (a) shows winning speeds, while plot (b) shows the number of starters. In Figure 1.2(a), we see that the primary response, winning speed, follows a distribution with a slight left skew, with a large number of horses winning with speeds between 53-55 feet per second. Plot (b) shows that the number of starters is mainly distributed between 5 and 20, with the largest number of races having between 15 and 20 starters. The primary categorical explanatory variable is track condition, where 88 (72%) of the 122 races were run under fast conditions, 10 (8%) under good conditions, and 24 (20%) under slow conditions. 1.5.3 Bivariate Summaries The next step in an initial exploratory analysis is the examination of numerical and graphical summaries of relationships between model covariates and responses. Figure 1.3 is densely packed with illustrations of bivariate relationships. The relationship between two continuous variables is depicted with scatterplots below the diagonal and correlation coefficients above the diagonal. Here, we see that higher winning speeds are associated with more recent years, while the relationship between winning speed and number of starters is less clear cut. We also see a somewhat strong correlation between year and number of starters—we should be aware of highly correlated explanatory variables whose contributions might overlap too much. Figure 1.3: Relationships between pairs of variables in the Kentucky Derby data set Relationships between categorical variables like track condition and continuous variables can be illustrated with side-by-side boxplots as in the top row, or with stacked histograms as in the first column. As expected, we see evidence of higher speeds on fast tracks and also a tendency for recent years to have more fast conditions. These observed trends can be supported with summary statistics generated by subgroup. For instance, the mean speed under fast conditions is 53.6 feet per second, compared to 52.7 ft/s under good conditions and 51.7 ft/s under slow conditions. Variability in winning speeds, however, is greatest under slow conditions (SD = 1.36 ft/s) and least under fast conditions (0.94 ft/s). Finally, notice that the diagonal illustrates the distribution of individual variables, using density curves for continuous variables and a bar chart for categorical variables. Trends observed in the first and third diagonal entries match trends observed in Figure 1.2. By using shape or color or other attributes, we can incorporate the effect of a third or even fourth variable into the scatterplots of Figure 1.3. For example, in the coded scatterplot of Figure 6.2 we see that speeds are generally faster under fast conditions, but the rate of increasing speed over time is greater under good or slow conditions. Figure 1.4: Linear trends in winning speeds over time, presented separately for fast conditions vs. good or slow conditions Of course, any graphical analysis is exploratory, and any notable trends at this stage should be checked through formal modeling. At this point, a statistician begins to ask familiar questions such as: are winning speeds increasing in a linear fashion? does the rate of increase in winning speed depend on track condition or number of starters? after accounting for other explanatory variables, is greater field size (number of starters) associated with faster winning speeds (because more horses in the field means a greater chance one horse will run a very fast time) or slower winning speeds (because horses are more likely to bump into each other or crowd each others’ attempts to run at full gait)? are any of these associations statistically significant? how well can we predict the winning speed in the Kentucky Derby? As you might expect, answers to these questions will arise from proper consideration of variability and properly identified statistical models. 1.6 Multiple linear regression modeling 1.6.1 Simple linear regression with a continuous predictor We will begin by modeling the winning speed as a function of time; for example, have winning speeds increased at a constant rate since 1896? For this initial model, let \\(Y_{i}\\) be the speed of the winning horse in year \\(i\\). Then, we might consider Model 1: \\[\\begin{equation} Y_{i}=\\beta_{0}+\\beta_{1}\\textstyle{Year}_{i}+\\epsilon_{i} \\textrm{ where } \\epsilon_{i}\\sim N(0,\\sigma^2). \\tag{1.1} \\end{equation}\\] In this case, \\(\\beta_{0}\\) represents the true intercept—the expected winning speed during Year 0. \\(\\beta_{1}\\) represents the true slope—the expected increase in winning speed from one year to the next, assuming the rate of increase is linear (i.e., constant since 1896). Finally, the error (\\(\\epsilon_{i}\\)) terms represent the deviations of the actual winning speed in Year \\(i\\) (\\(Y_i\\)) from the expected scores under this model (\\(\\beta_{0} + \\beta_{1}\\textstyle{Year}_{i}\\))—the part of a horse’s winning speed that is not explained by a linear trend over time. The variability in these deviations from the regression model is denoted by \\(\\sigma^2\\). The parameters in this model (\\(\\beta_{0}\\), \\(\\beta_{1}\\), and \\(\\sigma^2\\)) can be estimated through OLS methods; we will use hats to denote estimates of population parameters based on empirical data. Values for \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) are selected to minimize the sum of squared residuals, where a residual is simply the observed prediction error—the actual winning speed for a given year minus the winning speed predicted by the model. In the notation of this section, Predicted speed: \\(\\hat{Y}_{i}=\\hat{\\beta}_{0}+\\hat{\\beta}_{1}\\textstyle{Year}_{i}\\) Residual (estimated error): \\(\\hat{\\epsilon}_{i}=Y_{i} - \\hat{Y}_{i}\\) Estimated variance of points around the line: \\(\\hat{\\sigma}^2 = \\sum \\hat{\\epsilon}^2_{i} / (n-2)\\) Using Kentucky Derby data, we estimate \\(\\hat{\\beta}_{0}=2.05\\), \\(\\hat{\\beta}_{1}=0.026\\), and \\(\\hat{\\sigma}=0.90\\). Thus, according to our simple linear regression model, winning horses of the Kentucky Derby have an estimated winning speed of 2.05 ft/s in Year 0 (almost 2000 years ago!), and the winning speed improves by an estimated 0.026 ft/s every year. With an \\(R^2\\) of 0.513, the regression model explains a moderate amount (51.3%) of the year-to-year variability in winning speeds, and the trend toward a linear rate of improvement each year is statistically significant at the 0.05 level (t(120) = 11.251, p &lt; .001). lm(formula = speed ~ year, data = derby.df) Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.053473 4.543754 0.452 0.652 year 0.026126 0.002322 11.251 &lt;2e-16 *** --- Residual standard error: 0.9032 on 120 degrees of freedom Multiple R-squared: 0.5134, Adjusted R-squared: 0.5093 You may have noticed in Model 1 that the intercept has little meaning in context, since it estimates a winning speed in Year 0, when the first Kentucky Derby run at the current distance (1.25 miles) was in 1896. One way to create more meaningful parameters is through centering. In this case, we could create a centered year variable by substracting 1896 from each year for Model 2: \\[\\begin{equation} Y_{i}=\\beta_{0}+\\beta_{1}\\textstyle{Yearnew}_{i}+\\epsilon_{i} \\textrm{ where } \\epsilon_{i}\\sim N(0,\\sigma^2) \\textrm{ and } \\textstyle{Yearnew}=\\textstyle{Year}-1896. \\tag{1.2} \\end{equation}\\] Note that the only thing that changes from Model 1 to Model 2 is the estimated intercept; \\(\\hat{\\beta}_{1}\\), \\(R^2\\), and \\(\\hat{\\sigma}\\) all remain exactly the same. Now \\(\\hat{\\beta}_{0}\\) tells us that the estimated winning speed in 1896 is 51.59 ft/s, but estimates of the linear rate of improvement or the variability explained by the model remain the same. As Figure 1.5 shows, centering year has the effect of shifting the y-axis from year 0 to year 1896, but nothing else changes. lm(formula = speed ~ yearnew, data = derby.df) Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 51.588393 0.162549 317.37 &lt;2e-16 *** yearnew 0.026126 0.002322 11.25 &lt;2e-16 *** --- Residual standard error: 0.9032 on 120 degrees of freedom Multiple R-squared: 0.5134, Adjusted R-squared: 0.5093 Figure 1.5: Compare Model 1 (with intercept at 0) to Model 2 (with intercept at 1896) We should also attempt to verify that our LINE linear regression model assumptions fit for Model 2 if we want to make inferential statements (hypothesis tests or confidence intervals) about parameters or predictions. Most of these assumptions can be checked graphically using a set of residual plots as in Figure 1.6: The upper left plot, Residuals vs. Fitted, can be used to check the Linearity assumption. Residuals should be patternless around Y = 0; if not, there is a pattern in the data that is currently unaccounted for. The upper right plot, Normal Q-Q, can be used to check the Normality assumption. Deviations from a straight line indicate that the distribution of residuals does not conform to a theoretical normal curve. The lower left plot, Scale-Location, can be used to check the Equal Variance assumption. Positive or negative trends across the fitted values indicate variability that is not constant. The lower right plot, Residuals vs. Leverage, can be used to check for influential points. Points with high leverage (having unusual values of the predictors) and/or high absolute residuals can have an undue influence on estimates of model parameters. Figure 1.6: Residual plots for Model 2 In this case, the Residuals vs. Fitted plot indicates that a quadratic fit might be better than the linear fit of Model 2; other assumptions look reasonable. Influential points would be denoted by high values of Cook’s Distance; they would fall outside cutoff lines in the northeast or southeast section of the Residuals vs. Leverage plot. Since no cutoff lines are even noticeable, there are no potential influential points of concern. We recommend relying on graphical evidence for identifying regression model assumption violations, looking for highly obvious violations of assumptions before trying corrective actions. While some numerical tests have been devised for issues such as normality and influence, most of these tests are not very reliable, highly influenced by sample size and other factors. There is typically no residual plot, however, to evaluate the Independence assumption; evidence for lack of independence comes from knowing about the study design and methods of data collection. In this case, with a new field of horses each year, the assumption of independence is pretty reasonable. Based on residual diagnostics, we should test Model 2Q, in which a quadratic term is added to the linear term in Model 2. \\[\\begin{equation} Y_{i}=\\beta_{0}+\\beta_{1}\\textstyle{Yearnew}_{i}+\\beta_{2}\\textstyle{Yearnew}^2_{i}+\\epsilon_{i} \\textrm{ where } \\epsilon_{i}\\sim N(0,\\sigma^2). \\tag{1.3} \\end{equation}\\] This model could suggest, for example, that the rate of increase in winning speeds is slowing down over time. In fact, there is evidence that the quadratic model improves upon the linear model (see Figure 1.7). \\(R^2\\), the proportion of year-to-year variability in winning speeds explained by the model, has increased from 51.3% to 64.1%, and the pattern in the Residuals vs. Fitted plot of Figure 1.6 has disappeared in Figure 1.8, although normality is a little sketchier in the left tail, and the larger mass of points with fitted values near 54 appears to have slightly lower variability. The significantly negative coefficient for \\(\\beta_{2}\\) suggests that the rate of increase is indeed slowing in more recent years. Figure 1.7: Linear vs. quadratic fit lm(formula = speed ~ yearnew + yearnew2, data = derby.df) Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.059e+01 2.082e-01 243.010 &lt; 2e-16 *** yearnew 7.617e-02 7.950e-03 9.581 &lt; 2e-16 *** yearnew2 -4.136e-04 6.359e-05 -6.505 1.92e-09 *** --- Residual standard error: 0.779 on 119 degrees of freedom Multiple R-squared: 0.641, Adjusted R-squared: 0.635 Figure 1.8: Residual plots for Model 2Q 1.6.2 Simple linear regression with a binary predictor We also may want to include track condition as an explanatory variable. We could start by using fast as the lone predictor: Do winning speeds differ for fast and non-fast conditions? fast is considered an indicator variable—it takes on only the values 0 and 1, where 1 indicates presence of a certain attribute (like fast racing conditions). Since fast is numeric, we can use simple linear regression techniques to fit Model 3: \\[\\begin{equation} Y_{i}=\\beta_{0}+\\beta_{1}\\textstyle{Fast}_{i}+\\epsilon_{i} \\textrm{ where } \\epsilon_{i}\\sim N(0,\\sigma^2). \\tag{1.4} \\end{equation}\\] Here, it’s easy to see the meaning of our slope and intercept by writing out separate equations for the two conditions: Good or slow conditions (fast = 0) \\[\\begin{equation} Y_{i} = \\beta_{0}+\\epsilon_{i} \\end{equation}\\] Fast conditions (fast = 1) \\[\\begin{equation} Y_{i} = (\\beta_{0}+\\beta_{1})+\\epsilon_{i} \\end{equation}\\] \\(\\beta_{0}\\) is the expected winning speed under good or slow conditions, while \\(\\beta_{1}\\) is the difference between expected winning speeds under fast conditions vs. non-fast conditions. According to our fitted Model 3, the estimated winning speed under non-fast conditions is 52.0 ft/s, while mean winning speeds under fast conditions are estimated to be 1.6 ft/s higher. lm(formula = speed ~ fast, data = derby.df) Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 51.9938 0.1826 284.698 &lt; 2e-16 *** fast 1.6292 0.2150 7.577 8.17e-12 *** --- Residual standard error: 1.065 on 120 degrees of freedom Multiple R-squared: 0.3236, Adjusted R-squared: 0.318 You might be asking at this point: If we simply wanted to compare mean winning speeds under fast and non-fast conditions, why didn’t we just run a two-sample t-test? The answer is: we did! The t-test corresponding to \\(\\beta_{1}\\) is equivalent to an independent-samples t-test with under equal variances. Convince yourself that this is true, and that the equal variance assumption is needed. 1.6.3 Multiple linear regression with two predictors The beauty of the linear regression framework is that we can add additional explanatory variables in order to explain more variability in our response, obtain better and more precise predictions, and control for certain covariates while evaluating the effect of others. For example, we could consider adding yearnew to Model 3, which has the indicator variable fast as its only predictor. In this way, we would estimate the difference between winning speeds under fast and non-fast conditions after accounting for the effect of time. As we observed in Figure 1.3, recent years have tended to have more races under fast conditions, so Model 3 might overstate the effect of fast conditions because winning speeds have also increased over time. A model with terms for both year and track condition will estimate the difference between winning speeds under fast and non-fast conditions for a fixed year; for example, if it had rained in 2016 and turned the track muddy, how much would we have expected the winning speed to decrease? Our new model (Model 4) can be written: \\[\\begin{equation} Y_{i}=\\beta_{0}+\\beta_{1}\\textstyle{Yearnew}_{i}+\\beta_{2}\\textstyle{Fast}_{i}+\\epsilon_{i} \\textrm{ where } \\epsilon_{i}\\sim N(0,\\sigma^2). \\tag{1.5} \\end{equation}\\] and OLS provides the following parameter estimates: lm(formula = speed ~ yearnew + fast, data = derby.df) Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 50.917822 0.154602 329.35 &lt; 2e-16 *** yearnew 0.022583 0.001919 11.77 &lt; 2e-16 *** fast 1.226846 0.150721 8.14 4.39e-13 *** --- Residual standard error: 0.7269 on 119 degrees of freedom Multiple R-squared: 0.6874, Adjusted R-squared: 0.6822 Our new model estimates that winning speeds are, on average, 1.23 ft/s faster under fast conditions after accounting for time trends, which is down from an estimated 1.63 ft/s without accounting for time. It appears our original model (Model 3) may have overestimated the effect of fast conditions by conflating it with improvements over time. Through our new model, we also estimate that winning speeds increase by 0.023 ft/s per year, after accounting for track condition. This yearly effect is also smaller than the 0.026 ft/s per year we estimated in Model 1, without adjusting for track condition. Based on the \\(R^2\\) value, Model 4 explains 68.7% of the year-to-year variability in winning speeds, a noticeable increase over using either explanatory variable alone. 1.6.4 Inference in multiple linear regression So far we have been using linear regression for descriptive purposes, which is an important task. We are often interested in issues of statistical inference as well—determining if effects are statistically significant, quantifying uncertainty in effect size estimates with confidence intervals, and quantifying uncertainty in model predictions with prediction intervals. Under LINE assumptions, all of these inferential tasks can be completed with the help of the t-distribution and estimated standard errors. Here are examples of inferential statements based on Model 4: We can be 95% confident that average winning speeds under fast conditions are between 0.93 and 1.53 ft/s higher than under non-fast conditions, after accounting for the effect of year. Fast conditions lead to significantly faster winning speeds than non-fast conditions (t = 8.14 on 119 df, p &lt; .001), holding year constant. Based on our model, we can be 95% confident that the winning speed in 2017 under fast conditions will be between 53.4 and 56.3 ft/s. Note that Always Dreaming’s actual winning speed barely fit within this interval—the 2017 winning speed was a borderline outlier on the slow side. confint(model4) 2.5 % 97.5 % (Intercept) 50.61169473 51.22394836 yearnew 0.01878324 0.02638227 fast 0.92840273 1.52528902 new.data &lt;- data.frame(yearnew = 2017 - 1896, fast = 1) predict(model4, new = new.data, interval = &quot;prediction&quot;) fit lwr upr 1 54.87718 53.4143 56.34006 Remember that you must check LINE assumptions using the same residual plots as in Figure 1.6 to ensure that these inferential statements are valid. In cases when model assumptions are shaky, one alternative approach to statistical inference is bootstrapping; in fact, bootstrapping is a robust approach to statistical inference that we will use frequently throughout this book because of its power and flexibility. In bootstrapping, we use only the data we’ve collected and computing power to estimate the uncertainty surrounding our parameter estimates. Our primary assumption is that our original sample represents the larger population, and then we can learn about uncertainty in our parameter estimates through repeated samples (with replacement) from our original sample. If we wish to use bootstrapping to obtain confidence intervals for our coefficients in Model 4, we could follow these steps: take a (bootstrap) sample of 122 years of Derby data with replacement, so that some years will get sampled several times and others not at all. This is case resampling, so that all information from a given year (winning speed, track condition, number of starters) remains together. fit Model 4 to the bootstrap sample, saving \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\), and \\(\\hat{\\beta}_2\\). repeat the two steps above a large number of times (say 1000) the 1000 bootstrap estimates for each parameter can be plotted to show the bootstrap distribution (see Figure 1.9) a 95% confidence interval for each parameter can be found by taking the middle 95% of each bootstrap distribution—i.e., by picking off the 2.5 and 97.5 percentiles. This is called the percentile method. library(broom) bootregr &lt;- derby.df %&gt;% bootstrap(1000) %&gt;% do(tidy(lm(speed ~ yearnew + fast, .))) bootregr %&gt;% group_by(term) %&gt;% dplyr::summarize(low=quantile(estimate, .025), high=quantile(estimate, .975)) # A tibble: 3 x 3 term low high &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 50.6 51.3 2 fast 0.929 1.56 3 yearnew 0.0183 0.0268 Figure 1.9: Bootstrapped distributions for Model 4 coefficients In this case, we see that 95% bootstrap confidence intervals for \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\beta_2\\) are very similar to the normal-theory confidence intervals we found earlier. For example, the normal-theory confidence interval for the effect of fast tracks is 0.93 to 1.53 ft/s, while the analogous bootstrap confidence interval is 0.93 to 1.55 ft/s. There are many variations on this bootstrap procedure. For example, you could sample residuals rather than cases, or you could conduct a parametric bootstrap in which error terms are randomly chosen from a normal distribution. In addition, researchers have devised other ways of calculating confidence intervals besides the percentile method, including normality, studentized, and bias-corrected and accelerated (Hesterberg 2015; Efron and Tibshirani 1993; Davison and Hinkley 1997). We will focus on case resampling and percentile confidence intervals for now for their understandability and wide applicability. 1.6.5 Multiple linear regression with an interaction term Adding terms to form a multiple linear regression model as we did in Model 4 is a very powerful modeling tool, allowing us to account for multiple sources of uncertainty and to obtain more precise estimates of effect sizes after accounting for the effect of important covariates. One limitation of Model 4, however, is that we must assume that the effect of track condition has been the same for 122 years, or conversely that the yearly improvements in winning speeds are identical for all track conditions. To expand our modeling capabilities to allow the effect of one predictor to change depending on levels of a second predictor, we need to consider interaction terms. Amazingly, if we create a new variable by taking the product of yearnew and fast (i.e., the interaction between yearnew and fast), adding that variable into our model will have the desired effect. Thus, consider Model 5: \\[\\begin{equation} Y_{i}=\\beta_{0}+\\beta_{1}\\textstyle{Yearnew}_{i}+\\beta_{2}\\textstyle{Fast}_{i}+\\beta_{3}\\textstyle{Yearnew X Fast}_{i}+\\epsilon_{i} \\textrm{ where } \\epsilon_{i}\\sim N(0,\\sigma^2). \\tag{1.6} \\end{equation}\\] where OLS provides the following parameter estimates: lm(formula = speed ~ yearnew + fast + yearnew:fast, data = derby.df) Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 50.528629 0.205072 246.394 &lt; 2e-16 *** yearnew 0.030751 0.003471 8.859 9.84e-15 *** fast 1.833523 0.262175 6.994 1.73e-10 *** yearnew:fast -0.011490 0.004117 -2.791 0.00613 ** --- Residual standard error: 0.7071 on 118 degrees of freedom Multiple R-squared: 0.7068, Adjusted R-squared: 0.6993 According to OLS estimates, estimated winning speeds can be found by: \\[\\begin{equation} \\hat{Y}_{i}=50.53+0.031\\textstyle{Yearnew}_{i}+1.83\\textstyle{Fast}_{i}-0.011\\textstyle{Yearnew X Fast}_{i}. \\tag{1.7} \\end{equation}\\] Interpretations of model coefficients are most easily seen by writing out separate equations for fast and non-fast track conditions: \\[\\begin{eqnarray*} \\textstyle{Fast}=0: &amp; &amp; \\\\ \\hat{Y}_{i} &amp; = &amp; 50.53+0.031\\textstyle{Yearnew}_{i} \\\\ \\textstyle{Fast}=1: &amp; &amp; \\\\ \\hat{Y}_{i} &amp; = &amp; (50.53+1.83)+(0.031-0.011)\\textstyle{Yearnew}_{i} \\end{eqnarray*}\\] leading to the following interpretations for estimated model coefficients: \\(\\hat{\\beta}_{0} = 50.53\\). The expected winning speed in 1896 under non-fast conditions was 50.53 ft/s. \\(\\hat{\\beta}_{1} = 0.031\\). The expected yearly increase in winning speeds under non-fast conditions is 0.031 ft/s. \\(\\hat{\\beta}_{2} = 1.83\\). The winning speed in 1896 was expected to be 1.83 ft/s faster under fast conditions compared to non-fast conditions. \\(\\hat{\\beta}_{3} = -0.011\\). The expected yearly increase in winning speeds under fast conditions is 0.020 ft/s, which is 0.011 ft/s less than the expected annual increase under non-fast conditions. In fact, using interaction allows us to model the relationships we noticed in Figure 6.2, where both the intercept and slope describing the relationships between speed and year differ depending on whether track conditions were fast or not. Note that we interpret the coefficient for the interaction term by comparing slopes under fast and non-fast conditions; this produces a much more understandable interpretation for a reader than attempting to interpret the -0.011 directly. 1.6.6 Building a multiple linear regression model We now begin iterating toward a “final model” for these data, on which we will base conclusions. Typical features of a “final multiple linear regression model” include: explanatory variables allow one to address primary research questions explanatory variables control for important covariates potential interactions have been investigated variables are centered where interpretations can be enhanced unnecessary terms have been removed LINE assumptions and the presence of influential points have both been checked using residual plots the model tells a “persuasive story parsimoniously” Although the process of reporting and writing up research results often demands the selection of a sensible final model, it’s important to realize that (a) statisticians typically will examine and consider an entire taxonomy of models when formulating conclusions, and (b) different statisticians sometimes select different models as their “final model” for the same set of data. Choice of a “final model” depends on many factors, such as primary research questions, purpose of modeling, tradeoff between parsimony and quality of fitted model, underlying assumptions, etc. Modeling decisions should never be automated or made completely on the basis of statistical tests; subject area knowledge should always play a role in the modeling process. You should be able to defend any final model you select, but you should not feel pressured to find the one and only “correct model”, although most good models will lead to similar conclusions. Several tests and measures of model performance can be used when comparing different models for model building: \\(R^2\\) we have seen; it measures the variability in the response variable explained by the model. One problem is that \\(R^2\\) always increases with extra predictors, even if the predictors add very little information. adjusted \\(R^2\\). Adds a penalty for model complexity to \\(R^2\\) so that any increase in performance must outweigh the cost of additional complexity. We should ideally favor any model with higher adjusted \\(R^2\\), regardless of size, but the penalty for model complexity (additional terms) is fairly ad-hoc. AIC (Akaike Information Criterion). Again attempts to balance model performance with model complexity, with smaller AIC levels being preferable, regardless of model size. The BIC (Bayesian Information Criterion) is similar to the AIC, but with a greater penalty for additional model terms. extra sum of squares F test. This is a generalization of the t-test for individual model coefficients which can be used to perform significance tests on nested models, where one model is a reduced version of the other. For example, we could test whether our final model (below) really needs to adjust for track condition, which is comprised of indicators for both fast condition and good condition (leaving slow condition as the reference level). Our null hypothesis is then \\(\\beta_{3}=\\beta_{4}=0\\). We have statistically significant evidence (F = 57.2 on 2 and 116 df, p &lt; .001) that track condition is associated with winning speeds, after accounting for quadratic time trends and number of starters. One potential final model for predicting winning speeds of Kentucky Derby races is: \\[\\begin{equation} Y_{i}=\\beta_{0}+\\beta_{1}\\textstyle{Yearnew}_{i}+\\beta_{2}\\textstyle{Yearnew}^2_{i}+\\beta_{3}\\textstyle{Fast}_{i}+\\beta_{4}\\textstyle{Good}_{i}+\\beta_{5}\\textstyle{Starters}_{i}+\\epsilon_{i} \\\\ \\textrm{ where } \\epsilon_{i}\\sim N(0,\\sigma^2). \\tag{1.8} \\end{equation}\\] and OLS provides the following parameter estimates: lm(formula = speed ~ yearnew + yearnew2 + fast + good + starters, data = derby.df) Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.002e+01 1.946e-01 256.980 &lt; 2e-16 *** yearnew 7.003e-02 6.130e-03 11.424 &lt; 2e-16 *** yearnew2 -3.697e-04 4.598e-05 -8.041 8.44e-13 *** fast 1.393e+00 1.305e-01 10.670 &lt; 2e-16 *** good 9.157e-01 2.077e-01 4.409 2.33e-05 *** starters -2.528e-02 1.360e-02 -1.859 0.0656 . --- Residual standard error: 0.5483 on 116 degrees of freedom Multiple R-squared: 0.8267, Adjusted R-squared: 0.8192 Analysis of Variance Table Model 1: speed ~ yearnew + yearnew2 + starters Model 2: speed ~ yearnew + yearnew2 + fast + good + starters Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 118 69.257 2 116 34.870 2 34.386 57.196 &lt; 2.2e-16 *** This model accounts for the slowing annual increases in winning speed with a negative quadratic term, adjusts for baseline differences stemming from track conditions, and suggests that, for a fixed year and track condition, a larger field is associated with slower winning times (unlike the positive relationship we saw between speed and number of starters in our exploratory analyses). The model explains 82.7% of the year-to-year variability in winning speeds, and residual plots show no serious issues with LINE assumptions. We tested interaction terms for different effects of time or number of starters based on track condition, but we found no significant evidence of interactions. 1.7 Preview Having reviewed key ideas from multiple linear regression, you are now ready to extend those ideas, especially to handle non-normal responses and lack of independence. This section provides a preview of the type of problems you will encounter in the book. For each journal article cited, we provide an abstract in the authors’ words, a description of the type of response and, when applicable, the structure of the data. each of these examples appears later as an exercise, where you can play with the actual data or evaluate the analyses detailed in the articles. 1.7.1 Soccer Roskes M, Sligte D, Shalvi S, De Dreu C. (2011). The right side? Under time pressure, approach motivation leads to right-oriented bias. Psychological Science [Online] 22(11):1403-7. DOI: 10.1177/0956797611418677, October 2011. Abstract: Approach motivation, a focus on achieving positive outcomes, is related to relative left-hemispheric brain activation, which translates to a variety of right-oriented behavioral biases. [\\(\\ldots\\)] In our analysis of all Federation Internationale de Football Association (FIFA) World Cup penalty shoot-outs, we found that goalkeepers were two times more likely to dive to the right than to the left when their team was behind, a situation that we conjecture induces approach motivation. Because penalty takers shot toward the two sides of the goal equally often, the goalkeepers’ right-oriented bias was dysfunctional, allowing more goals to be scored. The response for this analysis is the direction of the goalie dive, a binary variable. For example, you could let Y=1 if the dive is to the right and Y=0 if the dive is to the left. This response is clearly not normally distributed. One approach to the analysis is logistic regression as described in Chapter 6. A binomial random variable could also be created for this application by summing the binary variables for each game so that Y= the number of dives right out of the number of dives the goalie makes during a game. [Thought question: Do you buy the last line of the abstract?] 1.7.2 Elephant Mating Poole J. (1989). Mate guarding, reproductive success and female choice in African elephants. Animal Behavior 37:842-49. Abstract: Male guarding of females, male mating success and female choice were studied for 8 years among a population of African elephants, Loxodonta africana. Males were not able to compete successfully for access to oestrous female until approximately 25 years of age. Males between 25 and 35 years of age obtained matings during early and late oestrus, but rarely in mid-oestrus. Large musth males over 35 years old guarded females in mid-oestrus. Larger, older males ranked above younger, smaller males and the number of females guarded by males increased rapidly late in life. Body size and longevity are considered important factors in determining the lifetime reproductive success of male elephants… Poole and her colleagues recorded, for each male elephant, his age (in years) and the number of matings for a given year. The researchers were interested in how age affects the males’ mating patterns. Specifically, questions concern whether there is a steady increase in mating success as an elephant ages or if there is an optimal age after which the number of matings decline. Because the responses of interest are counts (number of matings for each elephant for a given year), we will consider a Poisson regression (see Chapter 4). The general form for Poisson responses is the number of events for a specified time, volume, or space. 1.7.3 Parenting and Gang Activity Walker-Barnes C, Mason C. (2001). Ethnic differences in the effect of parenting on gang involvement and gang delinquency: a longitudinal, hierarchical linear modeling perspective. Child Development 72(6):1814-31. Abstract: This study examined the relative influence of peer and parenting behavior on changes in adolescent gang involvement and gang-related delinquency. An ethnically diverse sample of 300 ninth-grade students was recruited and assessed on eight occasions during the school year. Analyses were conducted using hierarchical linear modeling. Results indicated that, in general, adolescents decreased their level of gang involvement over the course of the school year, whereas the average level of gang delinquency remained constant over time. As predicted, adolescent gang involvement and gang-related delinquency were most strongly predicted by peer gang involvement and peer gang delinquency, respectively. Nevertheless, parenting behavior continued to significantly predict change in both gang involvement and gang delinquency, even after controlling for peer behavior. A significant interaction between parenting and ethnic and cultural heritage found the effect of parenting to be particularly salient for Black students, for whom higher levels of behavioral control and lower levels of lax parental control were related to better behavioral outcomes over time, whereas higher levels of psychological control predicted worse behavioral outcomes. The response for this study is a gang activity measure which ranges from 1 to 100. While it may be reasonable to assume this measure is approximately normal, the structure of this data implies that it is not a simple regression problem. Individual students have measurements made at 8 different points in time. We cannot assume that we have 2400 independent observations because the same measurements on one individual are more likely to be similar than a measurement of another student. Multilevel modeling as discussed in Chapters 10, 9, and ?? can often be used in these situations. 1.7.4 Crime Gelman A, Fagan J, Kiss A. (2007). An analysis of the NYPD’s stop-and-frisk policy in the context of claims of racial bias. Journal of the American Statistical Association 102(479):813-823. Abstract: Recent studies by police departments and researchers confirm that police stop racial and ethnic minority citizens more often than whites, relative to their proportions in the population. However, it has been argued stop rates more accurately reflect rates of crimes committed by each ethnic group, or that stop rates reflect elevated rates in specific social areas such as neighborhoods or precincts. Most of the research on stop rates and police-citizen interactions has focused on traffic stops, and analyses of pedestrian stops are rare. In this paper, we analyze data from 175,000 pedestrian stops by the New York Police Department over a fifteen-month period. We disaggregate stops by police precinct, and compare stop rates by racial and ethnic group controlling for previous race-specific arrest rates. We use hierarchical multilevel models to adjust for precinct-level variability, thus directly addressing the question of geographic heterogeneity that arises in the analysis of pedestrian stops. We find that persons of African and Hispanic descent were stopped more frequently than whites, even after controlling for precinct variability and race-specific estimates of crime participation. This application involves both non-normal data (number of stops by ethnic group can be modeled as a Poisson response) and multilevel data (number of stops within precincts will likely be correlated due to characteristics of the precinct population). This type of analysis will be the last type you encounter, generalized linear multilevel modeling, as addressed in Chapter ??. 1.8 Exercises 1.8.1 Conceptual Exercises Applications that do not violate the OLS assumptions for inference. Identify the response and explanatory variable(s) for each problem. Write the OLS assumptions for inference in the context of each study. Researchers record the number of cricket chirps per minute and temperature during that time to investigate whether the number of chirps varies with the temperature. A random selection of women aged 20-24 years are selected and their shoe size is used to predict their height Applications that do violate the OLS assumptions for inference. All of the examples in this section have at least one violation of the OLS assumptions for inference. Begin by identifying the response and explanatory variables. Write the assumptions in the context of each study as if there were no violations. Then identify which assumptions are likely to be invalid. Explain your answer in the context of the study. Fragile Family. Researchers using the Fragile Family data are attempting to see if socioeconomic status and parental stability are predictive of low birthweight. Note that a newborn is classified as having a low birthweight with potentially hazardous consequences if their birthweight is less than 2500 gm, hence our response is binary: 1 for low birthweight, and 0 when the birthweight is not low. Computer Mating. eHarmony.com claims that it is responsible for 2% of the marriages in the U.S. eHarmony.com researchers want to determine which factors predict a successful match. Clinical Trial I. A Phase I clinical trial is designed to compare the number of patients getting relief at different dose levels. Clinical Trial II. A randomized clinical trial investigated postnatal depression and the use of an estrogen patch (Gregoire et al. 1996). Patients were randomly assigned to either use the patch or not. Depression scores were recorded on 6 different visits. Schizophrenia. Thara et al. (1994) investigated the question ``Does the course of schizophrenia differ for patients with early and late onset?&quot; Doctors assess patients’ mood using a 0-100 scale at each of 10 visits. Pollution and traffic. Minnesota Pollution Control Agency is interested in using traffic volume data to generate predictions of particulate distributions as measured in counts per cubic feet. Canoes and zip codes. For each of over 27,000 overnight permits for the Boundary Water Canoe area, the zip code for the group leader has been translated to the distance traveled and socioeconomic data. This data is used to create a model for the number trips made per zip code. Factors affecting beginning salary. As part of a study investigating possible gender discrimination in beginning salaries at a particular company, researchers study the relationship between years of education and beginning salary among company employees. Elephant mating. Researchers are interested in how elephant age affects mating patterns among males. In particular, do older elephants have greater mating success, and is there an optimal age for mating among males? Data collected includes, for each elephant, age and number of matings in a given year. Mandatory holds. A mandatory hold is a revocation of a patient’s right to leave a psychiatric facility without consent of the monitoring physician. These holds have been classified as “good” or “bad”, and we want to know if certain groups (Native Americans, poor, uninsured) have rates of good holds that differ from others, after controlling for important factors in the hold decision (reason for hold, alcohol/drug use, physical threats, etc.). Beating the blues. Can an interactive multimedia program of behavioral therapy help depression patients more than standard care? 167 subjects were randomized into one of two treatment groups, and their depression levels were assessed monthly for 8 months (2 active treatment and 6 follow-up). We want to compare treatments while controlling for baseline depression, concomitant drug therapy, etc. Basketball referee bias. Do college basketball referees tend to even out the foul calls on the two teams over the course of a game? For example, if several more fouls have been called on the visitors at a certain point in the game, does it become more likely that the next foul will be called on the home team? And do these chances depend on the score of the game, the size of the crowd, or the referees working the game? No Child Left Behind. The Minnesota Department of Education makes all test scores publically available at the classroom level; for each test type (math, science, reading), scores are available by grade within school within district. School boards are interested in mining this data to learn about how their district is performing, relative to other districts, after adjusting for demographic information at the school and district levels (e.g., percent free and reduced lunch, percent non-white, percent special education). They are also interested in quantifying the impact of potential policy changes, such as increased funding, decreased class size, additional charter schools, etc. Kentucky Derby. The next set of questions is related to the Kentucky Derby case study from this chapter. Discuss the pros and cons of using side-by-side boxplots vs. stacked histograms to illustrate the relationships between year and track condition in Figure 1.3. Why is a scatterplot more informative than a correlation coefficient to describe the relationship between speed of the winning horse and year in Figure 1.3. How might you incorporate a fourth variable, say number of starters, into Figure 6.2? Explain why \\(\\epsilon_i\\) in Equation (1.1) measures the vertical distance from a data point to the regression line. In the first t-test in Section 1.6.1 (t = 11.251 for \\(H_0:\\beta_1 = 0\\)), notice that \\(t = \\frac{\\hat{\\beta_1}}{SE(\\beta_1)} = \\frac{.026}{.0023} = 11.251\\). Why is the t-test based on the ratio of the estimated slope to its standard error? In Equation (1.4), explain why the t-test corresponding to \\(\\beta_{1}\\) is equivalent to an independent-samples t-test under equal variances. Why is the equal variance assumption needed? When interpreting \\(\\beta_2\\) in Equation (1.5), why do we have to be careful to say for a fixed year or after adjusting for year? Is it wrong to leave a qualifier like that off? Interpret in context a 95% confidence interval for \\(\\beta_0\\) in Model 4. State (in context) the result of a t-test for \\(\\beta_1\\) in Model 4. Why is there no \\(\\epsilon_i\\) term in Equation (1.7)? If you considered the interaction between two continuous variables (like yearnew and starters), how would you interpret the coefficient for that interaction term? Interpret (in context) the OLS estimates for \\(\\beta_3\\) and \\(\\beta_5\\) in Equation (1.8). Moneyball. In a 2011 article in The Sport Journal, Farrar and Bruggink attempt to show that Major League Baseball general managers did not immediately embrace the findings of Michael Lewis’s 2003 Moneyball book. They contend that players’ on-base percentage remained relatively undercompensated compared to slugging percentage three years after the book came out. Two regression models are described: a Team Run Production Model and a Player Salary Model. Discuss potential concerns (if any) with the LINE assumptions for linear regression in each model. In Table 3, the authors contend that Model 1 is better than Model 3. Could you argue that Model 3 is actually better? How could you run a formal hypothesis test comparing Model 1 to Model 3? If authors had chosen Model 3 in Table 3 with the two interaction terms, how would that affect their final analysis, in which they compare coefficients of slugging and on-base percentage? (Hint: write out interpretations for the two interaction coefficients—the first one should be NL:OBP and the second one should be NL:SLG) The authors write that “It should also be noted that the runs scored equation fit is better than the one Hakes and Sauer have for their winning equation.” What do you think they mean by this statement? Why might this comparison not be relevant? In Table 4, Model 1 has a higher adjusted \\(R^2\\) than Model 2, yet the extra term in Model 1 (an indicator value for the National League) is not significant at the 5% level. Explain how this is possible. What limits does this paper have on providing guidance to baseball decision makers? 1.8.2 Guided Exercise Gender discrimination in bank salaries. In the 1970’s, Harris Trust was sued for gender discrimination in the salaries it paid its employees. One approach to addressing this issue was to examine the starting salaries of all skilled, entry-level clerical workers between 1965 and 1975. The following variables were collected for each worker (Data and scenario from The Statistical Sleuth (Ramsey F, Schafer D; 2013)): bsal = beginning salary (annual salary at time of hire) sal77 = annual salary in 1977 sex = MALE or FEMALE senior = months since hired age = age in months educ = years of education exper = months of prior work experience You may want to start by creating an indicator variable based on sex. Identify observational units, the response variable, and explanatory variables. The mean starting salary of male workers ($5957) was 16% higher than the mean starting salary of female workers ($5139). Confirm these mean salaries. Is this enough evidence to conclude gender discrimination exists? If not, what further evidence would you need? How would you expect age, experience, and education to be related to starting salary? Generate appropriate exploratory plots; are the relationships as you expected? What implications does this have for modeling? Why might it be important to control for seniority (number of years with the bank) if we are only concerned with the salary when the worker started? By referring to exploratory plots and summary statistics, are any explanatory variables (including sex) closely related to each other? What implications does this have for modeling? Fit a simple linear regression model with starting salary as the response and experience as the sole explanatory variable (Model 1). Interpret the intercept and slope of this model; also interpret the R-squared value. Is there a significant relationship between experience and starting salary? Does Model 1 meet all linear regression assumptions? List each assumption and how you decided if it was met or not. Is a model with all 4 confounding variables (Model 2, with senior, educ, exper, and age) better than a model with just experience (Model 1)? Justify with an appropriate significance test in addition to summary statistics of model performance. You should have noticed that the term for age was not significant in Model 2. What does this imply about age and about future modeling steps? Generate an appropriate coded scatterplot to examine a potential age-by-experience interaction. How would you describe the nature of this interaction? A potential final model (Model 3) would contain terms for seniority, education, and experience in addition to sex. Does this model meet all regression assumptions? State a 95% confidence interval for sex and interpret this interval carefully in the context of the problem. Based on Model 3, what conclusions can be drawn about gender discrimination at Harris Trust? Do these conclusions have to be qualified at all, or are they pretty clear cut? Often salary data is logged before analysis. Would you recommend logging starting salary in this study? Support your decision analytically. Regardless of your answer to the previous question, provide an interpretation for the coefficient for the male coefficient in a modified Model 3 after logging starting salary. Build your own final model for this study and justify the selection of your final model. You might consider interactions with gender, since those terms could show that discrimination is stronger among certain workers. Based on your final model, do you find evidence of gender discrimination at Harris Trust? 1.8.3 Open-ended Exercises Pace of Life. Some believe that individuals with a constant sense of time urgency (often called type-A behavior) are more susceptible to heart disease than are more relaxed individuals. Although most studies of this issue have focused on individuals, some psychologists have investigated geographical areas. They considered the relationship of city-wide heart disease rates and general measures of the pace of life in the city. For each region of the United States (Northeast, Midwest, South, and West) they selected three large metropolitan areas, three medium-size cities, and three smaller cities. In each city they measured three indicators of the pace of life. The variable WALK is the walking speed of pedestrians over a distance of 60 feet during business hours on a clear summer day along a main downtown street. BANK is the average time a sample of bank clerks takes to make change for two $20 bills or to give $20 bills for change. The variable TALK was obtained by recording responses of postal clerks explaining the difference between regular, certified, and insured mail and dividing the total number of syllables by the time of their response. The researchers also obtained the age-adjusted death rates from ischemic heart disease (a decreased flow of blood to the heart) for each city (HEART). (Data from R. V. Levine, “The Pace of Life,” American Scientist 78 (1990): 450-9 via The Statistical Sleuth) The variables have been standardized on a 0-40 scale, so there are no units of measurement involved. Build a regression model for explaining heart disease rates as a function of pace of life. Can we conclude that type-A individuals are more susceptible to heart disease? Waitress Tips. A student collected data from a restaurant where she was a waitress. The student was interested in learning under what conditions a waitress can expect the largest tips—for example: At dinner time or late at night? From younger or older patrons? From patrons receiving free meals? From patrons drinking alcohol? From patrons tipping with cash or credit? And should tip amount be measured as total dollar amount or as a percentage? Here is a quick description of the variables collected: Day = day of the week Meal = time of day (Lunch, Dinner, Late Night) Payment = how bill was paid (Credit, Cash, Credit with Cash tip) Party = number of people in the party Age = age category of person paying the bill (Yadult, Middle, SenCit) GiftCard = was gift card used? Comps = was part of the meal complimentary? Alcohol = was alcohol purchased? Bday = was a free birthday meal or treat given? Bill = total size of the bill W.tip = total amount paid (bill plus tip) Tip = amount of the tip Tip.Percentage = proportion of the bill represented by the tip "],
["ch-beyondmost.html", "Chapter 2 Beyond Least Squares: Using Likelihoods to Fit and Compare Models 2.1 Learning Objectives 2.2 Case Study: Does sex run in families? 2.3 Model 0: Sex Unconditional Model (Equal probabilities, Independence) 2.4 Model 1: Sex Unconditional Model (Any Probability, Independence) and the Principle of Maximum Likelihood 2.5 Model 2: A Sex Conditional Model (Sex Bias) and Model Specification 2.6 Case Study: Analysis of the NLSY data 2.7 Model 3: Stopping Rule Model (Waiting for a boy) 2.8 Summary of Model Building 2.9 Likelihood-based Methods 2.10 Likelihoods and this Course 2.11 Exercises", " Chapter 2 Beyond Least Squares: Using Likelihoods to Fit and Compare Models 2.1 Learning Objectives After finishing this chapter, you should be able to: Describe the concept of a likelihood, in words. Know and apply the Principle of Maximum Likelihood for a simple example. Identify three ways in which you can obtain or approximate an MLE . Use likelihoods to compare models. Construct a likelihood for a simple model. This text encourages you to broaden your statistical horizons by moving beyond independent, identically distributed, normal responses (iidN). This chapter on likelihood focuses on ways to fit models, determine estimates, and compare models for a wide range of types of responses, not just iidN data. In your earlier study of statistics, you fit simple linear models using ordinary least squares (OLS). Fitting those models assumes that the mean value of a response, Y, is linearly related to some variable, X. However, often responses are not normally distributed. For example, a study in education may involve scoring responses on a test as correct or incorrect. This binary response may be explained by the number of hours students spend studying. However we do not expect a variable which takes on only 0 or 1 to be a linear function of time spent studying. Figure 2.1: An attempt to fit a linear regression model to a binary response variable. In this instance we’ll use logistic regression instead of OLS. Fitting a logistic regression requires the use of likelihood methods. Another setting where likelihood methods come into play is when data is produced from a complex structure which may imply correlation among outcomes. For example, test scores for students who have been taught by the same teacher may be correlated. We’ll see that likelihood methods are useful when modeling correlated data. Likelihood methods not only provide a great deal of flexibility in the types of models we can fit, but they also provide ways in which to compare models as well. You might find likelihood methods a bit more complicated, but conceptually the approach is straightforward. As you go through the material here, worry less about calculus and computational details and focus on the concepts. You will have software to help you with computation, but model specification and interpretation will be up to you. 2.2 Case Study: Does sex run in families? Doesn’t it seem that some families tend to have lots of boys while others have more than their share of girls? Is it really the case that each child human couples produce is equally likely to be a male or female? Or does sex run in families? It can be argued that these kinds of questions have implications for population demographics and sibling harmony. For example, a 2009 study at the University of Ulster found that growing up with sisters, as compared to brothers, can enhance the quality of life of an adult [@BBCNEWS1995]. Sibling harmony aside, why do people care about gender imbalance? Comparisons of sex ratios between countries illustrate some compelling reasons. Some think that genetic or biological influences within families, such as “sex running in families,” can affect sex ratios. Mating behavior such as waiting until the family includes a boy or both sexes affects sex ratios. Some believe that sex ratios point to the practice of sex selection in a country accomplished through abortion or infanticide. Furthermore, there is speculation that an excess of men could lead to unrest among young males unable to find marriage partners or start families. It is curious to note that sex ratios were of interest to some early prominent statisticians. In 1930, the statistician Sir Ronald Fisher proposed an explanation for why the sex ratio should tend toward a 50:50 equilibrium for species that reproduce sexually. R. A. Fisher was a statistician renowned for his work in Analysis of Variance (ANOVA) and in the method of maximum likelihood, the subject of this chapter that is used throughout the text. Besides remarkable contributions to statistics, Fisher was also one of the founders of population genetics. Fisher posited his 50:50 equilibrium theory regarding sex ratios in terms of parental expenditure. He predicted that parental expenditure on both sexes should be equal. Sex ratios that are 1:1 are hence known as Fisherian, and those that are not 1:1 are non-Fisherian or extraordinary and occur because they break the assumptions made in Fisher’s model [@Rodgers2001]. Most often, in practice, sex ratios differ from what Fisher predicted. From 1970 to 2002, the sex ratio at birth in the US among white non-Hispanics was 105 boys to 100 girls, but only 103 boys to 100 girls among African Americans and Native Americans [@Mathews2005]. A 1997 study in Nature by Komdeur reports evidence which suggests that the human sex ratio may be currently shifting in the United States toward more female babies, closer to Fisher’s prediction! [@Komdeur1997] Sex ratio comparisons between countries are also intriguing. For example, Switzerland has a sex ratio of 106 boys to 100 girls whereas there are 112 boys to every 100 girls in China acoording to the World Fact Book[@CIA2013]. In the next section, we bring the notion of gender imbalance closer to home by focusing on families instead of countries or sub-populations. To investigate this question and others, we look at the gender composition of 5,626 families collected by the National Longitudinal Survey of Youth [@NLSY1997]. We fit models to explore whether there is evidence sex runs in families, a model we refer to as a Sex Conditional Model. We also consider a separate but related question about whether couples are “waiting for a boy.”[@Rodgers2001]. You can read the entire article about analyzing the gender composition of families here. 2.2.1 Research Questions We specify several models related to gender balance in families. Our models liken having babies to flipping a coin (heads=boy, tails=girl), of course, recognizing that in truth there is a little more to having babies. The baseline model (Model 0) assumes that the probability of a boy is the same as a probability of a girl. The first model (Model 1) considers the situation that the coin is loaded and the probability of heads (a boy) is different than the probability of tails (a girl). Next, we consider a model (Model 2) that conditions on the previous number of boys or girls in a family to get at the question of whether sex runs in families. This data is also used for a different set of models that relate to couples’ behavior. Specifically, we look to see if there is evidence that couples are waiting for a boy. Waiting for a girl, or waiting for both a boy and a girl, are left as exercises. Models 0 and 1 assume that having children is like flipping a coin. The gender of each child is independent of the gender of other children and the probability of a boy is the same for each new child. Let \\(p_B\\) be the probability a child is a boy. Model 0: Sex Unconditional Model (Equal probabilities) Is a child just as likely to be a boy as it is to be a girl; is \\(p_B\\) = 0.5? Model 1: Sex Unconditional Model (Different probabilities) Is the coin loaded; is \\(p_B \\neq 0.5\\)? Model 2: Sex Conditional Model (Sex bias): Do boys or girls run in families? That is, is there a tendency for families with more boys than girls to be more likely to produce another boy? Is the case the same for girls? Model 3: Stopping Rule Model (Waiting for a boy) Is there evidence that couples stop having children once a boy is born? Ultimately, our goal is to incorporate the family composition data represented as series of coin flips to find the “best” estimate for the probability of having a boy, \\(p_B\\), and evaluate the assumptions built into these models. We will be using likelihood-based methods, not ordinary least squares, to fit and compare these models. While the NLSY data is of interest, we start with a smaller, hypothetical data set of 30 families with a total of 50 children in order to illustrate concepts related to likelihoods. The “data” are the frequencies of possible family gender compositions for one-, two-, and three-child families. The methods we develop on this small data set will then be applied to the one-, two- and three-family NLSY data. It is straightforward to include all of the family sizes up to the four-or five-child families in the NLSY data. Table 2.1: The gender composition of 30 families in the hypothetical data set of n=50 children. Composition Number of families Number of children B 6 6 G 7 7 BB 5 10 BG 4 8 GB 5 10 GGB 1 3 GBB 2 6 Total 30 50 2.3 Model 0: Sex Unconditional Model (Equal probabilities, Independence) For the Sex Unconditional models, having children is modeled using coin flips. With a coin flip model, the result of each flip is independent of results of other flips. With this version of the Sex Unconditional Model, the chance that a baby is a boy is specified to be \\(p_B=0.5\\). It makes no difference if the first and third children are boys, the probability that the second child is a boy is 0.5; that is, the results for each child are independent of the others. Under this model you expect to see equal numbers of boys and girls. 2.4 Model 1: Sex Unconditional Model (Any Probability, Independence) and the Principle of Maximum Likelihood You may want your model to allow for the probability of a boy, \\(p_B\\), to be something different than 0.5. With this version of the Sex Unconditional model, \\(p_B&gt;0.5\\) or \\(p_B&lt;0.5\\) or \\(p_B=0.5\\), in which case you expect to see more boys than girls or fewer boys than girls or equal numbers of boys and girls, respectively. We would retain the assumption of independence; that is, the probability of a boy is \\(p_B\\) is the same for each child. Seeing a boy for the first child will not lead you to change the probability that the second child is a boy; this would not imply that “sex runs in families.” 2.4.1 What is a likelihood? As is often the case in statistics, our objective is to find an estimate for a model parameter using our data; here, the parameter to estimate is the probability of a boy, \\(p_B\\), and the data is the gender composition for each family. One way in which to interpret probability is to image repeatedly producing children. The probability of a boy will be the proportion of boys as the number of children increases. With likelihood methods, conceptually we consider different possible values for our parameter(s), \\(p_B\\), and determine how likely we are to see our data in each case, \\(Lik(p_B)\\). We’ll select as our estimate the value of \\(p_B\\) for which our data is most likely. A likelihood is a function that tells us how likely we are to observe our data for a given parameter value, \\(p_B\\). For a single family which has a girl followed by two boys, GBB, the likelihood function looks like: \\[\\begin{equation} Lik(p_B) = (1-p_B)p_B^2 \\tag{2.1} \\end{equation}\\] Figure 2.2: Likehood function for GBB. From the likelihood in Figure 2.2, when \\(p_B\\) = 0.3 we see a family of a girl followed by two boys 6.3% (\\(0.7*0.3^2\\)) of the time. However, it indicates that we are much more likely to see our data if \\(p_B\\) = 0.6 where the likelihood of GBB is \\(0.4*0.6^2\\) or 14.4%. If the choice was between 0.3 and 0.6 for an estimate of \\(p_B\\), we’d choose 0.6. The “best” estimate of \\(p_B\\) would be the value where we are most likely to see our data from all possible values between 0 and 1, which we refer to as the maximum likelihood estimate or MLE. We can approximate an MLE using graphical or numerical approaches. Graphically, here it looks like the MLE is about 0.6. In many, but not all, circumstances, we can obtain an MLE exactly using calculus. In this simple example, the MLE is 2/3. This is consistent with our intuition since 2 out of the 3 children are boys. Suppose another family consisting of three girls is added to our data set. We’ve already seen that the Sex Unconditional Model multiplies probabilities to construct a likelihood because children are independent of one another. Extending this idea, families can be assumed to be independent of one another so that the likelihoods for each family can be obtained by multiplication. With two families (GBB and GGG) our likelihood is now: \\[ Lik(p_B)=[(1-p_B)p_B^2]^2\\\\ =(1-p_B)^2p_B^4 \\] A plot of this likelihood appears below. It is right skewed with an MLE at approximately 0.3. Using calculus, we can show that the MLE is precisely 1/3 which is consistent with intuition given the 2 boys and 4 girls in our data. Figure 2.3: Likehood function for the data of 2 families (GBB and GGG). The solid line is at the MLE, \\({p}_B=1/3\\) Turning now to our hypothetical data with 30 families who have a total of 50 children, we can create the likelihood contribution for each of the family compositions. Table 2.2: The likelihood factors for the hypothetical data set of n=50 children. Composition Likelihood contribution for one family Number of families Likelihood contribution for multiple families B pB 6 p6B G (1-pB) 7 (1-pB)7 BB p2B 5 p10B BG pB(1-pB) 4 p4B(1-pB)4 GB (1-pB)pB 5 (1-pB)5p5B GGB (1-pB)2 pB 1 (1-pB)2pB GBB (1-pB)p2B 2 (1-pB)2 p4B Total 30 The likelihood function for the hypothetical data set can be found by taking the product of the entries in the last column of Table 2.2 and simplifying. \\[\\begin{equation} Lik(p_B) = p_B^{30}(1-p_B)^{20} \\tag{2.2} \\end{equation}\\] It should be obvious that the likelihood for this Sex Unconditional Model (the coin flipping model) has the simple form: \\[\\begin{equation} Lik(p_B) = p_B^{nBoys}(1-p_B)^{nGirls} \\tag{2.3} \\end{equation}\\] and as we asserted above, the MLE will be the (number of boys)/(number of kids) or 30/50 here. Now, more formally, we demonstrate how we use the likelihood principle to approximate the MLE or determine it exactly. 2.4.2 Finding MLEs 2.4.2.1 Graphically approximating an MLE Figure 2.4: Likelihood and log-likelihood functions for 50 children (30 boys and 20 girls) and for 1000 children (600 boys and 400 girls). Figure 2.4(a) is the likelihood for the data set of 50 children. The height of each point is the likelihood and the possible values for \\(p_B\\) appear across the horizontal axis. It appears that our data is most likely when \\(p_B\\) = 0.6 as we would expect it. Note that the log of the likelihood function in Figure 2.4(b) is maximized at the same spot: \\(p_B\\) = 0.6. Figures 2.4(c) and (d) are also maximized at \\(p_B\\) = 0.6, but they illustrate less variability and a sharper peak with more data. 2.4.2.2 Numerically approximating an MLE Here a grid search is used with the software package R to find maximum likelihood estimates, something that can be done with most software. A grid search specifies a set of finite possible values for \\(p_B\\) and then the likelihood, \\(Lik(p_B)\\), is computed for each of the possible values. First, we define a relatively coarse grid by specifying 50 values for \\(p_B\\) and then computing how likely we would see our data for each of these possible values. The second example uses a finer grid, 1000 values for \\(p_B\\), which allows us to determine a better (more precise) approximation of the MLE. Another way in which to approximate the MLE using software is to use the package’s optimization function. Most packages, like R, have an optimize function which can also be used to obtain MLEs. Both of these approaches are illustrated in the following code. Lik.f &lt;- function(nBoys,nGirls,nGrid){ pb=seq(0,1,length=nGrid) # possible values for prob a boy is born lik=pb^{nBoys}*(1-pb)^{nGirls} max(lik) # maximum likelihood over nGrid values of pb pb[lik==max(lik)] # value of pb where likelihood maximized } Lik.f(nBoys = 30, nGirls = 20, nGrid = 50) # coarse grid ## [1] 0.5918367 Lik.f(nBoys = 30, nGirls = 20, nGrid = 1000) # finer grid ## [1] 0.5995996 ## Another approach: using R&#39;s optimize command ## Note that the log-likelihood is optimized here oLik.f &lt;- function(pb){ return(30*log(pb) + 20*log(1-pb)) } optimize(oLik.f, interval=c(0,1), maximum=TRUE) ## $maximum ## [1] 0.5999985 ## ## $objective ## [1] -33.65058 2.4.2.3 MLEs using calculus (Optional) Calculus may provide another way to determine an MLE. Here, we can ascertain the value of \\(p_B\\) where the likelihood is a maximum by using the first derivative of the likelihood with respect to \\(p_B\\). We obtain the first derivative using the Product Rule, set it to 0, solve for \\(p_B\\), and verify a maximum occurs there. \\[\\begin{equation*} \\frac{d}{dp_B}p_B^{30}(1-p_B)^{20} = 30p_B^{29}(1-p_B)^{20}-p^{30}_B20(1-p_B)^{19} = 0 \\end{equation*}\\] This approach would produce \\(p_B = .60\\) as we intuited earlier; however, we’ll find that likelihoods can get a lot more complicated than this one and there is a simpler way to proceed. This simpler approach is based on the fact that the log of a likelihood is maximized at the same value of \\(p_B\\) as the likelihood. Likelihoods are typically products which require the application of the Product Rule when differentiating, whereas log-likelihoods are sums which are much easier to differentiate. In addition, likelihoods can become tiny with large data sets. So we can take the log of the likelihood, differentiate it, and find the value of \\(p_B\\) where the log-likelihood is a maximum and have the MLE for \\(p_B\\). We observed this visually in Figure 2.4, where (a) and (b) are maximized at the same \\(p_B\\) (as are (c) and (d)). For the data set with 50 children: \\[\\begin{eqnarray*} Lik(p_B) &amp; = &amp; p_B^{30}(1-p_B)^{20} \\\\ logLik(p_B) &amp;=&amp; 30log(p_B)+20log(1-p_B) \\\\ \\frac{d}{dp_B} logLik(p_B) &amp;=&amp; \\frac{30}{p_B} -\\frac{20}{1-p_B} = 0 \\tag{2.4} \\end{eqnarray*}\\] It is now straightforward to determine that the log-likelihood is maximized when \\(p_B = 3/5.\\) We say that the MLE is \\(\\hat{p}_B = 0.6\\). This is the exact estimate that was approximated above. 2.4.2.4 How does sample size affect the likelihood? Consider two hypothetical cases under the Sex Unconditional Model: Hypothetical Case 1: n= 50 children with 30 boys and 20 girls In previous sections, we found the MLE, \\(\\hat{p}_B=0.6.\\) Hypothetical Case 2: n= 1000 children with 600 boys and 400 girls Our earlier work suggests that the MLE here is also \\(\\hat{p}_B=0.6.\\) The graphs of the likelihoods and log-likelihoods for these two cases in Figure 2.4 give us an idea of how the increase in the sample size affects the precision of our estimates. The likelihoods and log-likelihoods for the two sample sizes have similar forms; however, the graphs with the larger sample size are much narrower, reflecting the greater precision we have with more data. With only 50 children there is a wide range of \\(p_B\\) values that lead to values of the log-likelihodd near its maximum, so it’s less clear what the optimal \\(p_B\\) is. As we have seen in statistics courses before, a larger sample size will result in less variation in our estimates, thereby affecting the power of hypothesis tests and the width of the confidence intervals. 2.4.3 Summary Using likelihoods to find estimates of parameters is conceptually intuitive—select the estimate for your parameter where your data is most likely. Often MLEs make a lot of intuitive sense in the context of a problem as well; for example, here the MLE for the probability of a boy is the observed proportion of boys in the data. It may seem like a lot of work for such an obvious result, but MLEs have some nice, useful theoretical properties, and we’ll see that many more complex models can be fit using the principle of maximum likelihood. In summary, we constructed a likelihood that reflected features of our Sex Unconditional Model, then we approximated the parameter value for which our data is most likely using a graph or software, or we determined our optimal parameter value exactly using calculus. You may not be familiar with calculus, yet the concept is clear from the graphs: just find the value of \\(p_B\\) where the likelihood is a maximum. Our “best” estimate for \\(p_B\\), the MLE, is where our data is most likely to be observed. Work to understand the idea of a likelihood. Likelihoods are the foundation upon which estimates are obtained and models compared for most of this course. Do not be overly concerned with calculus and computation at this point. 2.4.4 Is a likelihood a probability function? (Optional) No. Even though we use probabilities to construct likelihoods, a likelihood is not a probability function. A probability function takes outcomes as input and outputs a probability of a particular outcomes. For example, you flip a loaded coin which comes up heads 25% of the time. After 5 flips, you observe the outcome of three heads and two tails. A probability function provides the probability of observing (3H,2T) when \\(p_H=.25\\). If you flip this same coin another 5 times and observe all tails (5T), the probability function provides the probability of (5T). In contrast, a likelihood is constructed using the data, say (3H,2T). It takes as input possible parameter values and returns the probability of seeing that data for the given parameter value. For example, the likelihood will provide the chance of seeing the data (3H,2T) if \\(p_H\\)=.6, the likelihood will provide the chance of seeing the data (3H,2T) if \\(p_H\\)=.3, and so on. With the likelihood we can find the value of \\(p_H\\) where we are most likely to see our data. 2.5 Model 2: A Sex Conditional Model (Sex Bias) and Model Specification Our first research question involves determining whether sex runs in the family. Do families who already have boys tend to have more additional boys than expected by chance, and do families who already have girls tend to have more additional girls than expected by chance? What do you think? And how could we use a statistical model to investigate this phenomenon? There are a number of different ways to construct a model for this question. Here’s one possibility. 2.5.1 Model Specification Unlike the previous model, the \\(p_{B}\\) in a Sex Conditional Model depends on existing family compositions. We introduce conditional probabilities and conditional notation to make the dependence explicit. One way to interpret the notation \\(P(A|B)\\) is the “probability of A given B has occurred.” Another way to read this notation is the “probability of A conditional on B.” Here, let \\(p_{B|N}\\) represent the probability the next child is a boy given that there are equal numbers of boys and girls (sex-neutral) in the existing family. Let \\(p_{B|B_{bias}}\\) represent the probability the next child is a boy if the family is boy-biased; i.e., there are more boys than girls prior to this child. Similarly, let \\(p_{B|G_{bias}}\\) represent the probability the next child is a boy if the family is girl-biased; i.e., there are more girls than boys prior to this child. Before we are mired in notation and calculus, let’s think about how these conditional probabilities can be used to describe sex running in families. While we only had one parameter, \\(p_B\\), to estimate in the Sex Unconditional Model, here we have three parameters: \\(p_{B|N}\\), \\(p_{B|B_{bias}}\\), and \\(p_{B|G_{bias}}\\). Clearly if all three of these probabilities are equal, the probability a child is a boy does not depend upon the existing gender composition of the family and there is no evidence of sex running in families. A conditional probability \\(p_{B|B_{bias}}\\) that is larger than \\(p_{B|N}\\) suggests families with more boys are more likely to produce additional boys in contrast to families with equal boys and girls. This finding would support the theory of “boys run in families.” An analogous argument holds for girls. In addition, comparisons of \\(p_{B|B_{bias}}\\) and \\(p_{B|G_{bias}}\\) to the parameter estimate \\(p_B\\) from the Sex Unconditional Model may be interesting and can be performed using likelihoods. While it may seem that including families with a single child (singletons) would not be helpful for assessing whether there is a preponderance of one sex or another in families, in fact singleton families would be helpful in estimating \\(p_{B|N}\\) because singletons join “neutral families.” Table 2.3: Family contributions to the likelihood for a Sex Conditional Model using a hypothetical data set of n=50 children from 30 families. Composition Likelihood contribution Prior Status Number of families B pB|N neutral 6 G 1-pB|N neutral 7 BB pB|NpB|Bbias neutral, boy bias 5 BG pB|N(1-pB|Bbias) neutral, boy bias 4 GB (1-pB|N)pB|Gbias neutral, girl bias 5 GGB (1-pB|N)(1-pB|Gbias)pB|Gbias neutral, girl bias, girl bias 1 GBB (1-pB|N)pB|GbiaspB|N neutral, girl bias, neutral 2 Total 30 2.5.2 Application to Hypothetical Data Using the family composition data for 50 children in the 30 families that appears in Table 2.3, we construct a likelihood. The six singleton families with only boys (B) contribute \\(p_{B|N}^6\\) to the likelihood and the seven families with only girls contribute \\(p_{G|N}^7\\) or \\((1-p_{B|N})^7\\). [Why do we use \\(1-p_{B|N}\\) instead of \\(p_{G|N}\\)?] There are five families with two boys (BB) each with probability \\(p_{B|N}*p_{B|B_{bias}}\\) contributing: \\[ [p_{B|N}p_{B|B_{bias}}]^{5}. \\] We construct the likelihood using data from all 30 families assuming families are independent to get: \\[\\begin{equation} lik(p_{B|N},p_{B|B_{bias}}, p_{B|G_{bias}}) = p_{B|N}^{17} *(1-p_{B|N})^{15} *p_{B|B_{bias}}^{5} *(1-p_{B|B_{bias}})^{4} *p_{B|G_{bias}}^{8} *(1-p_{B|G_{bias}}) \\tag{2.5} \\end{equation}\\] A couple of points are worth noting. First, there are 50 factors in the likelihood corresponding to the 50 children in these 30 families. Second, in the Sex Unconditional example we only had one parameter, \\(p_{B}\\); here we have three parameters. This likelihood does not simplify like the Sex Unconditional Model to one as a product of only two powers; one of \\(p_B\\) and the other of 1-\\(p_B\\). Yet, the basic idea we discussed regarding using a likelihood to find parameter estimates is the same. To obtain the MLEs, we need to find the combination of values for our three parameters where the data is most likely to be observed. Conceptually, we are trying different combinations of possible values for these three parameters, one after another, until we find the combination where the likelihood is a maximum. It will not be as easy to graph this likelihood and we will need multivariable calculus to locate the optimal combination of parameter values where the likelihood is a maximum. In this text, we do not assume you know multivariable calculus, but we do want you to retain the concepts associated with maximum likelihood estimates. In practice, we use software to obtain MLEs. From Table 2.3 note that every first child enters a neutral family with respect to sex composition. It is also clear from this table that there is a probability associated with every child which can be multiplied to obtain likelihood contributions for each sex composition. As we noted with the Sex Unconditional Model, we can write \\(p_{G|B_{bias}}= 1- p_{B|B_{bias}}\\) and likewise for families with a girl bias; it is unnecessary to have both \\(p_{G|B_{bias}}\\) and \\(p_{B|B_{bias}}\\) in the likelihood. Table 2.3 should help to give you an idea of how the notation is defined for the Sex Conditional Model, although as is often the case in statistical modeling, there are other ways to set up models to test for Sex Bias. With calculus, we can take partial derivatives of the likelihood with respect to each parameter assuming the other parameters are fixed. As we saw in Section @ref(calc.sec), differentiating the log of the likelihood often makes things easier. This same approach is recommended here. Set each partial derivative to 0 and solve for all parameters simultaneously. Knowing that it is easier to work with log-likelihoods, let’s take the log of the likelihood we constructed Equation (2.5). \\[\\begin{eqnarray*} log\\textrm{-}lik(p_{B|N},p_{B|B_{bias}}, p_{B|G_{bias}})&amp; = &amp;17logp_{B|N}+15log(1-p_{B|N})+5logp_{B|B_{bias}} \\\\ &amp; &amp;+4log(1-p_{B|B_{bias}}) +8logp_{B|G_{bias}}+1log(1-p_{B|G_{bias}}) \\\\ \\end{eqnarray*}\\] Taking a partial derivative with respect to \\(p_{B|N}\\) \\[\\begin{eqnarray*} \\frac{17}{p_{B|N}} - \\frac{15}{1-p_{B|N}}&amp;=&amp;0 \\\\ \\hat{p}_{B|N}&amp;=&amp; \\frac{17}{32} \\\\ &amp;=&amp;0.53 \\\\ \\end{eqnarray*}\\] This estimate follows naturally. First consider all of the children who enter into a family with an equal number of boys and girls. From Table 2.3, we can see there are 32 such children (30 are first kids and 2 are third kids in families with 1 boy and 1 girl). Of those children, 17 are boys. So, given that a child joins a sex neutral family, the chance they are a boy is 17/32. Similar calculations for \\(p_{B|B_{bias}}\\) and \\(p_{B|G_{bias}}\\) yield: \\[\\begin{eqnarray*} \\hat{p}_{B|N} &amp;=&amp; 0.53 \\\\ %17/32 \\hat{p}_{B|B_{bias}} &amp;=&amp; 0.56 \\\\% 5/9 \\hat{p}_{B|G_{bias}} &amp;=&amp; 0.89 \\\\ \\end{eqnarray*}\\] If we anticipate any “sex running in families” effect, we would expect \\(\\hat{p}_{B|B_{bias}}\\) to be larger than the the probability of a boy in the neutral setting, \\(p_{B|N}\\). In our small hypothetical example,\\(\\hat{p}_{B|B_{bias}}\\) is slightly greater than 0.53 providing light support for the “sex runs in families” theory when it comes to boys. What about girls? Do families with more girls than boys tend to have a greater probability of having a girl? We found that the MLE for the probability of a girl in a girl bias setting is 1-0.89=0.11. [Note: A nice property of MLEs is demonstrated here. We have the MLE for \\(p_{B|G_{bias}}\\), and we want the MLE of \\(p_{G|G_{bias}}=1-p_{B|G_{bias}}\\). We can get it by replacing \\(p_{B|G_{bias}}\\) with its MLE; i.e., \\(\\hat{p}_{G|G_{bias}}=1-\\hat{p}_{B|G_{bias}}\\). In mathematical terms, you can get the MLE of a function by applying the function to the original MLE.] This data does not provide evidence that girls run in families since \\(\\hat{p}_{G|G_{bias}} = 0.11 &lt; \\hat{p}_{B|N}=0.53\\); there is a markedly lower probability of a girl if the family is already girl based. This data is, however, hypothetical. Let’s take a look at some real data and see what we find. 2.6 Case Study: Analysis of the NLSY data 2.6.1 Model Building Plan You should now have a feel for using the Likelihood Principle to obtain estimates of parameters using family gender composition data. Next, these ideas will be applied to the NLSY data. In addition to considering the Sex Unconditional and Conditional Models, we investigate some models that incorporate choices couples may make about when to stop having more children. 2.6.2 Family Composition of Boys and Girls, NLSY: Exploratory Data Analysis We begin by performing an exploratory data analysis aimed at shedding some light on our research questions. We are looking for clues as to which of our models is most plausible. The first statistic of interest is the proportion of boys in the sample. There are 5,416 boys out of the 10,672 children or a proportion of 50.7% boys. While this proportion is very close to 50%, it is worth noting that a difference of 0.7% could be meaningful in population terms. Table 2.4: Number of families and children in families with given composition. Sex ratio and proportion males by family size. Family Composition Number of families Number of children males:females \\(p_B\\) B 930 930 97 boys to 100 girls 0.494 G 951 951 BB 582 1164 104 boys to 100 girls 0.511 BG 666 1332 GB 666 1332 GG 530 1060 BBB 186 558 104 boys to 100 girls 0.510 BBG 177 531 BGG 173 519 BGB 148 444 GBB 151 453 GGB 125 375 GBG 182 546 GGG 159 477 Table 2.4 displays family composition data for the 5,626 families with one, two, or three children in the data set. This dataset includes 10,672 children. Because our interest centers on the proportion of males, let’s calculate sex ratios and proportions of males for each family size. For one-child families the male to female ratio is less than one (97 males:100 females) whereas both two- and three-child families have ratios of 104 boys to 100 girls, what we may expect in a population which favors males. While our research questions do not specifically call for these measures stratified by family size, it still provides us with an idea of gender imbalance in the data. Table 2.5 provides insight into whether sex runs in families if the probability of a boy is 0.5. Simple probability suggests that the proportion of 2-child families with all the same sex would be 50% (BB or GG vs. BG or GB) but in our data we see only 45%. For 3-child families, we have 8 possible orderings of boys and girls and so we would expect 2 out of the 8 orderings (25%) to be of the same sex (BBB or GGG) to have all of the same sex, but in fact 27% have the same sex among the 3-children families. These results do not provide overwhelming evidence of sex running in families. There are some potentially complicating factors: the probability of a boy may not be 0.5 or couples may be waiting for a boy or a girl or both. Table 2.5: Proportion of families with all the same sex by number of children in the family. Note that 1-child families are all homogeneous with respect to sex so we look at 2- and 3- child families. Number of children Number of families Number with all same sex Percent with same sex Two Children 2444 1112 45% Three Children 1301 345 27% Table 2.6 contains the number of families by size and the percentage of those which are families with one boy who is last. It appears that these families “waited” for a boy and then quit childbearing after a boy was born. We see the proportion of one child families with a boy is slightly less than the 50% expected. We’d expect one out of four, or 25%, of 2-child family configurations to have one boy last and there is 27% in our dataset. Only 8.6% 3-child families has one boy last, but in theory we would expect one out of eight or 12.5% 3-child families to have one boy last. So if, in fact, the probability of a boy is 50%, there does not appear to be evidence supporting the notion that families wait for a boy. Table 2.6: Proportion of families with only one boy who is born last. Number of children Number of families Number with one boy last Percent with boy last One Child 1881 930 49.4% Two Children 2444 666 27.2% Three Children 1301 125 8.6% There are many other ways to formulate and explore the idea that sex runs in families or that couples wait for a boy (or a girl). See [@Rodgers2001] for other examples. 2.6.3 Likelihood for the Sex Unconditional Model: the NLSY data We construct a likelihood for the Sex Unconditional Model for the one-, two- and three-child families from the NLSY. See Table 2.4 for the frequencies of each gender composition. Families with different compositions will contribute different factors to the likelihood. For example, here is a sample of contributions for a few family compositions. See Table 2.4 to see where these numbers are coming from. G \\((1-p_B)^{951}\\) GB \\((1-p_B)^{666}p_B^{666}\\) BGB \\(p_B^{2*148}(1-p_B)^{148}\\) Now we create the entire likelihood for our data under the Sex Unconditional Model. \\[\\begin{eqnarray*} Lik(p_B) &amp;=&amp; p_B^{930}p_G^{951}p_{BB}^{582} \\cdots p_{BBG}^{177} \\cdots P_{GGG}^{159} \\\\ &amp;= &amp; p_B^{930+2*582+666+666+\\cdots+125}(1-p_B)^{951+666+666+2*530+\\cdots+3*159} \\\\ Lik(p_B) &amp; =&amp; p_B^{5416}(1-p_B)^{5256} \\\\ \\tag{2.6} \\end{eqnarray*}\\] This very simple likelihood implies that each child contributes a factor of the form \\(p_B\\) or \\(1-p_B\\). Given that there are 10,672 children, what would be your best guess of the estimated probability of a boy for this model? We can determine the MLE for \\(p_B\\) using our previous work. \\[\\begin{eqnarray} \\hat{p_B} &amp;=&amp; \\frac{nBoys}{nBoys + nGirls} \\\\ &amp; = &amp; \\frac{5416}{5416+5256} \\\\ &amp;=&amp; 0.507 \\tag{2.7} \\end{eqnarray}\\] 2.6.4 Likelihood for the Sex Conditional Model The contribution to a Sex Conditional Model likelihood for the same family compositions we considered in the previous section appear here. G \\((1-p_{B|N})^{951}\\) GB \\((1-p_{B|N})^{666}p_{B|G_{bias}}^{666}\\) BGB \\(p_{B|N}^{2*148}(1-p_{B|B_{bias}})^{148}\\) The products of the last three columns of Table 2.7 provide the likelihood contributions for the Sex Conditional Model for all of the one-, two- and three-child NLSY families. We write the likelihood as a function of the three parameters \\(p_{B|N}, p_{B|B_{bias}}\\), and \\(p_{B|G_{bias}}\\). Table 2.7: Likelihood contributions for families in Sex Unconditional and Sex Conditional Models. Sex Conditional Model Family composition Number of families Sex Unconditional Child1 Child 2 Child 3 B 930 pB pB|N G 951 (1-pB) 1-pB|N BB 582 pB2 pB|N pB|Bbias BG 666 pB (1-pB) pB|N 1-pB|Bbias GB 666 (1-pB)pB 1-pB|N pB|Gbias GG 530 (1-pB)2 1-pB|N 1-pB|Gbias BBB 186 pB3 pB|N pB|Bbias pB|Bbias BBG 177 pB2 (1-pB) pB|N pB|Bbias 1-pB|Bbias BGG 173 pB (1-pB)2 pB|N 1-pB|Bbias 1-pB|N BGB 148 pB2 (1-pB) pB|N 1-pB|Bbias pB|N GBB 151 pB2 (1-pB) 1-pB|N pB|Gbias pB|N GGB 125 pB (1-pB)2 1-pB|N 1-pB|Gbias pB|Gbias GBG 182 pB(1-pB)2 1-pB|N pB|Gbias 1-pB|N GGG 159 (1-pB)3 1-pB|N 1-pB|Gbias 1-pB|Gbias log-likelihood -7396.067 -7374.238 AIC 14794.13 14751.48 BIC 14810.68 14749.18 \\[\\begin{eqnarray} Lik(p_{B|N}, p_{B|B_{bias}}, p_{B|G_{bias}})&amp; =&amp; p_{B|N}^{930}(1-p_{B|N})^{951} ( p_{B|N}p_{B|B_{bias}})^{582}(p_{B|N}(1-p_{B|B_{bias}}))^{666}\\\\ &amp;&amp; \\cdots ((1-p_{B|N})(1-p_{B|G_{bias}})(1-p_{B|G_{bias}}))^{159}\\\\ &amp;=&amp; p_{B|N}^{3161}(1-p_{B|N})^{3119}p_{B|B_{bias}}^{1131}(1-p_{B|B_{bias}})^{1164}p_{B|G_{bias}}^{1124}(1-p_{B|G_{bias}})^{973} \\\\ \\tag{2.8} \\end{eqnarray}\\] \\[\\begin{eqnarray} logLik(p_{B|N}, p_{B|B_{bias}}, p_{B|G_{bias}})&amp;=&amp; 3161logp_{B|N}+3119log(1-p_{B|N})+\\\\ &amp; &amp;1131log(p_{B|B_{bias}})+1164log(1-p_{B|B_{bias}})\\\\ &amp; &amp;+1124logp_{B|G_{bias}}+ 973log(1-p_{B|G_{bias}})\\\\ \\tag{2.9} \\end{eqnarray}\\] To use calculus to estimate the probability of a boy entering a neutral family (a family with equal boys and girls), \\(p_{B|N}\\), we begin with the logarithm of the likelihood in equation (2.9). Differentiating the log-likelihood with respect to \\(p_{B|N}\\) holding all other parameters constant yields an intuitive estimate. \\[\\begin{eqnarray*} \\hat{p}_{B|N}&amp;=&amp;\\frac{3161}{3161+3119} \\\\ &amp;=&amp;0.5033 \\end{eqnarray*}\\] There are 6,280 times when a child is joining a neutral family and of those times 3,161 are boys. Thus the MLE of the probability of a boy joining a family where the number of boys and girls are equal (including when there are no children) is 0.5033. Similarly, MLEs for \\(p_{B|B_{bias}}\\) and \\(p_{B|G_{bias}}\\) can be obtained: \\[\\begin{eqnarray*} \\hat{p}_{B|B_{bias}}&amp;=&amp; \\frac{1131}{1131+1164}\\\\ &amp; = &amp; 0.4928 \\\\ &amp; &amp; \\\\ \\hat{p}_{B|G_{bias}}&amp;=&amp;\\frac{1124}{1124+973}\\\\ &amp;=&amp; 0.5360 \\end{eqnarray*}\\] Are these results consistent with the notion that boys or girls run in families? We consider the Sex Conditional Model because we hypothesized there would be a higher probability of boys among children born into families with a boy bias. However, we found that if there is a boy bias the probability of a subsequent boy was estimated to be actually less, 0.493. Similarly, girls join families with more girls than boys approximately 46.4% of the time so that there is little support for the idea that either “girls or boys run in families.” Even though initial estimates don’t support the idea, let’s formally take a look as to whether prior gender composition affects the probability of a boy. To do so, we’ll see if the Sex Conditional Model is statistically significantly better than the Sex Unconditional Model. 2.6.5 Comparing the Sex Unconditional to the Sex Conditional Model 2.6.5.1 Nested Models Likelihoods are not only useful for fitting models, but they are also useful when comparing models. If the parameters for a reduced model are a subset of parameters for a larger model, we say the models are nested and the difference between their likelihoods can be incorporated into a statistical test to help judge the benefit of including additional parameters. Another way in which to think of nesting is to consider whether parameters in the larger model can be equated to obtain the simpler model or whether some parameters in the larger model can be set to constants. Since \\(p_{B|B_{bias}}, p_{B|N}\\) and \\(p_{B|G_{bias}}\\) in the Sex Conditional Model can be set to \\(p_B\\) to obtain the Sex Unconditional Model, we can say the models are nested. If the parameters are not nested, comparing models with the likelihood can still be useful but will take a different form. We’ll see that the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) are functions of the log-likelihood that can be used to compare models even when the models are not nested. Either way we see that this notion of likelihood is pretty useful. Hypotheses \\(Ho: p_{B|N}=p_{B|B_{bias}}=p_{B|G_{bias}}=p_B\\) (Sex Unconditional Model) The probability of a boy does not depend on the prior family composition. \\(Ha: \\textrm{At least one parameter from } p_{B|N}, p_{B|B_{bias}},p_{B|G_{bias}}\\) differs from the others. (Sex Conditional Model) The probability of a boy does depend on the prior family composition. We start with the idea of comparing the likelihoods or, equivalently, the log-likelihoods of each model at their maxima. To do so, we use the log-likelihoods to determine the MLEs, and then replace the parameters in the log-likelihood with their MLEs, thereby finding the maximum value for the log-likelihood of each model. Here we will refer to the first model, the Sex Unconditional Model, as the reduced model noting that it has only a single parameter, \\(p_B\\). The more complex model, the Sex Conditional Model, has three parameters and is referred to here as the larger model. We’ll use the MLEs derived earlier. (Equations (2.7) and (2.8)). The maximum of the log-likelihood for the reduced model can be found by replacing \\(p_{B}\\) in the log-likelihood with the MLE of \\(p_{B}\\), 0.5075. \\[\\begin{eqnarray*} logLik(0.5075)&amp; = &amp;5416*log(.5075) + 5256* log(1-.5075)\\\\ &amp;=&amp; -7396.067\\\\ \\end{eqnarray*}\\] The maximum of the log-likelihood for the larger model can be found by replacing \\(p_{B|N}, p_{B|B_{bias}}, p_{B|G_{bias}}\\) in the log-likelihood with 0.5033, 0.4928, and 0.5360, respectively. \\[\\begin{eqnarray*} logLik(0.5033, 0.4928, 0.5360) &amp;= &amp; 3161*log(.5033)+3119*log(1-.5033)+\\\\ &amp; &amp; 1131*log(.4928)+1164*log(1-.4928)+\\\\ &amp; &amp; 1124*log(.5360)+973*log(1-.5360)\\\\ &amp;=&amp; -7374.236 \\end{eqnarray*}\\] Take a look at the log-likelihoods—the maximum log-likelihood for the larger model is indeed larger (less negative). The maximum log-likelihood for the larger model is guaranteed to be at least as large as the maximum log-likelihood for the reduced model, so we’ll be interested in whether this observed difference in maximum log-likelihoods, -7374.236 -(-7396.067) = 21.831, is significant. A result from statistical theory states that, when the reduced model is the true model, twice the difference of the maximum log-likelihoods follows a \\(\\chi^2\\) distribution with the degrees of freedom equal to the difference in the number of parameters between the two models. A difference of the maximum log-likelihoods can also be looked at as the log of the ratio of the likelihoods and for that reason the test is referred to as the Likelihood Ratio Test (LRT). Our test statistic is \\[ LRT = 2[maxlogLik(\\textrm{larger model}) - maxlogLik(\\textrm{reduced model})]\\] \\[LRT = 2log\\left(\\frac{maxLik(\\textrm{larger model})}{maxLik(\\textrm{reduced model})} \\right)\\] Intuitively, when the likelihood for the larger model is much greater than it is for the reduced model, we have evidence that the larger model is more closely aligned with the observed data. This isn’t really a fair comparison on the face of it. We need to account for the fact that more parameters were estimated and used for the larger model. That is accomplished by taking into account the degrees of freedom for the \\(\\chi^2\\) distribution. The expected value of the \\(\\chi^2\\) distribution is its degrees of freedom. Thus when the difference in the number of parameters is large, the test statistic will need to be much larger to convince us that it is not simply chance variation with two identical models. Here, under the reduced model we’d expect our test statistic to be 2, when in fact it is over 43. The evidence favors our larger model. More precisely, the test statistic is \\((-2)*(-7374.238+7396.073) = 43.67 (p&lt;0.0001)\\). We have convincing evidence that the Sex Conditional Model provides a significant improvement over the Sex Unconditional Model. However, keep in mind that our point estimates for a probability of a boy were not what we had expected for “sex runs in families.” It may be that this discrepancy stems from behavioral aspects of family formation. The next section on stopping rules explores types of couples’ decisions may affect the relative proportions of family compositions in the data. Note: You may notice that the LRT is similar in spirit to the extra-sum-of-squares F-test used in linear regression. Recall that the extra-sum-of-squares F-test involves comparing two nested models. When the smaller model is true, the F-ratio follows an F-distribution which on average is 1.0. A large, unusual F-ratio provides evidence that the larger model provides a significant improvement. Also note: It might have been more logical to start by using Likelihood Ratio Test determine whether the probability of having a boy differs significantly from 0.5. We leave this as an exercise. 2.7 Model 3: Stopping Rule Model (Waiting for a boy) [@Rodgers2001] offer one reason to explain the contradictory results: waiting for a male child. It has been noted by demographers that some parents are only interested in producing a male heir so that the appearance of a boy leads more often to the family ending childbearing. Stopping models investigate questions like: Are couples more likely to stop childbearing once they have a boy? Or are some parents waiting for a girl? Others might wish to have at least one boy and girl. The exploratory data analysis results in Table 2.6 provide some insight but cannot definitively settle the question about couples’ stopping once they have a boy. For stopping models, two probabilities are recorded for each child: the probability of the sex and the conditional probability of stopping after that child. As we have done in previous models, let \\(p_B\\) = probability the child is a boy. When conditioning, every possible condition must have a probability associated with it. Here the stopping conditions for Model 3 are: stop on first boy \\((S|B1)\\) or stopping on a child who is not the first boy \\((S|N)\\). Additional Parameters for the First Boy Stopping Model \\(p_{S|B1}=\\) probability of stopping after the first boy \\(1 - p_{S|B1}=\\) probability of not stopping after the first boy \\(p_{S|N}=\\) probability of stopping after a child who is not the first boy \\(1 - p_{S|N}=\\) probability of not stopping after a child who is not the first boy Our interest centers on whether the probability of stopping after the first boy, \\(p_{S|B1}\\) is greater than stopping when it is not a first boy, \\(p_{S|N}\\). Table 2.8: Likelihood contributions for families in Model 3: Waiting for a boy. Family Composition Number of families Likelihood Contribution B 930 pB pS|B1 G 951 (1-pB) pS|N BB 582 pB2 (1-pS|B1) pS|N BG 666 pB (1-pB) (1-pS|B1) pS|N GB 666 (1-pB)pB (1-pS|N) pS|B1 GG 530 (1-pB)2 (1-pS|N)pS|N BBB 186 pB3 (1-pS|B1) (1-pS|N)pS|N BBG 177 pB2 (1-pB) (1-pS|B1) (1-pS|N)pS|N BGG 173 pB (1-pB)2 (1-pS|B1) (1-pS|N)pS|N BGB 148 pB2 (1-pB) (1-pS|B1) (1-pS|N)pS|N GBB 151 pB2 (1-pB) (1-pS|N) (1-pS|B1)pS|N GGB 125 pB (1-pB)2 (1-pS|N)2 pS|B1 GBG 182 pB(1-pB)2 (1-pS|N) (1-pS|B1)pS|N GGG 159 (1-pB)3 (1-pS|N)2 pS|N Table 2.9: Patterns related to stopping decisions. Child is… total children prop of all children n.stops (n.families) prop stopped after these children a boy who is the only boy in the family up to that point 3,986 37.4% 1,721 43.2% not an only boy in the family up to that point 6,686 62.2% 3,905 58.4% a girl who is the only girl in the family up to that point 3,928 36.8% 1,794 45.7% not an only girl in the family up to that point 3,832 63.2% 3,832 56.8% 10,672 5,626 Using calculus, the MLEs are derived to be \\(\\hat{p}_B = 0.507\\), \\(\\hat{p}_{S|B1} = 0.432\\), and \\(\\hat{p}_{S|N} = 0.584\\). These are consistent with intuition. The estimated proportion of boys for this model is the same as the estimate for the Sex Unconditional Model(Model 1). The estimates of the stopping parameters are consistent with the fact that of the 3,986 first boys, parents stop 43.2% of the time and of the 6,686 children who are not first boys childbearing stopped 58.4% of the time. See Table 2.9 These results do, in fact, suggest that the probability a couple stops childbearing on the first boy is different than the probability of stopping at a child who is not the first boy, but the direction of the difference does not imply that couples “wait for a boy,” rather it appears that they are less likely to stop childbearing after the first boy in comparison to children who are not the first born male. Similarly, for girls, the MLEs are \\(\\hat{p}_{S|G1}\\) = 0.457 and \\(\\hat{p}_{S|N}\\) = 0.568. Once again, the estimates do not provide evidence of waiting for a girl. 2.7.1 Non-nested Models How does the waiting for a boy model compare to the waiting for a girl model? Thus far we’ve seen how nested models can be compared. But these two models are not nested. Two measures referred to as information criteria, AIC and BIC, are useful when comparing non-nested models. Each measure can be calculated for a model using a function of the model’s maximum log-likelihood. You can find the log-likelihood in the output from most modeling software packages. \\(AIC = -2 * \\textrm{maximum log-likelihood } + 2 * \\textrm{variables}\\), where variables represents the number of parameters in the fitted model. AIC stands for Akaike Information Criterion. Because smaller AICs imply better models, we can think of the second term as a penalty for model complexity, the more variables we use the larger the AIC. \\(BIC = -2 * \\textrm{maximum log-likelihood } + log(nobs) * \\textrm{variables}\\), where variables is the number of parameters and nobs is the number of observations. BIC stands for Bayesian Information Criterion, also known as Schwarz’s Bayesian criterion (SBC). Here we see that the penalty for the BIC differs from the AIC, where the log of the number of observations heavily penalizes models built with large datasets. So which explanation of the data seems more plausible—waiting for a boy or waiting for a girl? These models are not nested (i.e., one is not a simplified version of the other), so it is not correct to perform a Likelihood Ratio Test, but we can legitimately compare these models using information criteria. Table 2.10: Measures of model performance: Waiting for a Booy vs. Waiting for a Girl Model. Waiting for a boy Waiting for a girl max log-likelihood -14661.17 -14715.65 AIC 29324.33 29433.31 BIC 29331.61 29440.58 Smaller AIC and BIC are preferred, so here the Waiting for a Boy Model is judged superior to the Waiting for a Girl Model suggesting that couples waiting for a boy is a better explanation of the data than waiting for a girl. However, for either boys and girls, couples do not stop more frequently after the first occurrence. Other stopping rule models are possible. Another model could be that couples wait to stop until they have both a boy and a girl. We leave the consideration of this balance-preference model as an exercise. 2.8 Summary of Model Building Using a Likelihood Ratio Test, we found statistical evidence that the Sex Conditional Model (Sex Bias) is preferred to the Sex Unconditional Models. However, the parameter estimates were not what we expected if we believe that sex runs in families. Quite to the contrary, the results suggested that if there were more of one sex in a family, the next child is likely to be of the other sex. The results may support the idea that gender composition tends to “even out” over time. Using AICs and BICs to compare the non-nested models of waiting for a boy or waiting for a girl, we found that the model specifying stopping for a first boy was superior to the model for stopping for the first girl. Again, neither model suggested that couples were more likely to stop after the first male or female, rather it appeared just the opposite—couples were less likely to be stopping after the first boy or first girl. These results may need to be considered conditional on the size of a family. In which case, a look at the exploratory data analysis results may be informative. The reported percentages in Table 2.5 could be compared to the percentages expected if the sex of the baby occurs randomly, P(all one sex|2 child family) = 1/2, and we observed 45%. For three child families, P(all one sex|3 child family) = 1/4, and we observed 27%. There is very slight evidence for sex running in families for three child families and none for two child families. Under a random model that assumes the probability of a boy is 50%, the percentage of one-, two- and three- child families with the the first boy showing up last in the family is 50%, 25%, and 12.5%, respectively. Comparing these probabilities to what was observed in the data in Table 2.6, we find little support for the idea that couples are waiting for a boy. We can perform a LRT to compare stopping at the first boy to a Random Stopping Model. The parameters for the first model(waiting for a boy) are \\(p_B, p_{S|B1}, p_{S|N}\\) and the parameters for the second model(random stopping) are \\(p_B\\) and \\(p_S\\). The results suggest that the Waiting for a Boy Model is significantly better than the Random Stopping Model. The Random Stopping Model takes into account that the odds of stopping after a child are not 50-50, but may be closer to the MLE for \\(p_S\\) of 53. (=52.7%). We leave the derivation of this result as an exercise. 2.9 Likelihood-based Methods With likelihood methods, we are no longer restricted to independent, identically distributed normal responses (iidN). Likelihood methods can accommodate non-normal responses and correlated data. Likelihood-based methods are useful for every model in this text, so that it is worth your time and effort to understand them. Models that in the past you would fit using ordinary least squares can also be fit using the principle of maximum likelihood. It is pleasing to discover that under the right assumptions the maximum likelihood estimates (MLEs) for the intercept and slope in a linear regression are identical to ordinary least squares estimators (OLS) despite the fact that they are obtained in quite different ways. Beyond the intuitively appealing aspects of MLEs, they also have some very desirable statistical properties. You learn more about these features in a statistical theory course. Here we briefly summarize the highlights in non-technical terms. MLEs are consistent; i.e., MLEs converge in probability to the true value of the parameter as the sample size increases. MLEs are asymptotically normal; as the sample size increases, the distribution of MLEs is closer to normal. MLEs are efficient because no consistent estimator has a lower mean squared error. Of all the estimators that produce unbiased estimates of the true parameter value, no estimator will have a smaller mean square error than the MLE. While likelihoods and powerful and flexible, there are times when likelihood-based methods fail: MLEs do not exist, likelihoods cannot be written down, or MLEs cannot be written explicitly. It is also worth noting that other approaches to the likelihood, such as bootstrapping, can be employed. 2.10 Likelihoods and this Course [@Rodgers2001] noted that Many factors have been identified that can potentially affect the human sex ratio at birth. A 1972 paper by Michael Teitelbaum accounted for around 30 such influences, including drinking water, coital rates, parental age, parental socioeconomic status, birth order, and even some societal-level influences like wars and environmental pathogens. This chapter on likelihood ignored these complicating factors and was intentionally kept simple to impress you with the fact that likelihoods are conceptually straightforward. Likelihoods answer the sensible question of how likely you are to see your data in different settings. When the likelihood is simple as in this chapter, you can roughly determine an MLE by looking at a graph or you can be a little more precise by using calculus or, most conveniently, software. As we progress throughout the course, the likelihoods will become more complex and numerical methods may be required to obtain MLEs, yet the concept of an MLE will remain the same. Likelihoods will show up in parameter estimation, model performance assessment, and comparisons. One of the reasons many of the likelihoods will become complex is because of covariates. Here we estimated probabilities of having a boy in different settings, but we did not use any specific information other than sex composition about families. The problems in the remainder of the book will typically employ covariates. For example, suppose we had information on paternal age for each family. Consider the Sex Unconditional Model, and let \\[ p_B= \\frac{e^{\\beta_0+\\beta_1(\\textrm{parental age})}} {1+e^{\\beta_0+\\beta_1(\\textrm{parental age})}} \\]. (We will give a good reason for this crazy looking expression for \\(p_B\\) in later chapters.) The next step would be to replace \\(p_B\\) in the likelihood, \\(Lik(p_B)\\), (equation (2.2)), with the complicated expression for \\(p_B\\). The result would be a function of \\(\\beta_0\\) and \\(\\beta_1\\). We could then use calculus to find the MLEs for \\(\\beta_0\\) and \\(\\beta_1\\). Another compelling reason for likelihoods occurs when we encounter correlated data. For example, models with conditional probabilities do not conform to the independence assumption. The Sex Conditional Model is an example of such a model. We’ll see that likelihoods can be useful when the data has structure such as multilevel that induces a correlation. A good portion of the book addresses this. When the responses are are not normal such as in generalized linear models where we see binary responses and responses which are counts we’ll find it difficult to use the linear models of the past and we’ll find the flexibility of likelihood methods to be extremely useful. Likelihood methods will significantly broaden your statistical horizons! 2.11 Exercises 2.11.1 Conceptual Exercises Suppose we plan to use data to estimate one parameter, \\(p_B\\). When using a likelihood to obtain an estimate for the parameter, which is preferred a large or a small likelihood value? Why? The height of a likelihood curve is the probability of the data for the given parameter. The horizontal axis represents different possible parameter values. Does the area under the likelihood curve for an interval from .25 to .75 equal the probability that the true probability of a boy is between 0.25 and 0.75? Suppose the families with an “only child” were excluded for the Sex Conditional Model. How might the estimates for the three parameters be affected? Would it still be possible to perform a Likelihood Ratio Test to compare the Sex Unconditional and Sex Conditional Models? Why or why not? Come up with an alternative model to investigate whether “sex runs in families.” 2.11.2 Guided Exercise Write out the likelihood for a model which assumes the probability of a girl equals the probability of a boy. Carry out a LRT to determine whether there is evidence that the two probabilities are not equal. Comment on the practical significance of this finding (there is not necessarily one correct answer). Case 3 In Case 1 we used hypothetical data with 30 boys and 20 girls. Case 2 was a much larger study with 600 boys and 400 girls. Consider Case 3, a hypothetical data set with 6000 boys and 4000 girls. Use the methods for Case 1 and Case 2 and determine the MLE for \\(p_B\\) for the independence model. Compare your result to the MLEs for Cases 1 and 2. Describe how the graph of the log-likelihood for Case 3 would compare to the log-likelihood graphs for Cases 1 and 2. Compute the log-likelihood for Case 3. Why is it incorrect to perform an LRT comparing Cases 1, 2, and 3? Write out an expression for the likelihood of seeing our data (5,416 boys and 5,256 girls) if the true probability of a boy is: \\(p_B=0.5\\) \\(p_B=0.45\\) \\(p_B= 0.55\\) \\(p_B= 0.5075\\) Compute the value of the log-likelihood for each of the values of \\(p_B\\) above. Which of these four possibilities, \\(p_B=0.45, p_B=0.5, p_B=0.55,\\) or \\(p_B=0.5075\\) would be the best estimate of \\(p_B\\) given what we observed (our data)? Compare the Waiting for a Boy Model to a Random Stopping Model. The parameters for the first model (Waiting for a Boy) are \\(p_B\\), \\(p_{S|B1}\\), \\(p_{S|N}\\) and the parameters for the second model (Random Stopping) are \\(p_B\\) and \\(p_S\\). Use an intuitive approach to arrive at the MLEs for the parameters for each model. Perform a LRT to compare these two models. 2.11.3 Open-ended Exercise Another Stopping Rule Model: Balance-preference Can you construct a model which suggests that childbearing is stopped when couples have a boy and a girl? Define the parameter(s) for balance-preference stopping combined with the sex conditional model and write out likelihood contributions for the same family compositions that appear in Table 2.3. Extra Credit Obtain the MLEs for your balance-preference model parameters and interpret the results. Extra Extra Credit Compare the balance-preference stopping rule model to the random stopping model using a LRT. "],
["ch-distthry.html", "Chapter 3 Distribution Theory 3.1 Introduction 3.2 Modeling Responses 3.3 Distributions used in Testing 3.4 Mixtures 3.5 Exercises", " Chapter 3 Distribution Theory 3.1 Introduction What if it is not plausible that a response is normally distributed? You may want to construct a model to predict whether a prospective student will enroll at a school or model the lifetimes of patients following a particular surgery. In the first case you have a binary response (enrolls (1) or does not enroll (0)), and in the second case you are likely to have very skewed data with many similar values and a few hardy souls with extremely long survival. These responses are not expected to be normally distributed; other distributions will be needed to describe and model binary or lifetime data. Non-normal responses are encountered in a large number of situations. Luckily, there are quite a few possibilities for models. In this chapter, we begin with some general definitions, terms, and notation for different types of distributions with some examples of applications. We then create new random variables using combinations of random variables. A table of contents is presented below; details forthcoming. 3.1.1 Characteristics of Random Variables 3.1.2 Location, scale and shape parameters 3.2 Modeling Responses 3.2.1 Discrete Random Variables 3.2.1.1 Hypergeometric 3.2.2 Bernoulli Process 3.2.2.1 Binary Random Variable 3.2.2.2 Binomial 3.2.2.3 Geometric 3.2.2.4 Negative Binomial 3.2.3 Poisson Process 3.2.3.1 Poisson Random Variable 3.2.4 Continuous Random Variables 3.2.4.1 Exponential 3.2.4.2 Gamma 3.2.4.3 Gaussian or Normal 3.2.4.4 Beta 3.3 Distributions used in Testing 3.3.1 \\(\\chi^2\\) 3.3.2 F 3.4 Mixtures 3.4.1 Zero-inflated Poisson 3.4.2 Mixture of Two Normal Distributions 3.4.3 Beta-Binomial 3.4.4 Negative Binomial 3.5 Exercises 3.5.1 Conceptual Exercises 3.5.2 Guided Exercises "],
["ch-poissonreg.html", "Chapter 4 Poisson Regression 4.1 Learning Objectives 4.2 Preface 4.3 Introduction to Poisson Regression 4.4 Case Studies Overview 4.5 Case Study: Household Size in Haiti 4.6 Using Likelihoods to fit Poisson Regression Models (Optional) 4.7 Modeling 4.8 Case Study: Campus Crime 4.9 Analysis for crime data 4.10 Checking Assumptions 4.11 Modeling 4.12 Overdispersion 4.13 Case Study: Weekend drinking {cs:drinking} 4.14 References 4.15 Exercises", " Chapter 4 Poisson Regression 4.1 Learning Objectives After finishing this chapter, you should be able to: Describe a Poisson process and a corresponding Poisson random variable. Describe why simple linear regression is not ideal for Poisson data. Write out a Poisson regression model and identify the assumptions for inference. Write out the likelihood for a Poisson regression and describe how it could be used to estimate coefficients for a model. Interpret estimated coefficients from a Poisson regression and construct confidence intervals for them. Use deviances for Poisson regression models to compare and assess models. Use an offset to account for varying effort in data collection. Fit and use a zero-inflated Poisson (ZIP) model. 4.2 Preface This chapter introduces themes and approaches to modeling that you will use throughout Broaden Your Statistical Horizons. Major ideas you will see again and again include: Response: Explicitly define your response. This will be very helpful in choosing a modeling approach. Research question: The research question will be your road map as you proceed through your analysis, so do your best to articulate it. EDA: Tailor your exploratory data analysis to your research question(s). What kinds of graphs and numerical summaries will help you understand your research question? Modeling Fitting a model involves estimating coefficients using likelihoods, not least squares. Checking assumptions for models using graphs and numerical summaries. Comparing models using deviances. 4.3 Introduction to Poisson Regression 4.3.1 Initial Examples Are the number of motorcycle deaths in a given year related to a state’s helmet laws? Does the number of rapes on a campus during a year differ for public and private colleges? Does the daily number of asthma-related visits to an Emergency Room differ depending on air pollution indices? Has the number of deformed fish in randomly selected Minnesota lakes been affected by changes in trace minerals in the water over the last decade? Each example involves predicting a response using one or more explanatory variables, although these examples have response variables that are counts per some unit of time or space. We can explicitly define responses for these examples and the corresponding explanatory variables in the following way: Example 1: Y = number of motorcycle deaths in a given year by state X = helmet law indicator Example 2: Y = number of rapes on a college campus in a school year X = private or private school indicator Example 3: Y = number of daily asthma-related visits to an Emergency Room X = an air pollution index Example 4: Y = number of deformed fish in a randomly selected square meter of a Minnesota lake X = trace mineral measurements 4.3.2 Poisson Random Variables A Poisson random variable is often used to model counts. A description of key features of Poisson random variable follows. Properties of a Poisson random variable Y= number of events per unit (time or space) Possible values: 0, 1, 2, \\(\\ldots \\infty\\) Mean = \\(\\lambda\\) Variance = \\(\\lambda\\) which implies standard deviation = \\(\\sqrt{\\lambda}\\) \\(P(Y=y)=\\frac{e^{-\\lambda}\\lambda^y}{y!}\\) The sum of Poisson variables is also Poisson. The parameter of interest in Poisson modeling is \\(\\lambda\\), the rate of events per unit of time or space. The observed count, \\(Y\\), is made over those units of time (e.g., year, hour) or space (e.g., acre, cubic foot). The unit for the first example is calendar year, the second is school year, the third is a day, and the last is a square meter of a Minnesota lake. Define the unit so the rate, \\(\\lambda\\), is meaningful. 4.3.2.1 An example calculating Poisson probabilities Let Y = the number of soccer goals scored in a game by DC United. Suppose that it is known that on average DC United scores 1.02 goals per game (\\(\\lambda\\)=1.02). How frequently would exactly two goals be scored by DC United during a game? With Property 5 and the information that \\(\\lambda\\)=1.02 goals per game, we can calculate the probability for any number of goals per game using: \\[ P(Y=y)=\\frac{e^{-1.02}{1.02}^y}{y!} \\] For our example, we expect to see exactly 2 goals in a game 18.8% of the time: \\[P(Y=2)=\\frac{e^{-1.02}{1.02}^2}{2!} = 0.188 \\] Using the laws of probability it is also straightforward to calculate the chance of any events of interest such as seeing more than 2 goals in a game or at least one goal in a game. Conversely, in statistical applications you will have data such as Y=2, and your aim will be to use your data to estimate \\(\\lambda\\). 4.3.3 Modeling with Poisson variables A Poisson random variable is a count, so its minimum value is zero and, in theory, the maximum is unbounded. We’d like to model the parameter \\(\\lambda\\), the average count per unit, as a function of one or more covariates. For an OLS linear regression model, the parameter of interest is the average response, \\(\\mu_i\\), for subject \\(i\\), and \\(\\mu_i\\) is modeled as a line in the case of one explanatory variable. By analogy, it might seem reasonable to try to model the Poisson parameter \\(\\lambda\\) as a linear function of an explanatory variable but there are some problems with this approach. In fact, a model like \\(\\lambda_i=\\beta_0+\\beta_1x_i\\) doesn’t work well for Poisson data. A line is certain to yield negative values for certain \\(x_i\\), but \\(\\lambda_i\\) can only take on values from 0 to \\(\\infty\\). In addition, the equal variance assumption in linear regression is violated because as the mean rate for a Poisson variable increases the variance also increases (recall \\(E(Y)=Var(Y)=\\lambda\\)). One way to avoid these problems is to model log(\\(\\lambda_i\\)) instead of \\(\\lambda_i\\) as a function of the covariates. The log(\\(\\lambda_i\\)) takes on values from \\(-\\infty\\) to \\(\\infty\\). We can also take into account the increase in the variance using this approach. (Note that throughout Broaden Your Statistical Horizons we use log to represent the natural logarithm.) Thus, we will consider the Poisson regression model: \\[\\begin{equation} log(\\lambda_i)=\\beta_0+\\beta_1x_i \\tag{4.1} \\end{equation}\\] where the observed values \\(Y_i \\sim\\) Poisson with \\(\\lambda=\\lambda_i\\) for a given \\(x_i\\). 4.3.3.1 Poisson Regression Assumptions Much like OLS, using Poisson regression to make inferences requires model assumptions. Linearity The log of the mean rate, log(\\(\\lambda\\)), must be a linear function of x. Mean=Variance By definition, the mean of a Poisson random variable must be equal to its variance. Independence The observations must be independent of one another. 4.3.4 A Graphical Look at Poisson Regression Figure 4.1: Regession Models: Linear Regression (left) and Poisson Regression (right) Figure 4.1 illustrates a comparison of the OLS model for inference to Poisson regression using a log function of \\(\\lambda\\). The graphic displaying the ordinary least squares (OLS) inferential model appears in the left panel of Figure 4.1. It shows that for each level of X, the responses appear to be approximately normal. The panel on the right side of Figure 4.1 depicts what a Poisson regression model looks like. For each level of X, the responses follow a Poisson distribution. For Poisson regression small values of \\(\\lambda\\) are associated with a distribution that is noticeably skewed with lots of small values and only a few larger ones. As \\(\\lambda\\) increases the distribution of the responses begins to look more and more like a normal distribution. In the case of OLS, the mean responses for each level of X, \\(\\mu_{Y|X}\\), fall on a line. In the case of the Poisson model, the mean values of \\(Y\\) at each level of \\(X\\), \\(\\lambda_{Y|X}\\), fall on a curve, not a line. In the OLS regression model, the variation in \\(Y\\) at each level of X, \\(\\sigma^2\\), is the same. For Poisson regression the responses at each level of X become more variable with increasing means. 4.3.4.1 Poisson Models That single parameter, \\(\\lambda\\), specifies both the mean and variance of the distribution of counts. Thus as the mean increases the variance does as well. This is in contrast to OLS assumptions for inference which require the variance to be constant for all mean values of \\(Y\\). While relaxing the assumption of equal variances is convenient, specifying that the variance be equal to the mean is rather restrictive. It would be nice to be able to uncouple the specification of the mean and variance instead of having them be exactly equal to one another. You will see later in this chapter that there are ways in which to modify the Poisson model to allow for more flexibility. These modifications reflect a principle for Broadening Your Statistical Horizons: modeling can be flexible. When modeling, you are not restricted to only a few options. 4.3.4.2 Poisson Process (Optional) A Poisson model for counts can be thought of as being generated from a Poisson process. A time period or space can be broken into very small, equal size units. The probability of an event within a given unit is very small and independent of other units. For example, the probability of a rape within a very short period of time is very small. A Poisson model implies that the probability of a rape occurring during another very small, equal length time period is the same and independent of a rape occurring within another time period of equal length. Two related facts are worth noting. First, observations are independent. Second events occur during this process at a constant rate. Therefore, a Poisson process can be seen as the limiting case of a binomial distribution as \\(n \\rightarrow \\infty\\) and \\(p \\rightarrow 0\\). A Poisson process can also be generated by using exponentially distributed waiting times between events. 4.4 Case Studies Overview We take a look at the Poisson regression model in the context of three case studies. Each case study illustrates different concepts encountered in Poisson regression. They are based on real data and real questions. Modeling the household size in Haiti introduces the idea of regression with a Poisson response along with its assumptions. A quadratic term is added to a model to determine an optimal size per household, and methods of model comparison are introduced. The campus crime case study introduces two big ideas in Poisson regression modeling: offsets, to account for sampling effort, and overdispersion, when actual variability exceeds what is expected by the model. Finally, the weekend drinking example uses a modification of a Poisson model to account for more zeros than would be expected for a Poisson random variable. These three case studies also provide context for some of the familiar concepts related to modeling such as exploratory data analysis, estimation, and residual plots. An optional section demonstrates the use of likelihoods to fit Poisson models. 4.5 Case Study: Household Size in Haiti How many other people live with you in your home? The number of people sharing a house differs from country to country and often from village to village. International agencies use household size when determining needs of populations. Household sizes determine the magnitude of the household needs. Much of the work for maintaining a household falls to the female head of the household. At what age can a female head of a household expect to be caring for the largest number of people? The 2010 Census enumerated 308.7 million people in the United States, a 9.7 percent increase from 281.4 million in Census 2000. Of the total population in 2010, 300.8 million lived in 116.7 million households for an average of 2.58 people per household. Households and Families: 2010 - Census Bureau [@census2010] In a 2010 Census in Haiti, there was an average of 4.9 persons per household headed by women with a maximum of 15. The typical house in Haiti, however, differs markedly from one in the US. Of the 560 households in our data, there are anywhere from 1 to 13 people in the house and over 30% of these households have a thatched roof. This first case study introduces a number of key ideas. Here is a preview: Modeling: coefficients are estimated using likelihoods. Interpretation: Exponentiate coefficients for better interpretation. Assumptions: Check the Poisson assumptions and the fit of the model. Linearity: look at the log of mean responses vs. X Mean=Variance: look at the mean and variance of responses at different levels of X Outliers: look at residual plots Drop-in-Deviance Tests: Compare models using deviances. Overdispersion: When a model is correctly specified and outliers considered, adjust for overdispersion if necessary. 4.5.1 Research Question At what age are women in Haiti most likely to find the largest number of people in their household? Is this association similar for poorer households (measured by the presence of a thatched roof)? We begin by explicitly defining our response, \\(Y=\\) number of people other than the female head of the household, and the explanatory variables: age of the female head of the household, type of roof (thatched, cement or tile), and location (Chouquette, Laferme, Maurace, Roche Jabouin, Rousseau). Our response is a count so we consider a Poisson regression where the parameter of interest is \\(\\lambda\\), the average number of people per household. We will primarily examine the relationship between household size and age of the female head of household controlling for location. 4.5.2 Data Collection In the summer of 2010, 320 households in the region of Port Salut, Haiti, were included in a public health survey to ascertain the health needs of the population. Trained community members completed 320 household surveys in an eight week period. A systematic sampling method was used in which every third (or fourth) house in a village was included in the survey sample.The survey was administered by Dr. Therese Zink of the University of Minnesota. 4.5.3 Data Organization Each line of the data file refers to a household at the time of the survey: age = the age of the female head of household numLT5 = the number in the household under 5 years of age numHouse = the number in the household (including the female head) Roof = the type of roof in the household (thatched or iron/cement) Location = where the house is located (Chouquette, Laferme, Maurace, Roche Jabouin, or Rousseau) Location age numLT5 numHouse Roof 1 Chouquette 32 1 4 thatched 2 Chouquette 31 0 6 thatched 3 Chouquette 45 1 9 thatched 4 Chouquette 26 3 5 iron/cement 5 Chouquette 35 0 4 thatched 6 Chouquette 31 2 4 iron/cement 4.5.4 Exploratory Data Analysis Poisson random variables typically can have responses of no events (zeros). However, as constructed, the response of numHouse does not include any zeros because the female head of the household is also included. We can create a variable more appropriate for Poisson regression by subtracting one from the total in the household thereby describing the number in the household in addition to the female head of household. There are 320 households included in this analysis. For households with female heads, the mean number in the household in addition to the female head of the househhold (HH) is 4.4 (SD=2.5, Var=6.4). The mean number for houses with thatched roofs is 4.77 (SD=2.34, Var=5.5), whereas houses with iron or cement roofs average only 4.28 (SD=2.60, Var=6.8). Of the various locations, Rousseau has the largest households, on average, with a mean of 6.39 in the household, and Chouquette has the smallest with a mean of 3.42. Figure 4.2: Haiti: Distribution of household size Figure 4.2 reveals a fair amount of variability in the number in each house; responses range from 0 to 13 with many of the respondents reporting between 2 and 7 people in the house in addition to the female head of household. Like many Poisson distributions, this graph is right skewed. It clearly does not suggest that the number of people in a household is a normally distributed response. For the remainder of the analysis, we’ll proceed in the following way. Fit a linear model in age (using likelihoods). Interpret the estimated coefficient for the age term and test its significance using a Wald-type test and a drop-in-deviance test.. Assess the assumptions of the linear model. Linearity Independence Mean=Variance A residual plot with no trends or outliers Fit a quadratic model. Test the significance of the coefficient of the age\\(^2\\) term using a Wald-type test. Add the factor Location to the quadratic model to control for differences by village. Introduce the drop-in-deviance test as a way of comparing models Compare the linear to the quadratic model. Compare the quadratic model with and without the Location variable. Examine residual plots for the final model. Use calculus to determine the age at which there is the maximum number of people. Adjust for overdispersion. Make adjustments, if necessary, if the variance exceeds the mean. 4.6 Using Likelihoods to fit Poisson Regression Models (Optional) Before we have a statistical package produce estimated regression coefficients for our Poisson regression model, let’s look under the hood to see how the model is fit. The least squares approach requires a linear relationship between the parameter, \\(\\mu_i\\) (the expected or mean response for observation \\(i\\)), and \\(x_i\\). However, it is log\\((\\lambda_i)\\), not \\(\\lambda_i\\), that is linearly related to X with the Poisson model. As noted, the assumptions of equal variance and normality also do not hold for Poisson regression. So the method of least squares will not be helpful here. Instead of OLS, we employ the likelihood principle to find estimates of our model coefficients. We look for those coefficient estimates for which the likelihood of our data is maximized; these are the maximum likelihood estimates. The likelihood for n independent observations is the product of the probabilities. For example, if we observe five households with numHouse = \\((4,2,8,6,1)\\) the likelihood is: \\[ Likelihood = P(Y_1=4)*P(Y_2=2)*P(Y_3=8)*P(Y_4=6)*P(Y_5=1)\\] Recall that the probability of a Poisson response can be written \\[P(Y=y)=\\frac{e^{-\\lambda}\\lambda^y}{y!}\\] So the likelihood can be written as \\[\\begin{eqnarray*} Likelihood&amp;= &amp;\\frac{ e^{-\\lambda}\\lambda^4 }{ 4! }* \\frac{ e^{-\\lambda}\\lambda^2 }{ 2! } *\\frac{e^{-\\lambda}\\lambda^8}{8!}* \\frac{e^{-\\lambda}\\lambda^6}{6!}*\\frac{e^{-\\lambda}\\lambda^1}{1!}\\\\ \\end{eqnarray*}\\] As in the earlier likelihood chapter, it will be easier to find a maximum if we take the log of the likelihood and ignore the constant term resulting from the sum of the factorials: \\[\\begin{eqnarray} -logL&amp; \\propto&amp; \\lambda_{X=4}-4log(\\lambda_{X=4})+\\lambda_{X=2}-2log(\\lambda_{X=2}) \\\\ &amp; &amp;+\\lambda_{X=8}-8log(\\lambda_{X=8})+\\lambda_{X=6}-6log(\\lambda_{X=6})\\\\ &amp; &amp;+\\lambda_{X=1}-log(\\lambda_{X=1}) \\tag{4.2} \\end{eqnarray}\\] Now if we had the age of the female head of the household for each house (X), we consider the Poisson regression model: \\[ log(\\lambda_i)=\\beta_0+\\beta_1x_i \\] This implies that \\(\\lambda\\) differs for each age and can be determined using \\[\\lambda_i=e^{\\beta_0+\\beta_1x_i.}\\] If the ages are \\(X=(22,21,25,24,22)\\) years, our loglikelihood can be written: \\[\\begin{eqnarray} logL &amp;= &amp;[-e^{\\beta_0+\\beta_122}+4({\\beta_0+\\beta_122})]+ [-e^{\\beta_0+\\beta_121}+2({\\beta_0+\\beta_121})]+\\\\ &amp; &amp; [-e^{\\beta_0+\\beta_125}+8({\\beta_0+\\beta_125})]+ [-e^{\\beta_0+\\beta_124}+6({\\beta_0+\\beta_124})]+ \\\\ &amp; &amp; [-e^{\\beta_0+\\beta_122}+({\\beta_0+\\beta_122})] \\tag{4.3} \\end{eqnarray}\\] To see this, match the terms in Equation (4.2) with those in Equation (4.3) noting that \\(\\lambda_i\\) has been replaced with \\(e^{\\beta_0+\\beta_1x_i}\\). It is Equation (4.3) that will be used to estimate the coefficients \\(\\beta_0\\) and \\(\\beta_1\\). Although this looks a little more complicated than the loglikelihoods we saw in Chapter 2, the fundamental ideas are the same. In theory, we try out different possible values of \\(\\beta_0\\) and \\(\\beta_1\\) until we find the two for which the loglikelihood is largest. Most statistical software packages have automated search algorithms to find those values for \\(\\beta_0\\) and \\(\\beta_1\\) that maximize the loglikelihood. 4.7 Modeling We first consider a model for which log(\\(\\lambda\\)) is linear in age. We check the Poisson assumptions to determine whether a model with a quadratic term in age provides a significant improvement. 4.7.1 First Order Model Although we intend to fit a quadratic model, we’ll start with a linear model. Output for the linear model appears below. glm(formula = numHouse_1 ~ age, family = poisson, data = fHH1) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.624526 0.103838 6.014 1.8e-09 *** age 0.027403 0.003075 8.912 &lt; 2e-16 *** --- (Dispersion parameter for poisson family taken to be 1) Null deviance: 473.56 on 319 degrees of freedom Residual deviance: 392.79 on 318 degrees of freedom AIC: 1412.6 # CI for betas using profile likelihood confint(modela) ## 2.5 % 97.5 % ## (Intercept) 0.41924146 0.82633118 ## age 0.02139044 0.03344555 exp(confint(modela)) ## 2.5 % 97.5 % ## (Intercept) 1.520808 2.284920 ## age 1.021621 1.034011 # Wald type CI by hand beta1hat &lt;- summary(modela)$coefficients[2,1] beta1se &lt;- summary(modela)$coefficients[2,2] beta1hat - 1.96*beta1se # lower bound ## [1] 0.02137664 beta1hat + 1.96*beta1se # upper bound ## [1] 0.03343023 # p-value for test comparing the null and first order models drop.in.dev &lt;- modela$null.deviance - modela$deviance drop.in.dev ## [1] 80.77206 diff.in.df &lt;- modela$df.null - modela$df.residual diff.in.df ## [1] 1 1-pchisq(drop.in.dev, diff.in.df) ## [1] 0 4.7.2 Estimation and Inference Most software packages report the estimated coefficients so that the estimated regression equation for the previous model is: \\[ \\widehat{log(\\lambda)} = 0.625 + 0.0274 (age) \\] How can the coefficient estimates be interpreted in terms of this application? As done when interpreting slopes in the OLS models, we consider how the estimated mean number in the house, \\(\\lambda\\), changes as the age of the household head increases by an additional year. But in place of looking at change in the mean number in the house, with a Poisson regression we consider the log of the mean number in the house and then convert back to original units. \\[\\begin{eqnarray} log(\\lambda_X) &amp;=&amp; \\beta_0 + \\beta_1X \\\\ log(\\lambda_{X+1}) &amp;=&amp; \\beta_0 + \\beta_1(X+1) \\\\ log(\\lambda_{X+1})-log(\\lambda_X) &amp;= &amp; \\beta_1 \\\\ log \\left(\\frac{\\lambda_{X+1}}{\\lambda_X}\\right) &amp;= &amp;\\beta_1\\\\ \\frac{\\lambda_{X+1}}{\\lambda_X} &amp;=&amp; e^{\\beta_1} \\tag{4.4} \\end{eqnarray}\\] These results suggest that by exponentiating the coefficient on age we obtain the multiplicative factor by which the mean count changes. In this case, the mean number in the house changes by a factor of \\(e^{.0274}=1.028\\) or increases by a factor of 2.8% with each additional year older the household head is. The quantity on the left hand side of Equation (4.4) is referred to as a rate ratio or relative risk, and it represents a percent change in the response for unit changes in X. In fact, for regression models in general, whenever a variable (response or explanatory) is logged, we make interpretations about multiplicative effects on that variable, while with unlogged variables we can reach our usual interpretations about additive effects. Typically the standard errors for the estimated coefficients are included in Poisson regression output. Here the standard error for the estimated coefficient for age is 0.003075 so we can construct a confidence interval for \\(\\beta_1\\). A 95% CI provides a range of plausible values for the age coefficient and can be constructed: \\[0.0274-1.96*0.0031, 0.0274+1.96*0.0031\\] \\[ (.0214, .0334). \\] Exponentiating the endpoints yields a confidence interval for the relative risk, i.e., the percent change in household size for each additional year older. Thus \\((e^.0214,e^.0334)=(1.022,1.034)\\) suggests that we are 95% confident that the mean number in the house increases between 2.2% and 3.4% for each additional year older the head of household is. It is best to construct a CI for the coefficient and then exponentiate the endpoints because the estimated coefficients are closer to normal than the exponentiated coefficients. There are other approaches to constructing intervals in these circumstances, including profile likelihood, the delta method, and bootstrapping, but we do not discuss them here. If the null model were true, there is no change in household size for each additional year, so that \\(\\lambda_X\\) is equal to \\(\\lambda_{X+1}\\) and the ratio \\(\\lambda_{X+1}/\\lambda_X\\) is 1. Note that our interval for \\(e^{\\beta_1}\\), (1.022, 1.034), does not include 1, so the model with age is preferred to a model without age; i.e., age is significantly associated with household size. Note that we could have similarly confirmed that our interval for \\(\\beta_1\\), (.0214, .0334), does not include 0 to show the significance of age as a predictor of household size. One way to test the significance of the age term is to calculate a Wald-type statistic. A Wald-type test statistic is the estimated coefficient divided by its standard error. When the true coefficient is 0, this test statistic follows a standard normal distribution for sufficiently large \\(n\\). The estimated coefficient associated with the linear term in age is \\({\\hat{\\beta_1}}=0.0274\\) with standard error \\(SE(\\hat{\\beta_1})=0.003075\\). The value for the Wald test statistic is then \\(Z=\\hat{\\beta_1}/SE(\\hat{\\beta_1})=8.912\\), whose two-sided p-value based on the normal distribution for testing \\(H_O:\\beta_1=0\\) is tiny (“&lt;2e-16”). In conclusion, we have statistically significant evidence (Z = 8.91, p &lt; .001) that average household size increases as age of the female head of household increases. Another way in which to assess the contribution of the age term is to perform a drop-in-deviance test. 4.7.3 Using Deviances to Compare Models There is another way in which to assess how useful age is in the model. A deviance is a way in which to measure how the observed data deviates from the model predictions (it will be defined more precisely later). Keeping in mind that the deviance describes the remaining unexplained variation, we calculate the drop-in-deviance when adding age to the model with no covariates (the null model). The deviances for the null model and the model with age can be found on the model output. A residual deviance for the model with age is reported as 392.79 with 318 df. The output also includes the deviance and df for the null model (473.56 with 319 df). The drop-in-deviance is (473.56 - 392.79 = 80.77) with a difference of only 1 df. Theory (not covered here) tells us that if the null model is true, we would expect the drop-in-deviance to follow a \\(\\chi^2\\) distribution with 1 df. Therefore the p-value for comparing the null model to the model with age is found by determining the probability that the value for \\(\\chi^2\\) with one df exceeds 80.77, which is essentially 0. Once again, we can conclude that we have statistically significant evidence (\\(\\chi^2=80.77\\), \\(p &lt; .001\\)) that average household size increases as age of the female head of household increases. In order to use the drop-in-deviance test, the models being compared must be nested; i.e., all the terms in the smaller model must appear in the larger model. Here the smaller model is the null model with the single term \\(\\beta_0\\) and the larger model has \\(\\beta_0\\) and \\(\\beta_1\\), so the two models are indeed nested. For nested models, we can compare the models’ residual deviances to determine whether the larger model provides a significantly improvement. Here, then, is a summary of these two approaches to hypothesis testing about terms in Poisson regression models: Drop-in-deviance test to compare models Compute the deviance for each model, then calculate: drop-in-deviance = residual deviance for reduced model - residual deviance for the larger model. When the reduced model is true, the drop-in-deviance \\(\\sim \\chi^2_d\\) where d= the difference in the degrees of freedom associated with the two models (that is, the difference in the number of terms / coefficients). A large drop-in-deviance favors the larger model. Wald test for a single coefficient Wald-type statistic = estimated coefficient / standard error When the true coefficient is 0, for sufficiently large \\(n\\), the test statistic \\(\\sim\\) N(0,1). If the magnitude of the test statistic is large, there is evidence that the true coefficient is not 0. The drop-in-deviance and the Wald-type tests usually provide consistent results; however, if there is a discrepancy the drop-in-deviance is preferred. Not only does the drop-in-deviance test perform better in more cases, but it’s also more flexible. If two models differ by one term, then the drop-in-deviance test essentially tests if a single coefficient is 0 like the Wald test does, while if two models differ by more than one term, the Wald test is no longer appropriate. In this case, the Wald-type test and the drop-in-deviance test both suggest that a linear term in age is useful. But are the model assumptions satisfied? 4.7.4 Poisson Regression Assumptions 4.7.4.1 Linearity Figure 4.3: The log of the mean number in each household by grouped age of the female head of household, with loess smoother. Recall that the Poisson regression model implies that log(\\(\\lambda_i\\)), not the mean \\(\\lambda_i\\), is a linear function of \\(x_i\\). Therefore, to check the linearity assumption for Poisson regression we would like to plot log(\\(\\lambda_i\\)) by age,\\(x_i\\). Unfortunately, \\(\\lambda_i\\) is unknown. Our best guess of \\(\\lambda_i\\) is the observed mean number in the household for each age (level of \\(X\\)). Because these means are computed for observed data, they are referred to as empirical means. Taking the logs of the empirical means and plotting by age provides a way to assess the linearity assumption. The smoothed curve added to Figure 4.3 suggests that there is a curvilinear relationship between age and the log of the mean household size implying that adding a quadratic term should be considered. This finding is consistent with the researchers’ hypothesis that there is an age at which a maximum household size occurs. It is worth noting that we are not modeling the log of the empirical means, rather it is the log of the true rate that is modeled. Looking at empirical means, however, does provide an idea of the form of the relationship between log(\\(\\lambda)\\) and \\(x_i\\). 4.7.4.2 Mean = Variance For Poisson random variables, the variance of \\(Y\\) (i.e., the square of the standard deviation of \\(Y\\)), is equal to its mean, where \\(Y\\) represents the size of an individual household. As the mean increases, the variance increases. So, if the response is a count and the mean and variance are approximately equal for each group of \\(X\\), a Poisson regression model may be a good choice. In Table 4.1 we display only a portion of the age groups to check to see if the empirical means and variances of the number in the house are approximately equal for each age group. This provides us one way in which to check the Poisson assumption. Table 4.1: Compare mean and variance of household size within each age group Age Groups Mean Variance n (15,20] 3.720930 2.348837 43 (20,25] 4.648148 3.553110 54 (25,30] 4.769231 3.828054 52 (30,35] 6.058823 5.160667 68 (35,40] 6.642857 6.722996 42 (40,45] 6.574074 10.135919 54 (45,50] 9.000000 0.000000 2 (50,55] 5.000000 NA 1 NA 3.250000 1.583333 4 If there is a problem with this assumption most often we see variances much larger than means. Here we see that, for the most part, variances that are smaller than the means. Thus, there is no compelling evidence of a problem with the mean=variance assumption, although we should be aware of high variability among older heads of households. 4.7.4.3 Independence The independence assumption can be assessed using knowledge of the study design and the data collection process. In this case, we do not have enough information to assess the independence assumption with the information we are given. If selections of groups of households were made from different villages with differing customs about living arrangements, the independence assumption would be violated. If this were the case, we could use a multilevel model like those discussed in later chapters with a village term. 4.7.5 Residual Plot A residual plot can be of limited use when assessing Poisson regression model assumptions. It may provide some insight into linearity and outliers, although the plots are not quite as useful here as they are for OLS. There are a couple of options for computing residuals and predicted values. Residuals may have the form of residuals for OLS models or the form of deviance residuals (defined in the next section) which, when squared, sum to the total deviance for the model. Predicted values can be estimates of the counts, \\(e^{\\beta_0+\\beta_1X}\\), or estimates of the log of the count, \\(\\beta_0+\\beta_1X\\). We choose to use the deviance residuals and predicted counts. More detail follows. 4.7.6 Residuals for Poisson Models (Optional) Residuals for OLS are useful for comparing models. The residuals for OLS in simple linear regression have the form: \\[\\begin{eqnarray} OLS\\hspace{2mm}residual_i &amp;= &amp;obs_i - fit_i \\\\ &amp;=&amp;{Y_i-\\hat{\\mu_i}} \\\\ &amp;= &amp;Y_i-(\\hat{\\beta_0} +\\hat{\\beta_1}X_i) \\tag{4.5} \\end{eqnarray}\\] Residual sum of squares (RSS) are formed by squaring and adding these residuals, and we generally seek to minimize RSS in model building. We have several options for creating residuals for Poisson regression models. One is to create residuals in much the same way as we do in OLS. For Poisson residuals, the predicted values are denoted by \\(\\hat{\\lambda}_i\\) (in place of \\(\\hat{\\mu_i}\\) in Equation (4.5); they are then standardized by dividing by the standard error, \\(\\sqrt{\\lambda_i}\\). These kinds of residuals are referred to as Pearson residuals and are used with Poisson regression. \\[\\begin{equation} \\textrm{Pearson residual}_i = \\frac{Y_i-\\hat{\\lambda}_i}{\\sqrt{\\hat{\\lambda}_i}} \\tag{4.6} \\end{equation}\\] Pearson residuals have the advantage that you are probably familiar with their meaning and the kinds of values you would expect. For example, after standardizing we expect most residuals to fall between -2 and 2. However, deviance residuals have some useful properties that make them a better choice for Poisson regression. First, we define a deviance residual for an observation from a Poisson regression, \\[\\begin{equation} \\textrm{deviance residual}_i = sign(Y_i-\\hat{\\lambda}_i) \\sqrt{ 2 \\left[Y_i log\\left(\\frac{Y_i}{\\hat{\\lambda}_i}\\right) -(Y_i - \\hat{\\lambda}_i) \\right]} \\tag{4.7} \\end{equation}\\] As its name implies, a deviance residual describes how the observed data deviates from the fitted model. Squaring and summing the deviances for the observations produces the residual deviance \\(=\\sum \\textrm{deviance residual}_i\\). Relatively speaking, observations for good fitting models will have small deviances; that is, the predicted values will deviate little from the observed. However, you can see that the deviance for an observation does not easily translate to a difference in observed and predicted responses as is the case with OLS models. A careful inspection of the deviance formula reveals several places where the deviance compares \\(Y\\) to \\(\\hat{\\lambda}\\): the sign of the deviance is based on the difference between \\(Y\\) and \\(\\hat{\\lambda}\\), and under the radical sign we see the ratio \\(Y/\\hat{\\lambda}\\) and the difference \\(Y -\\hat{\\lambda}\\). When \\(Y = \\hat{\\lambda}\\), that is, when the model fits perfectly, the difference will be 0 and the ratio will be 1 (so that its log will be 0). So like the residuals in OLS, an observation that fits perfectly will not contribute to the sum of the squared deviances. This definition of a deviance depends on the likelihood for Poisson models. Other models will have different forms for the deviance depending on their likelihood. Figure 4.4: Residual plot for the Poisson model of number in the household besides the female HH by age of the female HH A plot (Figure 4.4) of the deviance residuals versus predicted responses for the first order model exhibits curvature indicating that the model may be improved by adding a quadratic term. Other details related to residual plots can be found in a variety of sources including [@McCullagh1989]. Our analysis thus far suggests that a quadratic term in age might be useful to add to the model. 4.7.7 Second Order Model As stated from the outset, our intention is to examine a quadratic model to see if there exists an age where the number in the house is, on average, a maximum. The output for a quadratic model appears below. glm(formula = numHouse_1 ~ age + age2, family = poisson, data = fHH1) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -0.1029160 0.3664001 -0.281 0.77880 age 0.0773529 0.0242037 3.196 0.00139 ** age2 -0.0007935 0.0003809 -2.083 0.03723 * --- (Dispersion parameter for poisson family taken to be 1) Null deviance: 473.56 on 319 degrees of freedom Residual deviance: 388.37 on 317 degrees of freedom AIC: 1410.2 # p-value for test comparing the null and first order models drop.in.dev &lt;- modela$deviance - modela2$deviance drop.in.dev ## [1] 4.42515 diff.in.df &lt;- modela$df.residual - modela2$df.residual diff.in.df ## [1] 1 1-pchisq(drop.in.dev, diff.in.df) ## [1] 0.03541299 We can assess the importance of the quadratic term in two ways. The p-value for the Wald-type statistic for age\\(^2\\) is statistically significant (Z = -2.083, p = .037). Another approach is to perform a drop-in-deviance test. \\(H_0\\): log(\\(\\lambda\\))=\\(\\beta_0+\\beta_1age\\) (reduced model) \\(H_1:\\) log(\\(\\lambda\\))=\\(\\beta_0+\\beta_1age + \\beta_2age^2\\) (larger model) The first order model has a residual deviance = 392.79 with 318 df and the second order model, the quadratic model, has a residual deviance= 388.37 with 317 df. The drop-in-deviance by adding the quadratic term to the linear model is 392.79 - 388.37 = 4.43 which can be compared to a \\(\\chi^2_{1df}\\). The p-value is .035, so the observed drop of 4.42 again provides significant support for including the quadratic term. Some statistical packages will summarize the drop-in-deviance tests in tabular form. The first drop-in-deviance test below (\\(\\chi^2=80.77, p&lt;2*10^{-16}\\)) compares the first-order model to a model with no predictors, while the second drop-in-deviance test (\\(\\chi^2=4.43, p=.035\\)) compares the second-order model to the first-order model. Analysis of Deviance Table Model 1: numHouse_1 ~ 1 Model 2: numHouse_1 ~ age Model 3: numHouse_1 ~ age + age2 Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) 1 319 473.56 2 318 392.79 1 80.772 &lt; 2e-16 *** 3 317 388.37 1 4.425 0.03541 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 4.7.8 Finding the age where the number in the house is a maximum We now have an equation in age which yields the estimated log(mean number in the house). \\[ \\textrm{log(mean numHouse)} = -0.1303 + 0.077(age) - 0.00079 (age^2) \\] With a little calculus, we determine that the maximum estimated additional number in the house is \\(e^{1.746} = 5.73\\) when the head of the household is 48.7 years old. This is consistent with our observations from the graphs. 4.7.9 Adding a covariate Location may account for differences in the size of the household. Assessing the utility of including the covariate Location is, in essence, comparing two nested models; here the quadratic model is compared to the quadratic model plus terms for Location. Results from the fitted model appears below. glm(formula = numHouse_1 ~ age + age2 + Location, family = poisson, data = fHH1) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -0.4076599 0.3826581 -1.065 0.286723 age 0.0840507 0.0246012 3.417 0.000634 *** age2 -0.0009552 0.0003882 -2.461 0.013868 * LocationLaferme 0.3825454 0.1074149 3.561 0.000369 *** LocationMaurace 0.1254272 0.0943147 1.330 0.183558 LocationRoche Jabouin 0.2605722 0.1072576 2.429 0.015124 * LocationRousseau 0.5866752 0.0982018 5.974 2.31e-09 *** --- (Dispersion parameter for poisson family taken to be 1) Null deviance: 473.56 on 319 degrees of freedom Residual deviance: 331.86 on 313 degrees of freedom AIC: 1361.7 Analysis of Deviance Table Model 1: numHouse_1 ~ age + age2 Model 2: numHouse_1 ~ age + age2 + Location Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) 1 317 388.37 2 313 331.86 4 56.507 1.57e-11 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # p-value for test comparing the null and first order models drop.in.dev &lt;- modela2$deviance - modela2L$deviance drop.in.dev ## [1] 56.5066 diff.in.df &lt;- modela2$df.residual - modela2L$df.residual diff.in.df ## [1] 4 1-pchisq(drop.in.dev, diff.in.df) ## [1] 1.570089e-11 Our Poisson regression model now looks like: \\[ \\textrm{log(mean numHouse)} = -0.408 + 0.084*\\textrm{age} - 0.00096*\\textrm{age}^2 + \\\\ .383*\\textrm{Laferme} + .125*\\textrm{Maurace} + .261*\\textrm{Roche Jabouin} + .587*\\textrm{Rousseau} \\] Notice that because there are 5 different locations, we must represent the effects of different locations through 4 indicator variables. For example, \\(\\hat{\\beta_3}=.382\\) indicates that, after controlling for age of the female head of household, log mean household size is .382 higher for households in Laferme than for households in the reference location of Chouquette, or, in more interpretable terms, mean household size is \\(e^.382=1.47\\) times higher (i.e., 47% higher) in Laferme than in Chouquette, holding age constant. To test if the mean number of people in a household significantly differs by location, we must use a drop-in-deviance test because four terms, not one, are added when including the Location variable. By the Analysis of Deviance table above, adding the four terms corresponding to location to the quadratic model with age is statistically significant \\((\\chi^2=56.51,p&lt;0.001)\\); there is significant evidence that mean household size differs by location, even after controlling for age of the female head of household. 4.7.10 Goodness-of-fit The model residual deviance can be used to assess the degree to which the predicted values differ from the observed. When a model is true, we can expect the residual deviance to be distributed as a \\(\\chi^2\\) with the degrees of freedom equal to the model’s residual degrees of freedom. Here, our model thus far, the quadratic plus indicators for location, has a residual deviance of 331.86 with 313 df. The probability of observing this deviance if the model fit is 0.222, providing no evidence of lack-of-fit. 1-pchisq(modela2L$deviance, modela2L$df.residual) # GOF test [1] 0.2219461 There are several reasons why lack-of-fit may be observed in certain cases. We may be missing important covariates; a more comprehensive data set may allow us to examine this possibility. There may be extreme observations that may cause the deviance to be larger than expected; however, our residual plots did not reveal any unusual points. Lastly, there may be a problem with the Poisson model. In particular, the Poisson model has only a single parameter, \\(\\lambda\\), for each combination of the levels of the predictors which must describe both the mean and the variance. This limitation can become manifest when the variance appears to be larger than the corresponding means. In that case, the response is more variable than the Poisson model would imply, and the response is considered to be overdispered. 4.7.11 Least Squares Regression vs. Poisson Regression \\[ \\underline{\\textrm{Response}} \\\\ \\mathbf{OLS:}\\textrm{ Normal} \\\\ \\mathbf{Poisson Regression:}\\textrm{ counts} \\\\ \\textrm{ } \\\\ \\underline{\\textrm{Variance}} \\\\ \\mathbf{OLS:}\\textrm{ Equal for each level of X} \\\\ \\mathbf{Poisson Regression:}\\textrm{ Equal to the mean for each level of X} \\\\ \\textrm{ } \\\\ \\underline{\\textrm{Model Fitting}} \\\\ \\mathbf{OLS:}\\mu=\\beta_0+\\beta_1x \\textrm{ using Least Squares}\\\\ \\mathbf{Poisson Regression:}log(\\lambda)=\\beta_0+\\beta_1x \\textrm{ using Maximum Likelihood}\\\\ \\textrm{ } \\\\ \\underline{\\textrm{EDA}} \\\\ \\mathbf{OLS:}\\textrm{ plot X vs. Y; add line} \\\\ \\mathbf{Poisson Regression:}\\textrm{ find }log(\\bar{y})\\textrm{ for several subgroups; plot vs. X} \\\\ \\textrm{ } \\\\ \\underline{\\textrm{Comparing Models}} \\\\ \\mathbf{OLS:}\\textrm{ extra sum of squares F-tests; AIC/BIC} \\\\ \\mathbf{Poisson Regression:}\\textrm{ Drop in Deviance tests; AIC/BIC} \\\\ \\textrm{ } \\\\ \\underline{\\textrm{Interpreting Coefficients}} \\\\ \\mathbf{OLS:}\\beta_1=\\textrm{ change in }\\mu_y\\textrm{ for unit change in X} \\\\ \\mathbf{Poisson Regression:}e^{\\beta_1}=\\textrm{ percent change in }\\lambda\\textrm{ for unit change in X} \\] 4.7.12 Optional topics to be developed Plots of predicted values by groups Robust Standard Errors 4.8 Case Study: Campus Crime 4.9 Analysis for crime data Students like to feel safe and secure when attending a college or university. In response to legislation, the US Department of Education seeks to provide data and reassurances to students and parents alike. All postsecondary institutions that participate in federal student aid programs are required by the Jeanne Clery Disclosure of Campus Security Policy and Campus Crime Statistics Act and the Higher Education Opportunity Act to collect and report data on crime occurring on campus to the Department of Education. In turn, this data is publicly available on the website of the Office of Postsecondary Education. 4.9.1 Research Question Are there regional differences in violent crime on campus controlling for differences in the type of school? 4.9.2 Data Organization Each row of this data set contains crime information from a post secondary institution, either a college or university. The variables include: type = college (C) or university (U) region = region of the country (C = Central, MW = Midwest, NE = Northeast, SE = Southeast, SW = Southwest, and W = West) nv = the number of violent crimes for that institution for the given year Enrollment = enrollment at the school enroll1000 = enrollment at the school, in thousands (to obviate problems with computing) nvrate = number of violent crimes per 1000 students Enrollment type nv nvrate enroll1000 region 1 5590 U 30 5.36672630 5.590 SE 2 540 C 0 0.00000000 0.540 SE 3 35747 U 23 0.64341064 35.747 W 4 28176 C 1 0.03549120 28.176 W 5 10568 U 1 0.09462528 10.568 SW 6 3127 U 0 0.00000000 3.127 SW 7 20675 U 7 0.33857316 20.675 W 8 12548 C 0 0.00000000 12.548 W 9 30063 U 19 0.63200612 30.063 C 10 4429 C 4 0.90313841 4.429 C 4.9.3 Exploratory Data Analysis Figure 4.5: Histogram of number of violent crimes by institution A graph of the number of violent crimes, Figure 4.5 reveals the pattern often found with distributions of counts of rare events. Many schools reported no violent crimes or very few crimes. A few schools have a large number of crimes making for a distribution that appears to be far from normal. A Poisson regression may be helpful here. Let’s take a look at the covariates of interest for these schools, type of institution and region. In our data, the majority of institutions are universities, 65% of the 81 schools, only 35% are colleges. Interest centers on whether the different regions tend to have different crime rates. Table 4.2 contains the name of each region and the percentages of the number of schools from that region by institution type in the data set. Each column represents the percentage of schools in that region which are colleges or universities. Table 4.2: Proportion of colleges and universities within region in the campus crime data set C MW NE SE SW W C 0.294 0.3 0.381 0.4 0.2 0.5 U 0.706 0.7 0.619 0.6 0.8 0.5 A Poisson regression model is likely to be useful because the responses are counts per thousand students. It is important to note that the counts are not directly comparable because they come from different size schools. We cannot compare the 30 violent crimes from the first school in the data set to no violent crimes for the second school when their enrollments are vastly different; 5,590 for school 1 versus 540 for school 2. We take the differences in enrollments into account by including an offset in our model which we will discuss when we get to modeling. For the remainder of the EDA, we examine the violent crime counts in terms of the rate per 1,000 enrolled, that is the (number of violent crimes)/(number enrolled) *1,000. There is a noticeable outlier for a Southeastern school (5.4 violent crimes per 1000 students), and there is an observed rate of 0 for the Southwestern colleges which can lead to some computational issues. Therefore, we combine the SW and SE to form a single category of the South, and we also remove the extreme observation from the data set. The table and boxplot below display mean violent crime rates that are generally lower rates at the colleges with the exception of the Northeast. In addition, the regional pattern of rates at universities appears to differ from that of the colleges. Table 4.3: The mean and variance of the violent crime rate by region and type of institution region type MeanCount VarCount MeanRate VarRate n C C 1.6000000 3.3000000 0.3979518 0.2780913 5 C U 4.7500000 30.9318182 0.2219441 0.0349266 12 MW C 0.3333333 0.3333333 0.0162633 0.0007935 3 MW U 8.7142857 30.9047619 0.4019003 0.0620748 7 NE C 6.0000000 32.8571429 1.1249885 1.1821000 8 NE U 5.9230769 79.2435897 0.4359273 0.3850333 13 S C 1.1250000 5.8392857 0.1865996 0.1047178 8 S U 8.6250000 68.2500000 0.5713162 0.2778065 16 W C 0.5000000 0.3333333 0.0680164 0.0129074 4 W U 12.5000000 57.0000000 0.4679478 0.0246670 4 Figure 4.6: . 4.9.4 Accounting for Enrollment Although working with the observed rates is useful during the exploratory data analysis, it is not the rates that go into the model; the counts are the responses when modeling. So we must take into account the enrollment in another way. Our approach is to include a term on the right side of the model called an offset which equals the log of the enrollment, in thousands. If we were using a Poisson regression to model the average count per 1,000, \\(\\lambda\\), for schools which all had the same enrollment, it may look like: \\[ log(\\lambda) = \\beta_0 + \\beta_1(\\textrm{SchoolType}) \\] This model doesn’t account for the enrollment, so we’ll add the offset to the right side. \\[ log(\\lambda) = \\beta_0 + \\beta_1(\\textrm{SchoolType})+ log(\\textrm{enroll1000}) \\] The offset has the unusual feature that the coefficient is fixed at 1.0. No estimated coefficient for enroll1000 will appear on the output. There is an intuitive heuristic for the form of the offset. If we think of \\(\\lambda\\) as the rate per 1,000 and then subtract log(enroll1000) from each side, the count associated with a particular enrollment will be adjusted to be comparable to all of the other counts in the data set. \\[\\begin{eqnarray} \\nonumber log(\\lambda)-log(\\textrm{enroll1000}) = \\beta_0 + \\beta_1(\\textrm{type})\\\\ \\nonumber log(\\frac{\\lambda}{\\textrm{enroll1000}} )= \\beta_0 + \\beta_1(\\textrm{type}) \\end{eqnarray}\\] While this heuristic is helpful, it is important to note that it is not \\(\\lambda / \\textrm{enroll1000}\\) that we are modeling. 4.10 Checking Assumptions 4.10.1 Is the Mean equal to the Variance? The mean of a Poisson random variable is also its variance. Consequently we should find that the mean count is approximately equal to its variance for combinations of our covariates. In Table 4.3, we see that the variances are greatly higher than the mean counts in almost every group. Thus, we have reason to question this assumption and will have to return to this issue after some initial modeling. The fact that the variance of the rate of violent crimes per 1000 students tends to be on the same scale as the mean tells us that adjusting for enrollment may provide some help, although that may not completely solve our issues with excessive variance. The remaining assumption is the independence of the observations. In this case, it is not possible to assess independence without knowing how the schools were selected. 4.11 Modeling We are interested primarily in differences in violent crime between institutional types controlling for difference in regions, so we fit a model with both region and type including log(enroll1000) as an offset. glm(formula = nv ~ type + region, family = poisson, data = c.data, offset = log(enroll1000)) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.54780 0.17114 -9.044 &lt; 2e-16 *** typeU 0.27956 0.13314 2.100 0.0358 * regionMW 0.09912 0.17752 0.558 0.5766 regionNE 0.77813 0.15307 5.084 3.70e-07 *** regionS 0.58238 0.14896 3.910 9.24e-05 *** regionW 0.26275 0.18753 1.401 0.1612 --- (Dispersion parameter for poisson family taken to be 1) Null deviance: 392.76 on 79 degrees of freedom Residual deviance: 348.68 on 74 degrees of freedom AIC: 573.32 The Region coefficients each compare the mean rate for that region to the Central region. According to the p-values, the Northeast and the South differ significantly from the Central region. The estimated coefficient of 0.77813 translates to the violent crime rate per 1,000 in the Northeast being nearly 2.2 times that of the Central region controlling for the type of school. A confidence interval for this factor can be constructed by first calculating a CI for the coefficient (0.77813 \\(\\pm\\) 1.96*0.15314) and then exponentiating. A 95% CI ranges from 1.61 to 2.94. Comparisons to regions other than the Central region can be accomplished by changing the reference region. If many comparisons are made, it would be best to use a multiple comparisons method such as Bonferroni’s, where the p-value is divided by the number of comparisons to be made, or Tukey’s Honestly Significant Differences described elsewhere. Multiple Comparisons of Means: Tukey Contrasts Linear Hypotheses: Estimate Std. Error z value Pr(&gt;|z|) MW - C == 0 0.09912 0.17752 0.558 0.9804 NE - C == 0 0.77813 0.15307 5.084 &lt;0.001 *** S - C == 0 0.58238 0.14896 3.910 &lt;0.001 *** W - C == 0 0.26275 0.18753 1.401 0.6209 NE - MW == 0 0.67901 0.15545 4.368 &lt;0.001 *** S - MW == 0 0.48327 0.15143 3.191 0.0120 * W - MW == 0 0.16364 0.18942 0.864 0.9079 S - NE == 0 -0.19574 0.12182 -1.607 0.4863 W - NE == 0 -0.51537 0.16587 -3.107 0.0157 * W - S == 0 -0.31963 0.16296 -1.961 0.2795 --- (Adjusted p values reported -- single-step method) With Tukey’s Honestly Significant Differences, we find that the Northeast differs significantly from the Central, Midwest, and Western regions while the South differs significantly from the Central and the Midwest controlling for the type of institution. An indicator of the type of institution is also included in the model. The University indicator is significant and after exponentiating the coefficient can be interpreted as an approximately (\\(e^{0.27956}\\)) 32% increase in violent crime rate over Colleges after controlling for region. These results certainly suggests significant differences in regions and type of institution. However, the EDA findings suggest the effect of the type of institution may vary depending upon the region, so we consider a model with an interaction between region and type. glm(formula = nv ~ type + region + region:type, family = poisson, data = c.data, offset = log(enroll1000)) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.4741 0.3536 -4.169 3.05e-05 *** typeU 0.1959 0.3775 0.519 0.60377 regionMW -1.9765 1.0607 -1.863 0.06239 . regionNE 1.5529 0.3819 4.066 4.77e-05 *** regionS -0.1562 0.4859 -0.322 0.74779 regionW -1.8337 0.7906 -2.319 0.02037 * typeU:regionMW 2.1965 1.0765 2.040 0.04132 * typeU:regionNE -1.0698 0.4200 -2.547 0.01086 * typeU:regionS 0.8121 0.5108 1.590 0.11185 typeU:regionW 2.4106 0.8140 2.962 0.00306 ** --- (Dispersion parameter for poisson family taken to be 1) Null deviance: 392.76 on 79 degrees of freedom Residual deviance: 276.70 on 70 degrees of freedom AIC: 509.33 These results provide convincing evidence of an interaction between the effect of region and the type of institution. A drop-in-deviance test like the one we carried out in the previous section confirms the significance of the contribution of the interaction to this model. We have statistically significant evidence (\\(\\chi^2=71.98, p&lt;.001\\)) that the difference between colleges and universities in violent crime rate differs by region. For example, our model estimates that violent crime rates are 13.6 (\\(e^{.1959+2.4106}\\)) times higher in universities in the West compared to colleges, while in the Northest we estimate that violent crime rates are 2.4 (\\(1/e^{.1959-1.0698}\\)) times higher in colleges. Analysis of Deviance Table Model 1: nv ~ type + region Model 2: nv ~ type + region + region:type Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) 1 74 348.68 2 70 276.70 4 71.981 8.664e-15 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The residual deviance (276.70 with 70 df) suggests that the interaction model does not fit well. One possibility is that there are other important covariates that could be used to describe the differences in the violent crime rates. Without additional covariates to consider, we look for extreme observations, but we have already eliminated the most extreme of the observations. In the absence of other covariates or extreme observations, we consider overdispersion as a possible explanation of the significant lack-of-fit. 4.12 Overdispersion 4.12.1 Dispersion parameter adjustment Overdispersion suggests that there is more variation in the response than the model implies. Under a Poisson model, we would expect the means and variances of the response to be about the same in various groups. Without adjusting for overdispersion, we use incorrect, artifically small standard errors and/or end up with overly complex models. We can take overdispersion into account in several different ways. The simplest is to use an estimated dispersion factor to inflate standard errors. Another way is to use a negative-binomial regression model. We begin with using the estimate of the dispersion parameter. We can estimate a dispersion parameter, \\(\\phi\\), by dividing the model deviance by its corresponding degrees of freedom; i.e., \\(\\hat\\phi=\\frac{\\sum(\\textrm{Pearson residuals})^2}{n-p}\\) where \\(p\\) is the number of model parameters. It follows from what we know about the \\(\\chi^2\\) distribution that if there is no overdispersion, this estimate should be close to one. It will be larger than one in the presence of overdispersion. We inflate the standard errors by multiplying the variance by \\(\\phi\\) so that the standard errors are larger than the likelihood approach would imply; i.e., \\(SE_Q(\\hat\\beta)=\\sqrt{\\hat\\phi}*SE(\\hat\\beta)\\). If we choose to use a dispersion parameter with our model, we refer to the approach as quasilikelihood. The following output illustrates a quasipoisson approach to our most recent model: # Assessing the goodness of fit of the interaction model gof &lt;- 1-pchisq(modeli$deviance, modeli$df.residual) gof [1] 0 glm(formula = nv ~ type + region + region:type, family = quasipoisson, data = c.data, offset = log(enroll1000)) Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -1.4741 0.7455 -1.977 0.0520 . typeU 0.1959 0.7961 0.246 0.8063 regionMW -1.9765 2.2366 -0.884 0.3799 regionNE 1.5529 0.8053 1.928 0.0579 . regionS -0.1562 1.0246 -0.152 0.8792 regionW -1.8337 1.6671 -1.100 0.2751 typeU:regionMW 2.1965 2.2701 0.968 0.3366 typeU:regionNE -1.0698 0.8856 -1.208 0.2311 typeU:regionS 0.8121 1.0771 0.754 0.4534 typeU:regionW 2.4106 1.7164 1.404 0.1646 --- (Dispersion parameter for quasipoisson family taken to be 4.446556) Null deviance: 392.76 on 79 degrees of freedom Residual deviance: 276.70 on 70 degrees of freedom AIC: NA In the absence of overdispersion, we expect the dispersion parameter estimate to be 1.0. The estimated dispersion parameter here is much larger than 1.0 (4.447) indicating overdispersion (extra variance) that should be accounted for. The larger estimated standard errors in the quasipoisson model reflect the adjustment. For example, the standard error for the West region term from a likelihood based approach is 0.7906, whereas the quasilikelihood standard error is \\(\\sqrt{4.47}*0.7906\\) or 1.6671. This term is no longer significant under the quasipoisson model. In fact, after adjusting for overdispersion (extra variation), none of the model coefficients in the quasipoisson model are significant at the .05 level! This is because standard errors were all increased by a factor of 2.1 (\\(\\sqrt{\\hat\\phi}=\\sqrt{4.447}=2.1\\)). Note that tests for individual parameters are now based on the t-distribution rather than a standard normal distribution, with test statistic \\(t=\\frac{\\hat\\beta}{SE_Q(\\hat\\beta)}\\) following an (approximate) t-distribution with \\(n-p\\) degrees of freedom if the null hypothesis is true. Drop-in-deviance tests can be similarly adjusted for overdispersion in the quasipoisson model. In this case, you can divide the test statistic by the estimated dispersion parameter and compare the result to an F-distribution with the difference in the model degrees of freedom for the numerator and the degrees of freedom for the larger model in the denominator. That is, \\(F=\\frac{\\textrm{drop in deviance}}{\\hat\\phi}\\) follows an (approximate) F-distribution when the null hypothesis is true. The output below then tests for an interaction between region and type of institution after adjusting for overdispersion (extra variance): phi &lt;- sum(resid(modeli, type=&#39;pearson&#39;)^2) / modeli$df.residual drop.in.dev &lt;- modeltr$deviance - modeli$deviance diff.in.df &lt;- modeltr$df.residual - modeli$df.residual Fstat &lt;- drop.in.dev / summary(modeliq)$dispersion Fstat [1] 16.18795 1-pf(Fstat, diff.in.df, modeli$df.residual) [1] 1.975114e-09 Here, even after adjusting for overdispersion, we still have statistically significant evidence (\\(F=16.19, p&lt;.001\\)) that the difference between colleges and universities in violent crime rate differs by region. Summary: Accounting for Overdispersion Use the dispersion parameter \\(\\hat\\phi=\\frac{\\sum(\\textrm{Pearson residuals})^2}{n-p}\\) to inflate standard errors of model coefficients Wald test statistics: multiply the standard errors by \\(\\sqrt{\\hat{\\phi}}\\) so that \\(SE_Q(\\hat\\beta)=\\sqrt{\\hat\\phi}*SE(\\hat\\beta)\\) and conduct tests using the t-distribution CIs which use the adjusted standard errors and are thereby wider \\(\\hat\\beta \\pm t_{n-p}*SE_Q(\\hat\\beta)\\) Drop-in-deviance test statistic comparing Model 1 (larger model with \\(p\\) parameters) to Model 2 (smaller model with \\(q&lt;p\\) parameters) = \\[\\begin{equation} F=\\frac{1}{\\hat\\phi} \\frac{D_2 - D_1}{p-q} \\tag{4.8} \\end{equation}\\] where \\(D_1\\) and \\(D_2\\) are the residual deviances for models 1 and 2 respectively and \\(p-q\\) is the difference in the number of parameters for the two models. Note that both \\(D_2-D_1\\) and \\(p-q\\) are positive. This test statistic is compared to an F-distribution with \\(p-q\\) and \\((n-p)\\) degrees of freedom. Note that it was important to obtain the correct model prior to considering adjustment for overdispersion. 4.12.2 Negative binomial modeling Another approach to dealing with overdispersion is to model the response using a negative binomial instead of Poisson. An advantage of this approach is that it introduces another parameter in addition to \\(\\lambda\\) which gives the model a little more flexibility and, as opposed to the quasipoisson model, the negative binomial model assumes an explicit likelihood model. You may recall that negative binomial random variables take on nonnegative integer values which is consistent with modeling counts. This model posits selecting a \\(\\lambda\\) for each institution and then generating a count using a Poisson random variable with the selected \\(\\lambda\\). With this approach, the counts will be more dispersed than would be expected for observations based on a single Poisson variable with rate \\(\\lambda\\). Mathematically, you can think of the negative binomial model as a Poisson model where \\(\\lambda\\) is also random, following a gamma distribution. Specifically, if \\(Y|\\lambda \\textrm{~ Poisson}(\\lambda)\\) and \\(\\lambda \\textrm{~ gamma}(\\theta,\\frac{1-p}{p})\\), then \\(Y \\textrm{~ NegBinom}(\\theta,p)\\) where \\(E(Y)=\\frac{p\\theta}{1-p}=\\mu\\) and \\(Var(Y)=\\frac{p\\theta}{(1-p)^2}=\\mu+\\frac{\\mu^2}{\\theta}\\). The overdispersion in this case is given by \\(\\frac{\\mu^2}{\\theta}\\), which approaches 0 as \\(\\theta\\) increases (so smaller values of \\(\\theta\\) indicate greater overdispersion). Here is what happens if we apply the negative binomial model to the interaction model, which we’ve already established suffers from overdispersion issues under regular Poisson regression: glm.nb(formula = nv ~ type + region + region:type, data = c.data2, weights = offset(log(enroll1000)), init.theta = 1.312886384, link = log) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.4904 0.4281 1.145 0.25201 typeU 1.2174 0.4608 2.642 0.00824 ** regionMW -1.0953 0.8075 -1.356 0.17500 regionNE 1.3966 0.5053 2.764 0.00571 ** regionS 0.1461 0.5559 0.263 0.79275 regionW -1.1858 0.6870 -1.726 0.08435 . typeU:regionMW 1.6342 0.8498 1.923 0.05447 . typeU:regionNE -1.1259 0.5601 -2.010 0.04441 * typeU:regionS 0.4513 0.5995 0.753 0.45164 typeU:regionW 2.0387 0.7527 2.709 0.00676 ** --- (Dispersion parameter for Negative Binomial(1.3129) family taken to be 1) Null deviance: 282.43 on 78 degrees of freedom Residual deviance: 199.57 on 69 degrees of freedom These results differ from the quasipoisson model. Several effects are now statistically significant at the .05 level: the effect of type of institution for the Central region (\\(Z=2.64, p=.008\\)), the difference between Northeast and Central regions for colleges (\\(Z=2.76, p=.006\\)), the difference between Northeast and Central regions in type effect (\\(Z=-2.01, p=.044\\)), and the difference between West and Central regions in type effect (\\(Z=2.71, p=.007\\)). Compared to the quasipoisson model, negative binomial coefficient estimates are generally in the same direction and similar in size, but negative binomial standard errors are smaller. In summary, we explored the possibility of regional differences in the violent crime rate controlling for the type of institution (or vice versa). Our initial efforts seemed to suggest that there are indeed regional differences and the pattern of those differences depend upon the type of institution. However, this model exhibited significant lack-of-fit which remained after the removal of an extreme observation. In the absence of additional covariates, we accounted for the lack-of-fit by using a quasilikelihood approach and a negative binomial regression, which provided slightly different conclusions. We may want to look for additional covariates and or more data. 4.13 Case Study: Weekend drinking {cs:drinking} Sometimes when analyzing Poisson data, you may see many more zeros in your data set than you would expect for a Poisson random variable. For example, an informal survey of students in an introductory statistics course included the question, “How many alcoholic drinks did you consume last weekend?” This survey was conducted on a dry campus where no alcohol is officially allowed, even among students of drinking age, so we expect that some portion of the respondents never drink. The non-drinkers would thus always report zero drinks. However, there will also be students who are drinkers reporting zero drinks because they just did not happen to drink during the past weekend. Our zeros, then, are a mixture of responses from non-drinkers and drinkers who abstained. Ideally, we’d like to sort out the non-drinkers and drinkers when performing our analysis. 4.13.1 Research Question The purpose of this survey is to explore factors related to drinking behavior on a dry campus. What proportion of students on this dry campus never drink? What factors, such as off campus living and sex, are related to whether students drink? Among those who do drink, to what extent is moving off campus associated with the number of drinks in a weekend? It is commonly assumed that males’ alcohol consumption is greater than females’; is this true on this campus? Answering these questions would be a simple matter if we knew who was and was not a drinker in our sample. Unfortunately, the non-drinkers did not identify themselves as such, so we will need to use the data available with a model that allows us to estimate the proportion of drinkers and non-drinkers. 4.13.2 Data Organization Each line of this file contains data provided by a student in an introductory statistics course. In this analysis, the response of interest is the respondent’s report of the number of alcoholic drinks they consumed the previous weekend, whether the student lives off.campus, and sex. We will also consider whether a student is likely a firstYear student based on the dorm he or she lives in. Here is a sample of observations from this dataset: head(zip.data[2:4]) drinks sex off.campus 1 0 f 0 2 5 f 0 3 10 m 0 4 0 f 0 5 0 m 0 6 3 f 0 4.13.3 Exploratory Data Analysis As always we take stock of the amount of data; here there are 77 observations. Large sample sizes are preferred for the type of model we consider, and n=77 is on the small side. We proceed with that in mind. A premise of this analysis is that we believe that those responding zero drinks are coming from a mixture of non-drinkers and drinkers who abstained the weekend of the survey. Non-drinkers: respondents who never drink and would always reply with zero Drinkers: obviously this includes those responding with one or more drinks, but it also includes people who are drinkers but did not happen to imbibe the past weekend. These people reply zero but are not considered non-drinkers. Figure 4.7: Observed (a) versus Modeled (b) Number of Drinks Beginning the EDA with the response, number of drinks, we find that over 46% of the students reported no drinks during the past weekend. Figure 4.7a portrays the observed number of drinks reported by the students. The mean number of drinks reported the past weekend is 2.013. Our sample consists of 74% females and 26% males, only 9% of whom live off campus. Because our response is a count it is natural to consider a Poisson regression model. You may recall that a Poisson distribution has only one parameter, \\(\\lambda\\), for its mean and variance. Here we will include an additional parameter, \\(\\alpha\\). We define \\(\\alpha\\) to be the true proportion of non-drinkers in the population. The next step in the EDA is especially helpful if you suspect your data contains excess zeros. Figure 4.7b is what we might expect to see under a Poisson model. Bars represent the probabilities for a Poisson distribution (using the Poisson probability formula) with \\(\\lambda\\) equal to the mean observed number of drinks, 2.013 drinks per weekend. Comparing this Poisson distribution to what we observed (Figure 4.7a), it is clear that many more zeros have been reported by the students than you would expect to see if the survey observations were coming from a Poisson distribution. This doesn’t surprise us because we had expected a subset of the survey respondents to be non-drinkers; i.e., they would not be included in this Poisson process. This circumstance actually arises in many Poisson regression settings. We can model both \\(\\lambda\\), the mean number of drinks among those who drink, and \\(\\alpha\\), the proportion of non-drinkers (“true zeros”), as well as determine the effects of covariates like sex and off-campus residence on both parameters. This type of model is referred to as a zero-inflated Poisson model or ZIP model. 4.13.4 Modeling We first fit a simple Poisson model with the covariates off.campus and sex. glm(formula = drinks ~ off.campus + sex, family = poisson, data = zip.data) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.1293 0.1241 1.041 0.298 off.campus 0.8976 0.2008 4.470 7.83e-06 *** sexm 1.1154 0.1611 6.925 4.36e-12 *** --- (Dispersion parameter for poisson family taken to be 1) Null deviance: 294.54 on 76 degrees of freedom Residual deviance: 230.54 on 74 degrees of freedom AIC: 357.09 # Exponentiated coefficients exp(coef(pois.m1)) ## (Intercept) off.campus sexm ## 1.137996 2.453819 3.050707 # Goodness-of-fit test gof.ts = pois.m1$deviance gof.pvalue = 1 - pchisq(gof.ts, pois.m1$df.residual) gof.pvalue ## [1] 0 Both covariates are statistically significant, but a goodness-of-fit test reveals that there remains significant lack-of-fit (residual deviance: 230.54 with only 74 df; p&lt;.001 based on \\(\\chi^2\\) test with 74 df). In the absence of important missing covariates or extreme observations, this lack-of-fit may be explained by the presence of a group of non-drinkers. A zero-inflated Poisson regression model to take non-drinkers into account consists of two parts: One part models the association among drinkers of the number of drinks with predictors sex and off campus residence. The other part uses the predictors for sex and off campus residence to obtain an estimate of the proportion of non-drinkers based on the reported zeros. The form for each part of the model follows. The first part looks like an ordinary Poisson regression model: \\[ log(\\lambda)=\\beta_0+\\beta_1(\\textrm{off.campus})+ \\beta_2(\\textrm{sex}) \\] where \\(\\lambda\\) is the mean number of drinks in a weekend among those who drink. The second part has the form \\[ logit(\\alpha)=\\beta_0+\\beta_1(\\textrm{firstYear})) \\] where \\(\\alpha\\) is the probability of being in the non-drinkers group and \\(logit(\\alpha) = log( \\alpha/(1-\\alpha))\\). We’ll provide more detail on the logit in the Logistic Regression chapter. There are many ways in which to structure this model; here we use different predictors in the two pieces, athough it would have been perfectly fine to use the same predictors for both pieces, or even no predictors for one of the pieces. 4.13.5 Fitting a ZIP Model How is it possible to fit such a model? We cannot observe whether a respondent is a drinker or not (which probably would’ve been good to ask). The ZIP model is a special case of a more general type of statistical model referred to as a latent variable model. More specifically, it is a type of a mixture model where observations for one or more groups occur together and the group membership is unknown. Zero-inflated models are a particularly common example of a mixture model, but the response does not need to follow a Poisson distribution. Likelihood methods are at the core of this methodology, but fitting is an iterative process where it is necessary to start out with some guesses (or starting values). In general, it is important to know that models like ZIP exist, although we’ll only explore interpretations and fitting options for a single case study here. Here is the general idea of how ZIP models are fit. Imagine that the graph of the Poisson distribution Figure 4.7b is removed from the observed data distribution in Figure 4.7a. Some zero responses will remain. These would correspond to non-drinkers, and the proportion of all observations these zeros constitute might make a reasonable estimate for \\(\\alpha\\), the proportion of non-drinkers. The likelihood is used and some iterating in the fitting process is involved because the Poisson distribution in (Figure 4.7b is based on the mean of the observed data, which means it is the average among all students, not only among drinkers. Furthermore, the likelihood incorporates the predictors, sex and off.campus. So there is a little more to it than computing the proportion of zeros, but this heuristic should provide you a general idea of how these kinds of models are fit. Fortunately, various software packages will fit these kinds of models. For example, the software package R along with the library(pscl) will fit a ZIP model for us using the function zeroinfl. zeroinfl(formula = drinks ~ off.campus + sex | firstYear, data = zip.data) Count model coefficients (poisson with log link): Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.7543 0.1440 5.238 1.62e-07 *** off.campus 0.4159 0.2059 2.021 0.0433 * sexm 1.0209 0.1752 5.827 5.63e-09 *** Zero-inflation model coefficients (binomial with logit link): Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -0.6036 0.3114 -1.938 0.0526 . firstYearTRUE 1.1364 0.6095 1.864 0.0623 . --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 exp(coef(zip.m2)) # exponentiated coefficients ## count_(Intercept) count_off.campus count_sexm ## 2.1260689 1.5157979 2.7756924 ## zero_(Intercept) zero_firstYearTRUE ## 0.5468312 3.1154949 Our model uses firstYear to distinguish drinkers and non-drinkers (“Zero-inflation model coefficients”) and off.campus and sex to help explain the differences in the number of drinks among drinkers (“Count model coefficients”). Again, we could have used the same covariates for the two pieces of a ZIP model, but neither off.campus or sex proved to be a useful predictor of drinkers vs. non-drinkers. We’ll first consider the “Count model coefficients,” which provide information on how the sex and off-campus status of a student who is a drinker are related to the number of drinks reported by that student over a weekend. As we have done with previous Poisson regression models, we exponentiate each coefficient for ease of interpretation. Thus, for those who drink, the average number of drinks for males is \\(e^{1.0209}\\) or 2.76 times the number for females (Z = 5.827, p &lt; 0.001) given that you are comparing people who live in comparable settings, i.e., either both on or both off campus. Among drinkers, the mean number of drinks for those living off campus is \\(e^{0.4159}=1.52\\) times that of those living on campus for those of the same sex (Z = 2.021, p = 0.0433). The “Zero-inflation model coefficients” refer to separating drinkers from non-drinkers. An important consideration is separating drinkers from non-drinkers may be whether this is their first year, where firstYear is a 0/1 indicator variable. We have \\[ log(\\alpha/(1-\\alpha)) =-0.6036+1.1364(\\textrm{firstYear}) \\] However we are interested in \\(\\alpha\\), the proportion of non-drinkers. Exponentiating the coefficient for the first year term for this model yields 3.12. Here it is interpreted as the odds that a first year is a non-drinker is 3.12 times the odds that an upper class student is a non-drinker. Furthermore, with a little algebra (solving the equation with \\(log(\\alpha/(1-\\alpha)\\)) for \\(\\alpha\\)), we have \\[ \\hat{\\alpha} = \\frac{exp^ {-0.6036+1.1364(\\textrm{firstYear})}} {1+e^{ -0.6036+1.1364(\\textrm{firstYear}) } }. \\] The estimated chance that a first year student is a non-drinker is \\[ \\frac{e^{0.533}}{1+e^{0.533}} = 0.630 \\] or 63.0%, while for non-first years, the estimated probability of being a non-drinker in 0.354. If you have seen logistic regression, you’ll recognize that this transformation is what is used to estimate a probability. More on this in the logistic regression chapter. 4.13.6 Comparing the ordinary Poisson regression model to the ZIP model Moving from ordinary Poisson to zero-inflated Poisson has helped us address additional research questions: What proportion of students are non-drinkers and what factors are associated with whether or not a student is a non-drinker? While a ZIP model seems more faithful to the nature and structure of this data, can we quantitatively show that a zero-inflated Poisson is better than an ordinary Poisson model? We cannot use the drop-in-deviance test we discussed earlier because these two models are not nested within one another. Vuong (1989) devised a test to make this comparison for the special case of comparing a zero-inflated model and ordinary regression model. Vuong Non-Nested Hypothesis Test-Statistic: (test-statistic is asymptotically distributed N(0,1) under the null that the models are indistinguishible) ------------------------------------------------------------- Vuong z-statistic H_A p-value Raw -2.688691 model2 &gt; model1 0.0035866 AIC-corrected -2.534095 model2 &gt; model1 0.0056369 BIC-corrected -2.352922 model2 &gt; model1 0.0093133 Here, we have structured the Vuong Test to compare Model 1: Ordinary Poisson Model to Model 2: Zero-inflation Model. If the two models do not differ, the test statistic for Vuong would be asymptotically standard Normal and the p-value would be relatively large. Here the first line of the output table indicates that the zero-inflation model is better (\\(Z=-2.69,p=.0036\\)). Note that the test depends upon sufficiently large n for the Normal approximation, so since our sample size (n=77) is somewhat small, we need to interpret this result with caution. More research is underway to address statistical issues related to these comparisons. 4.13.7 Residual Plot Fitted values (\\(\\hat{y}\\)) and residuals (\\(y-\\hat{y}\\)) can be computed for zero-inflation models and plotted. Figure 4.8 reveals that one observation appears to be extreme (Y=22 drinks during the past weekend). Is this a legitimate observation or was there a transcribing error? Without the original respondents we cannot settle this question. It might be worthwhile to get a sense of how influential this extreme observation is when fitting the model. Removing Y=22 and refitting the model can provide some idea about this. Figure 4.8: Residuals by fitted counts model. 4.13.8 Caveats and Extensions Given that you have progressed this far in your statistical education, the weekend drinking survey question should raise some red flags. What time period constitutes the “weekend”? Will some students be thinking of only Saturday night while others include Friday night or possibly Sunday evening? What constitutes a drink—a bottle of beer? How many drinks will a respondent report for a bottle of wine? Precise definitions would vastly improve the quality of this data. There is also an issue related to confidentiality. If the data is collected in class, will the teacher be able to identify the respondent? Will respondents worry that a particular response will affect their grade in the class or lead to repercussions on a dry campus? In addition to these concerns, there are a number of other caveats that should be noted here. Following the concern of whether this data represents a random sample of any population (it doesn’t), we also must be concerned with the size of this data set. ZIP models are not appropriate for small samples and this data set is not impressively large. At times, a mixture of zeros occurs naturally. It may not come about because of neglecting to ask a critical question on a survey, but the information about the subpopulation may simply not be ascertainable. For example, visitors from a state park were asked as they departed how many fish they caught, but those who report 0 could be either non-fishers or fishers who had bad luck. These kinds of circumstances occur often enough that ZIP models are becoming increasingly common. Actually there are many fewer ordinary Poisson regression applications in contrast to ZIPs and other Poisson modeling approaches such as hurdle models and quasi-Poisson applications. So it is worth taking a look at these variations of Poisson regression models. Not enough detail is presented here to deal with certain nuances of zero-inflated models, but we want you to know about models of this type. ZIP models demonstrate that modeling can be flexible and creative which we hope you see as a theme throughout this book. 4.14 References 4.15 Exercises 4.15.1 Conceptual Exercises What are features of inferential OLS models that make them less suitable for count data? Models of the form \\(Y_i=\\beta_0+\\beta_1X_i, \\epsilon \\sim iidN(0,\\sigma)\\) are fit using the method of least squares [note that least squares estimates for linear regression are also MLEs]. What method is used to fit Poisson regression models? What should be done before adjusting for overdispersion? Why is quasilikelihood used and how do the results compare for corresponding models using likelihoods? Why is the log of means, log(\\(\\bar{Y}\\)), not \\(\\bar{Y}\\), plotted against X when assessing the assumptions for Poisson regression? How can this assumption of mean=variance be checked for Poisson regression? What if there are not many repeated observations at each level of X? Is it possible that a predictor is significant for a model fit using a likelihood, but not for a model for the same data fit using a quasilikelihood? Explain. Complete (a)-(d) in the context of the study for Exercises 8-10. Define the response. What are the possible values for the response? What does \\(\\lambda\\) represent? Would a zero-inflated model be considered here? If so, what would be a “true zero?” Fish (Poisson) A state wildlife biologist collected data from 250 park visitors as they left at the end of their stay. Each were asked to report the number of fish they caught during their one week stay. On average visitors caught 21.5 fish per week. Methadone Program Recidivism. Program facilitators keep track of the number of times their program’s patients relapse within five years of initial treatment. Data on 100 patients yielded a mean number of 2.8 relapses per patient within the five years of initial treatment. Clutch size. Thirty nests were located and the number of eggs in each nest are counted at the start of a season. Later in the season following a particularly destructive storm, the mean clutch size of the 30 nests was only 1.7 eggs per nest. Credit card use A survey of 1,000 consumers asked respondents how many credit cards they use. Interest centers on the relationship between credit card use and income, in $10,000. The estimated coefficient for income is 2.1. Identify the predictor and interpret the estimated coefficient for the predictor in this context. Describe how the assumption of linearity can be assessed in this example. Suggest a way in which to assess the equal mean and variance assumption. Dating Online Researchers are interested in the number of dates respondents arranged online and whether the rates differ by age group. Questions which elicit responses similar to this can be found in the Pew survey concerning dating online and relationships (Pew 2013). Each survey respondent was asked how many dates they have arranged on line in the past 3 months as well as the typical amount of time, \\(t\\), in hours, they spend on line weekly. Some rows of data appear below. Age Time online Number of dates arranged online 19 35 3 29 20 5 38 15 0 55 10 0 Identify the response, predictor and offset in this context. Write out a model for this data. As part of your model description, define the parameter, \\(\\lambda\\). Consider a Zero-inflated Poisson model for this data. Describe what the `true zeros’ would be in this setting. Poisson Approximation: Rare Events For rare diseases, the probability of a case occurring, \\(p\\), in a very large population, \\(n\\), is small. With a small \\(p\\) and large \\(n\\), the random variable \\(Y\\)= the number of cases out of \\(n\\) people can be approximated using a Poisson random variable with \\(\\lambda = np\\). If the count of those with the disease is observed in several different populations independently of one another, the \\(Y_i\\) represents the number of cases in the \\(i^{th}\\) population and can be approximated using a Poisson random variable with \\(\\lambda_i=n_ip_i\\) where \\(p_i\\) is the probability of a case for the \\(i^{th}\\) population. Poisson regression can take into account the differences in the population sizes, \\(n_i\\), using as an offset which is the log(\\(n_i\\)). The coefficient of the offset is set at one, it is not estimated like the other coefficients.Thus the model statement has the form: \\[ log(\\lambda_i) = \\beta_0+\\beta_1x_i + log(n_i) \\] where \\(Y_i \\sim\\) Poisson(\\(n_i\\lambda_i\\)). Note that \\(\\lambda_i\\) depends on \\(x_i\\) which may differ for the different populations. Skin Cancer: an example. Data from Scotto, Kopf, and Urbach (1974) reported the number of cases of non melanoma skin cancer for women by age group in two metropolitan areas (Minneapolis-St Paul and Dallas-Ft Worth); the year is unknown. They wondered if cancer rates by age group differ by city? The columns contain: number of cases of skin cancer, population size of the age group per city, age group (1=15-24, 2=25-34, 3=35-44, 4=45-54, 5=55-64, 6=65-74, 7=75-84, 8=85+), and metropolitan area (1=Minneapolis-St Paul, 2=Dallas-Ft Worth). Number of Cases Population Age Group City 1 172675 15-24 1 16 123065 25-34 1 226 29007 75-84 2 65 7538 85+ 2 Identify the following quantities which appear in the description of the Poisson approximation for rare events: A case, The population size, n, The random variable \\(Y=\\)the number of events out of \\(n\\) trials, The offset The corresponding Poisson random variables, and The predictors. 4.15.2 Guided Exercise Elephant Mating How does age affect male elephant mating patterns? An article by Poole (1989) investigated whether mating success in male elephants increases with age and whether there is a peak age for mating success. To address this question, the research team followed 41 elephants for one year and recorded both their ages and their number of matings. The data is found in elephant.csv, and relevant R code can be found under elephantMating.R. The variables are: MATINGS: the number of matings in a given year AGE: the age of the elephant in years. Create a histogram of MATINGS. Is there preliminary evidence that number of matings could be modeled as a Poisson response? Explain. Plot MATINGS by AGE. Add a least squares line. Is there evidence that modeling matings using a linear regression with age might not be appropriate? Explain. For each age, calculate the mean number of matings. Take the log of each mean and plot it by AGE. What assumption can be assessed with this plot? Is there evidence of a quadratic trend on this plot? Fit a Poisson regression model with a linear term for AGE. Interpret the coefficient for AGE. Exponentiate and interpret the result. Construct a 95% confidence interval for the slope and interpret in context (you may want to exponentiate endpoints). Are the number of matings significantly related to age? Test with a Wald test and a drop in deviance test. Add a quadratic term in AGE to determine whether there is a maximum age for the number of matings for elephants. Is a quadratic model preferred to a linear model? To investigate this question, use a Wald test and a drop in deviance test. What can we say about the goodness of fit of the model with age as the sole predictor? Compare the residual deviance for the linear model to a \\(\\chi^2\\) distribution with the residual model degrees of freedom. Fit the linear model using a quasilikelihood. (Why?) How do the estimated coefficients change? How do the standard errors change? What is the estimated dispersion parameter? An estimated dispersion parameter greater than one suggests overdispersion. When adjusting for overdispersion, are you more of less likely to obtain a significant result when testing coefficients? Why? Source: [@Ramsey2002]. Sensitivity training Microaggressions have been defined as “brief, everyday exchanges that send denigrating messages to certain individuals because of their group membership.” [@Sue2010]. Microaggressions are different from overt, deliberate acts of bigotry, such as the use of racist epithets, because the people perpetrating microaggressions often intend no offense and are unaware they are causing harm. A (hypothetical) study was conducted to determine whether sensitivity training would reduce the number of microaggressions. Students from the dominant culture were randomly selected from a student population in a small liberal arts college. Participants were randomly assigned to one, two, three or four days of sensitivity training. A control group received no training. Following the completion of the training, subjects agreed to be observed while taking part in several task-oriented activities and discussions over the course of one or more hours. The number of hours under surveillance varied due to the availability of the participant. The data is recorded in sensitivity.csv. id = Subject identifier daysTraining = Days of sensitivity training microaggressions = Number of observed microaggressions hrsSurv = Number of hours under observation since the completion of the sensitivity training How many students took part in the study? How many students had none, one, two, three or four days of training? What is the distribution of the number of hours of surveillance? Plot the distribution of the number of microaggressions. Does the distribution suggest that a Poisson regression would be useful for modeling? Why is this plot not expected to display a single Poisson distribution? Does it appear that average number of microaggressions differs for different levels of sensitivity training? Why is it misleading to simply compare the average number of microaggressions by hours of training? Does the mean number of microaggressions per hour differ for different levels of training? In what way? Fit the model with the offset \\(log(hrsSurv)\\) and interpret the coefficients. Determine how well the model fits using the deviance test. Fit a model using quasilikelihood. How does this approach change the model results? Source: [@Sue2010] ISBN 0-470-49140-X. Bullfrogs Big Bullfrog on the Pond: Use the field observation bullfrog data bullfrogs.csv to determine whether there is convincing evidence that the number of mates is related to the size of the bullfrog. Graph number of mates by size and comment. Graph log(number of mates) by size and comment. Write out the likelihood for the Poisson regression model. Fit the Poisson regression model and interpret the coefficient. Provide a measure of uncertainty. Conduct a goodness-of-fit test. Look at the residuals and comment. Is there evidence of overdispersion? Explain. Source: [@Ramsey2002]. 4.15.3 Open-ended Current Population Survey: Number of children at home. We examine factors associated with the number of children at home in the U.S. using an (admittedly old) sample from the March 1993 U. S. Current Population Survey (CPS). This data set is the Health Insurance and Hours Worked By Wives (HI) available through the Data Sets for Econometrics Ecdat package in R. There are 22,272 observations in data(HI) of which we sample 1000. We will focus on the following 12 variables: whrswk = hours worked per week by wife hhi = wife covered by husband’s health insurance (HI) ? whi = wife has HI thru her job ? hhi2 = husband has HI thru own job ? education = a factor with levels, “&lt; 9years”, “9-11years”, “12years”, “13-15years”, “16years”, “&gt; 16years” race = one of white, black, other hispanic = hispanic ? experience = years of potential work experience kidslt6 = number of kids under age of 6 at home kids618 = number of kids 6-18 years old at home husby = husband’s income in thousands of dollars region = one of other, northcentral, south, west wght = sampling weight Source [@Olson1998] Preliminaries. First total up the number of childen (which counts the number of children still at home). Second, calculate the wife’s age, taking advantage of the experience variable which is years since the wife turned 16. set.seed(1234) HIsample = sample(HI,1000) %&gt;% mutate(totalKids=kidslt6+kids618, age = 15+experience) It is nice to center continuous covariates so the intercepts are meaningful. # Centering income cIncome = with(HIsample,husby-mean(husby)) The levels for the education variable show up in a messy format when modeling, so we can relabel them. favstats(totalKids~education,data=HIsample) levels(HIsample$education) levels(HIsample$education) c(&quot;Middle School&quot;, &quot;Some HS&quot;, &quot;HS Grad&quot;, &quot;Some College&quot;, &quot;College Grad&quot;, &quot;Grad School&quot;) #check: favstats(totalKids~education,data=HIsample) EDA. Perform an exploratory data analysis using the HIsample data set. Use totalKids as the response variable. Modeling. Part I. Fit a series of models to investigate which factors are associated with the number of children at home. Start with model(s) involving age. Modeling. Part II. Consider assessing the need for fitting a zero-inflated by comparing the observed distribution of the number of totalKids to a modeled distribution of a Poisson model with \\(\\lambda\\) = mean(totalKids). Fit the simple model which includes only age. Perform a lack-of-fit test. What would a “true zero” be for a zero-inflated model in this case? Fit a ZIP model detailing your choice of covariates for the Poisson and binomial portions of your model. Fit a ZIP model with an offset for age. Defend your choice of covariates for the Poisson and binomial portions of your model. Summarize the results of your modeling including interpretations of the estimated coefficients. Airbnb In a break from traditional industrial models, companies like Airbnb and Uber are promoting the growth of a “sharing economy,” a way to supplement income by renting out or otherwise utilizing existing assets for individual profit. There are few barriers to entry to become a seller on the rental market for Airbnb, and its global reach provides an instantaneous way to make money for its wide user base. Airbnb is an online service that allows travelers to book a stay with only a few clicks at any given time. Customers are given the ability to tailor their travel experience to fit their budget and lifestyle, choosing from castles to tree houses, and finding plenty of offerings in rural towns as well as busy metropolitan areas. The range of options and immediate availability of many properties contribute to the rising popularity of Airbnb, but there has been little research done on the impact of individual user ratings for each listing on specific property features (such as location, number of bedrooms, etc.). As its consumer base expands across the globe, it is worth considering the factors that matter more or less to renters in different countries, and why that variation exists. With the data provided you can investigate the different housing characteristics associated with the number of reservations for an Airbnb listing and how these factors vary by country. London, New York City, and Sydney represent three of the top ten most popular destinations for both inbound and outbound guests, so those cities are included with this data to allow for an intercontinental comparative analysis of housing factors. There are three data files (LONDON 2 W DATES.csv, NYC 2 W DATES.csv, SYDNEY 2 W DATES.csv) and a file with preliminary R code (Intro Code for Airbnb Data.R) to help you get started. Perform an analysis that describes variation in the number of reviews (a proxy for the number or rentals, which is not available) as a function of the variables provided. Don’t forget to consider an offset, if needed. Use your model to determine whether there exists differences in this relationship between the cities. Source: [@Awad2017] Crab Satellites [@Brockmann1996] carried out a study of nesting female horseshoe crabs. Female horseshoe crabs often have male crabs attached to a female’s nest known as satellites. One objective of the study was to determine which characteristics of the female were associated with the number of satellites. Of particular interest is the relationship between the width of the female carapace and satellites. The data can be found in crab.csv It includes: NumSat = number of satellites Width = carapace width Wt = weight Sp = spine condition and C = color. Use Poisson regression to investigate the research question. Be sure you work to obtain an appropriate model before considering overdispersion. Doctor Visits I Data available via the US National Medical Expenditure Study [@NMES1988] was used to study the demand for medical care as measured by the number of doctor visits. It was conducted by Deb and Trivedi and involved a study of 4406 person 66 years and older. [@Deb1997]. The R package, Applied Econometrics with R (AER), contains a number of datasets. Here we consider the dataset related to doctor visits. To access the data #Install the package AER. library(AER) data(NMES1988) help(NMES1988) # to see variable descriptions Use the number of office visits as your dependent variable. Define a well-articulated research question and perform an analysis that includes relevant EDA. Consider the use of zero-inflated Poisson regression. Doctor Visits II Data on doctor visits on a sample of 5,190 from the 1977/1978 Australian Health Survey. The study seeks to explain the variation in doctor visits using one or more explanatory variables. The data is an R data set accessible with the command data(&quot;DoctorVisits&quot;). Variable descriptions can be found under help(NMES1988) Explore the use of a zero-inflated model for this data. Begin with a histogram of the number of visits, some EDA and fitting several models. Summarize your results. More Fish The number of fish caught, persons in the party, the number of children in the party, and the length of stay were recorded for 250 campers. The data can be found in fish2.csv. Create and assess a model for the number of fish caught. "],
["ch-glms.html", "Chapter 5 Generalized Linear Models (GLMs): A Unifying Theory 5.1 Learning Objectives 5.2 One parameter exponential families 5.3 Generalized Linear Modeling 5.4 Exercises", " Chapter 5 Generalized Linear Models (GLMs): A Unifying Theory 5.1 Learning Objectives Determine if a probability distribution can be expressed in one-parameter exponential family form. Identify canonical links for distributions of one parameter exponential family form. 5.2 One parameter exponential families Thus far, we have expanded our repertoire of models from OLS to include Poisson regression. But in the early 1970s Nelder and Wedderburn identified a broader class of models that generalizes the multiple linear regression we considered in the introductory chapter and are referred to as generalized linear models (GLMs) [@Nelder1972]. GLMs have similar forms for the likelihoods, MLEs, and variance. This makes it easier to find model estimates and their corresponding uncertainty. To determine whether a model is a GLM, we consider the following properties. When a probability formula can be written in the form below \\[\\begin{equation} f(y;\\theta)=exp^{[a(y)b(\\theta)+c(\\theta)+d(y)]} \\tag{5.1} \\end{equation}\\] and if the support (the set of possible input values) does not depend upon \\(\\theta\\), it is said to have a one-parameter exponential family form. We demonstrate that the Poisson distribution is a member of the one parameter exponential family by writing its probability mass function (pmf) in the form of Equation (5.1) and assessing its support. 5.2.1 One Parameter Exponential Family: Possion Recall we begin with \\[ P(Y=y)=\\frac{e^{-\\lambda}{\\lambda}^y}{y!} \\textrm{ where } y=0,1,2\\ldots\\infty \\] and consider the following useful identities for establish exponential form: \\[a=e^{log(a)} \\] \\[a^x = e^{xlog(a)}\\] \\[log(ab)=log(a)+log(b)\\] \\[log\\frac{a}{b}=log(a)-log(b)\\] Determining whether the Poisson model is a member of the one-parameter exponential family is a matter of writing the Poisson pmf in the form of Equation (5.1) and checking that the support does not depend upon \\(\\lambda\\). First, consider the condition concerning the support of the distribution. The set of possible values for any Poisson random variable is \\(y=0,1,2\\ldots\\infty\\) which does not depend on \\(\\lambda\\). The support condition is met. Now we see if we can rewrite the probability mass function in one-parameter exponential family form. \\[\\begin{eqnarray} P(Y=y)={e^{-\\lambda}e^{ylog \\lambda}e^{-log (y!)}} \\nonumber \\\\ =e^{ylog \\lambda-\\lambda-log (y!)} \\tag{5.2} \\end{eqnarray}\\] The first term in the exponent for Equation (5.1) must be the product of two factors, one solely a function of y, \\(a(y)\\) and another, \\(b(\\lambda)\\) a function of \\(\\lambda\\) only. The middle term in the exponent must be a function of \\(\\lambda\\) only; no \\(y&#39;s\\) should appear. The last term has only \\(y&#39;s\\) and no \\(\\lambda\\). Since this appears to be the case here, we can identify the different functions in this form: \\[\\begin{eqnarray} a(y)&amp;=&amp;y \\\\ b(\\lambda)&amp;=&amp;log(\\lambda) \\\\ c(\\lambda)&amp;=&amp;-\\lambda \\nonumber \\\\ d(y)&amp;=&amp;-log (y!) \\\\ \\tag{5.3} \\end{eqnarray}\\] These functions have useful interpretations in statistical theory. We won’t be going in to this in detail, but we will note that function \\(b(\\lambda)\\) or more generally \\(b(\\theta)\\) will be particularly helpful in GLMs. The function \\(b(\\theta)\\) is referred to as the canonical link. The canonical link is often a good choice to model as a linear function of the explanatory variables. That is, Poisson regression should be set up as \\(log(\\lambda)=\\beta_0+\\beta_1x_1+\\beta_2x_2+...\\). In fact, there is a distinct advantage modeling the canonical link as opposed to other functions of \\(\\theta\\), but it is also worth noting that other choices are possible, and at times preferred, depending upon the context of the application. There are other benefits of identifying a response as being from a one parameter exponential family. For example, by creating an unifying theory for regression modeling, Nelder and Wedderburn made possible a common and efficient method for finding estimates of model parameters using iteratively reweighted least squares (IWLS). In addition, we can use the one parameter exponential family form to determine the expected value and standard deviation of \\(Y\\). With statistical theory you can show that \\[E(Y) =-\\frac{c&#39;(\\theta)}{b&#39;(\\theta)}\\ \\textrm{ and } Var(Y) =\\frac{b&#39;&#39;(\\theta)c&#39;(\\theta)-c&#39;&#39;(\\theta)b&#39;(\\theta)}{[b&#39;(\\theta)]^3} \\] where differentiation is with respect to \\(\\theta\\). Verifying these results for the Poisson response: \\[\\begin{equation*} E(Y)=-\\frac{-1}{\\frac{1}{\\lambda}}=\\lambda \\textrm{ and } Var(Y)=\\frac{1/{{\\lambda}^2}} {(1/{\\lambda}^3)}=\\lambda \\end{equation*}\\] We’ll find that other distributions are members of the one parameter exponential family by writing their pdf or pmf in this manner and verifying the support condition. So for example, we’ll see that the binomial distribution meets these conditions, so it is also a member of the one parameter exponential family. The normal distribution is a special case. We note that we have two parameters, a mean and standard deviation. If we assume, however, that one of the parameters is known, then we can show that a normal random variable is from a one parameter exponential family. 5.2.2 One parameter exponential family: Normal Here we determine whether a normal distribution is a one parameter exponential family member. First we will need to assume that \\(\\sigma\\) is known. Next, possible values for a normal random variable range from \\(-\\infty\\) to \\(\\infty\\), so we have no problems with the support. Finally, we’ll need to write the probability density function (pdf) in the one parameter exponential family form. We start with the familiar form: \\[ f(y)=\\frac{1}{{\\sigma \\sqrt{2\\pi}}}{e^{-\\frac{(y-\\mu)^2}{2\\sigma^2}}} \\] Even writing \\({1/{\\sigma \\sqrt{2\\pi}}}\\) as \\(e^{-log{\\sigma}-log(2\\pi)/2}\\) we still do not have the pdf written in one parameter exponential family form. We will first need to expand the exponent so that we have \\[ f(y)=e^{-log{\\sigma}-log(2\\pi)/2}{e^{-\\frac{y^2-2y\\mu +\\mu^2}{2\\sigma^2}}} \\] Assuming \\(\\sigma\\) is known, we have \\[ f(y) \\propto e^{-(-2y\\mu + \\mu^2 + y^2 )/2\\sigma^2} \\] From this result, we can see that the canonical link for a normal response is \\(\\mu\\) which is consistent with what we’ve been doing with OLS, since the simple linear regression model has the form: \\[ \\mu_{Y|X} = \\beta_0 + \\beta_1X. \\] 5.3 Generalized Linear Modeling GLM theory suggests that the canonical link can be modeled as a linear combination of the explanatory variable(s). This approach unifies a number of modeling results used throughout the text. One example is that likelihoods will be used to compare models in the same way for parameter exponential family members. We have now generalized our modeling to handle non-normal responses. For example, in addition to normally distributed responses, we are able to handle Poisson responses, binomial responses, and more. Writing a pmf or pdf for a response in one parameter exponential family form reveals the canonical link which can be modeled as a linear function of the predictors. This linear function of the predictors is the last piece of the puzzle for performing generalized linear modeling. But, in fact, it is really nothing new. We already use linear combinations when modeling normally distributed data. Three Components of a GLM Distribution of Y (e.g., Poisson) Link Function (a function of the parameter, e.g., log\\((\\lambda)\\) for Poisson) Linear Predictor (choice of predictors, e.g., \\(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ...\\)) Table 5.1: One parameter exponential family form and canonical links. Distribution One-parameter Exponential Family Form Canonical Link Binary Binomial logit(p) Poisson P(Y=y) = eylogλ-λ-y! log(λ) Normal f(y) ∝ e^(-2y+ 2 + y2) Exponential Gamma Geometric Completing Table 5.1 is left as an exercise. In the chapter on Poisson modeling, we provided heuristic rationale for using the log() function as our link. That is, counts would be non-negative but a linear function inevitably goes negative. By taking the logarithm of our parameter \\(\\lambda\\) we could use a linear predictor and not worry that it can take on negative values. Now we have theoretical justification for this choice, as the log is the canonical link for poisson data. In the next chapter we encounter yet another type of response, a binary response, which calls for a different link function. Our work here suggests that we will model logit(p)=\\(log(\\frac{p}{1-p})\\) using a linear predictor. [Note that generalized linear models (GLMs) differs from General Linear Models. The general linear model is a statistical linear model with multivariate vectors as responses. For example, each subject in a study may have their height, weight, and shoe size recorded and modeled as a function of age and sex. The response is a vector, Y = (height, weight, shoe size), for each study participant. Age and sex are explanatory variables in the model. The residual is usually assumed to follow a multivariate normal distribution. If the residual is not a multivariate normal distribution, then generalized linear models may be used to relax assumptions about Y and the variance-covariance structure.] 5.4 Exercises For each distribution, Write the pdf in one parameter exponential form, if possible. Describe an example of a setting where this random variable might be used. Identify the canonical link function, and Compute \\(\\mu = -\\frac{c&#39;(\\theta)}{b&#39;(\\theta)}\\) and \\(\\sigma^2 = \\frac{b&#39;&#39;(\\theta)c&#39;(\\theta)-c&#39;&#39;(\\theta)b&#39;(\\theta)}{[b&#39;(\\theta)]^3}\\) and compare with known E(Y) and Var(Y). Binary: Y = 1 for a success, 0 for a failure \\[p(y)=p^{y}(1-p)^{(1-y)} \\] Binomial: Y = number of successes in n independent, identical trials \\[p(y)=\\left(\\begin{array} {c} n\\\\y \\end{array}\\right) p^y(1-p)^{(n-y)} \\] Poisson: Y = number of events occurring in a given time (or space) when the average event rate is \\(\\lambda\\) per unit of time (or space) \\[ P(Y=y)=\\frac{e^{-\\lambda}\\lambda^y}{y!} \\] Normal (with fixed \\(\\sigma\\)) \\[f(y; \\mu)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(y-\\mu)^2}{2\\sigma^2}}\\] Exponential: Y = time spent waiting for the first event in a Poisson process with an average rate of \\(\\lambda\\) events per unit of time \\[f(y)=\\lambda e^{-\\lambda y}\\] Gamma (for fixed \\(r\\)): Y = time spent waiting for the \\(r^{th}\\) event in a Poisson process with an average rate of \\(\\lambda\\) events per unit of time \\[f(y; \\lambda)=\\frac{\\lambda^r y^{(r-1)}e^{-\\lambda y}}{\\Gamma(r)}\\] Geometric: Y = number of failures before the first success in a Bernoulli process \\[p(y)=(1-p)^{y}p\\] Negative Binomial (for fixed \\(r\\)): Y = number of failures prior to the \\(r^{th}\\) success in a Bernoulli process \\[\\begin{eqnarray} p(y; r) &amp; = &amp; \\left(\\begin{array} {c} y+r-1\\\\r-1 \\end{array}\\right)(1-p)^{y}p^r \\nonumber \\\\ &amp; = &amp; \\frac{\\Gamma(y+r)}{\\Gamma(r)y!} (1-p)^{y}p^r \\textrm{ for non-integer } r \\\\ \\end{eqnarray}\\] Pareto (for fixed \\(k\\)): \\[f(y; \\theta)=\\frac{\\theta k^\\theta}{y^{(\\theta+1)}} \\textrm{ for } y\\geq k; \\theta \\geq 1\\] Complete Table 5.1 containing your results of the preceding exercises. "],
["ch-logreg.html", "Chapter 6 Logistic Regression 6.1 Learning Objectives 6.2 Introduction 6.3 A Modeling Framework 6.4 Case Studies Overview 6.5 GLM Theory for Binomial Outcomes 6.6 Case Study: Reconstructing Alabama 6.7 Case Study: Who wants to lose weight? Sex, Media, Sports, and BMI. 6.8 References 6.9 Exercises", " Chapter 6 Logistic Regression 6.1 Learning Objectives Identify a binomial random variable and assess the validity of the binomial assumptions. Write the binomial probability mass function in one parameter exponential family form and identify the canonical link. Describe how to determine the maximum likelihood estimate (MLE) for \\(p\\) using the likelihood. Write a generalized linear model for binomial responses in two forms, one as a function of the logit and one as a function of \\(p\\). Explain how fitting a logistic regression differs from fitting an ordinary least squares (OLS) regression model. Interpret estimated coefficients. Use the residual deviance to compare models, to test for lack-of-fit when appropriate, and to check for unusual observations or needed transformations. 6.2 Introduction 6.2.1 Binary Responses A binary response takes on only two values: success (Y=1) or failure (Y=0), Yes (Y=1) or No (Y=0), etc. Binary responses are ubiquitous; in fact, binary responses are one of the most common types of data statisticians encounter. Here are just a few examples; note how the response in each example is binary, while the explanatory can take on many forms: Are students with poor grades more likely to binge drink? [@Brewer2007]. Y = a student will or will not binge drink X = a measure of academic performance What is the chance you are accepted into medical school given your GPA and MCAT scores? Y = accepted to medical school or rejected X = MCAT scores and GPA Does a single mom have a better chance of marrying the baby’s father if she has a boy? [@Lundberg2003] Y = a mother marries baby’s father or not X = sex of the baby Are students participating in sports in college more or less likely to graduate? [@Stevenson2010] Y = a student graduates or not X = participation in sports, type of sport, and gender Is exposure to a particular chemical associated with a cancer diagnosis? Y = cancer diagnosis X = chemical exposure 6.2.2 Binomial Responses: sums of binary responses The binary responses above can be summed to yield a binomial response. Binomial responses are characterized by the number of identical, independent trials, \\(n\\), and \\(p\\), the probability of success on a given trial. A sequence of independent trials like this with the same probability of success is called a Bernoulli process. Our objective in modeling binomial responses is to quantify how the probability of success, \\(p\\), is associated with relevant covariates. When modeling a binomial response, responses with common covariate values are assumed to have the same probability of success. For example, consider the following 2 \\(\\times\\) 2 table with exposure status by cancer diagnosis. Exposed Unexposed Cancer Ye Yu Cancer-free ne - Ye nu - Yu Total ne nu The number with cancer diagnoses among those (\\(n_e\\)) who are exposed, \\(Y_e\\), could be modeled using a binomial random variable. The variable \\(Y_u\\) could be defined in a similar manner. Each of the \\(n_e\\) exposed are assumed to have the same probability of cancer, \\(p_e\\). Likewise, each of the \\(n_u\\) unexposed is thought to have had the same probability of cancer, \\(p_u\\). A summary of features of a binomial random variable is given below. Binomial Random Variables Properties of a Binomial random variable Y = number of successes out of n independent, identical trials \\(p\\) = the probability of success on a single trial Possible values for Y: 0, 1, 2, \\(\\ldots\\)n Mean = \\(E(Y)=np\\) Variance = \\(Var(Y)=np(1-p)\\) implies a standard deviation = \\(SD(Y)=\\sqrt{np(1-p)}\\) Probability of exactly \\(y\\) succcesses: \\[\\begin{equation} P(Y=y) = \\left(\\begin{array}{c} n \\\\ y\\\\ \\end{array}\\right)p^y(1-p)^{n-y} \\tag{6.1} \\end{equation}\\] A binary outcome is a special case of a binomial response where \\(n=1\\). The parameter of interest is \\(p\\). Once \\(n\\) is known, the variance changes as \\(p\\) changes, similar to what we saw with a Poisson response and \\(\\lambda\\) in Chapter 4. With a binomial response, however, the variance \\(np(1-p)\\) is maximized at \\(p = 0.5\\); it does not always increase like Poisson random variables. Nonetheless, we are no longer bound by the equal variance assumption of OLS inference. 6.2.3 An Example: Binge Drinking Here we demonstrate how a probability can be calculated for a binomial random variable and how the binomial assumptions can be evaluated. Suppose we know that among a college aged population the probability of binge drinking is 0.05, and \\(n\\)= 10 students are randomly selected. What is the probability that at least 1 of the students selected binge drinks. With Equation ??, we can calculate the probability of any number of students from 0 to \\(n\\). \\[ P(Y=y) = \\left(\\begin{array}{c} 10 \\\\ y\\\\ \\end{array}\\right)0.05^y0.095^{10-y} \\] We are interested in \\(P(Y \\geq 1)=P(Y=1)+P(Y=2)+...+P(Y=10)\\). But to calculate \\(P(Y \\geq 1)\\), it’s easier to compute \\(1- P(Y=0)\\): \\[\\begin{eqnarray*} P(Y \\geq 1)&amp; = &amp; 1 - \\left(\\begin{array}{c} 10 \\\\ 0\\\\ \\end{array}\\right)0.05^00.095^{10}\\\\ &amp; = &amp; 0.40 \\end{eqnarray*}\\] The assumption for the binomial response is that the trials (sample selections) are independent of one another. Because the selections were made at random, there is no reason to believe they will not be independent. If instead different fraternities were selected and members of the fraternity questioned about their drinking habits, responses are not likely to be independent of one another. 6.2.4 A Graphical Look at Binomial Regression Figure 6.1 illustrates a comparison of an ordinary least squares (OLS) used for inference to a regression for a binomial variable. Figure 6.1: Regession Models: Linear Regression (left) and Binomial Regression (right) The graphic displaying OLS inferential model appears in the left panel of Figure 6.1. It shows that for each level of X, the responses appear to be approximately normal. The panel on the right side of Figure 6.1 depicts what a binomial regression model looks like. For each level of X, the responses follow a binomial distribution. In the case of OLS, the mean responses for each level of X, \\(\\mu_{Y|X}\\), fall on a line. In the case of the binomial data, the mean values of \\(Y\\) at each level of \\(X\\), \\(np_{Y|X}\\), fall on a curve, not a line. The variation in \\(Y\\) at each level of X for the OLS example, \\(\\sigma_{Y|X}^2\\), is the same. The responses at each level of X become more variable near \\(p=.50\\). This is to be expected given that the in the case of binomial regression the variance changes with the mean; specifically, the function \\(Var(Y)=np_{Y|X}(1-p_{Y|X})\\) is maximized at \\(p_{Y|X}=.50\\) and minimized at \\(p_{Y|X}=0\\) or \\(p_{Y|X}=1\\). For binomial regression small values of \\(np_{Y|X}\\), are associated with a distribution that is noticeably skewed. As \\(np_{Y|X}\\) increases the distribution of the responses begins to look more and more like a normal distribution. For the remainder of the chapter, we drop the \\(Y|X\\) subscripts but keep in mind that when modeling that the probability of success differs for different values of X. 6.3 A Modeling Framework In the past with OLS, you have used a line to model the average response, \\(\\mu_Y\\), as a linear function of an explanatory variable, \\(X\\). Because the objective of modeling a binomial response is to associate the probability of success, \\(p\\), with the covariates, we may be tempted to try a linear model for the average binary response, \\(p\\), such as \\(p = \\beta_0+\\beta_1X\\). This model, however, doesn’t work well for binomial data. Describing the parameter, \\(p\\), with a line has some serious drawbacks. One problem is that \\(p\\) should range from 0 to 1. A line, however, is certain to yield estimates for \\(p\\) that are less than 0 and greater that 1. One way to avoid this problem is to model the odds instead of \\(p\\). Odds are calculated by taking the ratio of the number of successes to the number of failures. Odds can take on values from 0 to \\(\\infty\\) but still cannot be negative. One solution is to take the log of the odds, referred to as a logit. Logits will take on values from -\\(\\infty\\) to \\(\\infty\\) and, more significantly, represent the canonical link from the Generalized Linear Model form of the binomial model from Chapter 5. Thus, logits will be suitable for modeling with a linear function of the predictors: \\[\\begin{equation} log(\\frac{p}{1 - p})=\\beta_0+\\beta_1X \\tag{6.2} \\end{equation}\\] Models of this form are referred to as binomial regression models, or more generally as logistic regression models. We take a look at these models in the context of a few case studies. 6.4 Case Studies Overview After the first section of this chapter you should be able to recognize binary and binomial random variables and know the assumptions needed for inference. Next we consider three examples with some real data. The first two examples involve binomial data (Soccer Goalkeepers and Reconstruction in Alabama). The last case uses binary data (Trying to Lose Weight). Here are the statistical concepts you will encounter for each Case Study. The soccer goalkeeper data can be written in the form of a 2 \\(\\times\\) 2 table. This example is used to describe some of the underlying theory for logistic regression. We demonstrate how binomial probability mass functions (pmfs) can be written in one parameter exponential family form, from which we can identify the canonical link as in Chapter 5. Using the canonical link, we write a Generalized Linear Model for binomial counts and determine corresponding MLEs for model coefficients. Interpretation of the estimated parameters involves a fundamental concept, the odds ratio. The Railroad Refernda is another binomial example which introduces the notion of deviances. Deviances are used in logistic regression to compare and assess models. We check the assumptions of logistic regression using empirical logit plots and deviance residuals. In addition, under certain circumstances, deviances are used to determine if a model exhibits significant lack-of fit. The last case study addresses why teens try to lose weight. Here the response is a binary variable which allows us to analyze individual level data. The analysis builds on concepts from the previous sections in the context of a random sample from CDC’s Youth Risk Behavior Survey (YRBS). 6.5 GLM Theory for Binomial Outcomes 6.5.1 Case Study: Soccer Goalkeeper Saves [@Roskes2011] looked at penalty kicks in the men’s World Cup soccer championship from 1982 - 2010 and found data on 204 penalty kick shootouts. 6.5.2 Research Question Does the probability of a save depend upon whether the goalkeeper’s team is behind or not? 6.5.3 Data Organization The data for this study is summarized in Table 6.1. Table 6.1: Soccer goalkeepers’ saves when their team is and is not behind. Source: Roskes et al. 2011 Psychological Science Behind Not Behind Total Saves 2 39 41 Scores 22 141 163 Total 24 180 204 As noted in Section 6.3, the odds are the number of successes divided by the number of failures. The log of the odds, the logit, makes a good choice for a linear model because it takes on values from \\(-\\infty\\) to \\(\\infty\\). Odds are one way to quantify a goalkeeper’s performance. Here the odds that a goalkeeper makes a save when his team is behind is 2 to 22 or 0.09 to 1. Or equivalently, the odds that a goal is scored on a penalty kick is 22 to 2 or 11 to 1. An odds of 11 to 1 tells you that a shooter who’s team is behind will score 11 times for every 1 shot that the goalkeeper saves. When the goalkeeper’s team is not behind the odds a goal is scored is 141 to 39 or 3.61 to 1. We see that the odds of a goal scored on a penalty kick are better when the goalkeeper’s team is behind than when it is not behind (i.e., better odds of scoring when the shooter’s team is ahead). We can compare these odds by calculating the odds ratio (OR), 11/3.61 or 3.05, which tells us that the odds of a successful penalty kick are 3.05 times higher when the shooter’s team is leading. In our example, it is also possible to estimate the probability of a goal, \\(p\\), for either circumstance. When the goalkeeper’s team is behind, we have the probability of a successful penalty kick is \\(p\\) = 22/24 or 0.833. It is easy to see that the ratio of the probability of a goal scored divided by the probability of no goal is \\((22/24)/(2/24)=22/2\\) or 11, the odds we had calculated above. The same calculation can be made when the goalkeeper’s team is not behind. More generally, when it makes sense to estimate a probability from data, the odds can be written as \\[\\frac{\\# \\textrm{successes}}{\\# \\textrm{failures}}= \\frac{\\# \\textrm{successes}/n}{\\# \\textrm{failures}/n}= \\frac{p}{1-p}\\]. It is then the log of the odds, the logit, that we model in logistic regression. We present some rationale below using GLM theory. We established that generalized linear models (GLMs) are a way in which to model a variety of different types of responses, including binomial responses. In this chapter, we apply the general results of the GLMs to this specific application of binomial responses. Let Y = the number scored out of \\(n\\) penalty kicks. The parameter, \\(p\\), is the probability of a score on a penalty kick. Recall that the theory of GLM is based on the unifying notion of the one-parameter exponential family form: \\[\\begin{equation} f(y;\\theta)=e^{[a(y)b(\\theta)+c(\\theta)+d(y)]} \\tag{6.3} \\end{equation}\\] To see that we can apply the general approach of GLMs to binomial responses, we first write an expression for the probability of a binomial response and then use a little algebra to rewrite it until we can demonstrate that it, too, can be written in one-parameter exponential family form with \\(\\theta = p\\). This will provide a way in which to specify the canonical link and the form for the model. Additional theory allows us to deduce the mean, standard deviation, and more from this form. If Y follows a binomial distribution with \\(n\\) trials and probability of success \\(p\\), we can write: \\[\\begin{eqnarray} P(Y=y)&amp;=&amp; \\left(\\begin{array}{c} n \\\\ y\\\\ \\end{array}\\right)p^y(1-p)^{n-y} \\\\ &amp;=&amp;e^{ylogp + (n-y)log(1-p) + log\\scriptsize{(\\begin{array}{c} n \\\\ y\\\\ \\end{array})}} \\tag{6.4} \\end{eqnarray}\\] However, this probability mass function is not quite in one parameter exponential family form. Note that there are two terms in the exponent which consist of a product of functions of \\(y\\) and \\(p\\). So more simplification is in order: \\[\\begin{equation} P(Y=y) = e^{ylog(\\frac{p}{1-p}) + nlog(1-p)+\\scriptsize log\\scriptsize{(\\begin{array}{c} n \\\\ y\\\\ \\end{array})}} \\tag{5.2} \\end{equation}\\] Don’t forget to consider the support; we must make sure that the set of possible values for this response is not dependent upon \\(p\\). For fixed \\(n\\) and any value of \\(p\\), \\(0&lt;p&lt;1\\), all integer values from 0 to n are possible, so the support is indeed independent of \\(p\\). The one parameter exponential family form for binomial responses suggests that the canonical link is \\(log(\\frac{p}{1-p})\\). GLM theory suggests that constructing a model using the logit, the log odds \\(\\frac{p}{1-p}\\), of a score, as a linear function of covariates is a reasonable approach. In our example we could define \\(X=0\\) for not behind and \\(X=1\\) for behind and fit the model: \\[\\begin{equation} log(\\frac{p_X}{1-p_X})=\\beta_0 +\\beta_1X \\tag{6.5} \\end{equation}\\] where \\(p_X\\) is the probability of a successful penalty kick given X. So, based on this model, the log odds of a successful penalty kick when the goalkeeper’s team is not behind is: \\[ log(\\frac{p_0}{1-p_0}) = log(\\frac{p_0}{1-p_0})=\\beta_0 \\nonumber \\] and the log odds when the team is behind is: \\[ log(\\frac{p_1}{1-p_1})=\\beta_0+\\beta_1. \\nonumber \\] We can see that \\(\\beta_1\\) is the difference between the log odds of a successful penalty kick between games when the goalkeeper’s team is not behind and games when the team is behind. Using rules of logs: \\[\\begin{equation} \\beta_1 = (\\beta_0 + \\beta_1) - \\beta_0 = log(\\frac{p_1}{1-p_1}) - log(\\frac{p_0}{1-p_0}) = log\\frac{p_1/(1-p_1)}{p_0/{(1-p_0)}}. \\tag{6.6} \\end{equation}\\] Thus \\(e^{\\beta_1}\\) is the ratio of the odds of scoring when the goalkeeper’s team is not behind compared to scoring when the team is behind. In general, exponentiated coefficients in logistic regression are odds ratios (OR). A general interpretation of an OR is the odds of success for group A compared to the odds of success for group B—how many times greater the odds of success are in group A compared to group B. The logit model (Equation (6.6)) can also be re-written in a probability form: \\[\\begin{equation} p=\\frac{e^{\\beta_0+\\beta_1X}}{1+e^{\\beta_0+\\beta_1X}} \\tag{6.7} \\end{equation}\\] which can be re-written for games when the goalkeeper’s team is behind as: \\[\\begin{equation} p_1=\\frac{e^{\\beta_0+\\beta_1}}{1+e^{\\beta_0+\\beta_1}} \\tag{6.8} \\end{equation}\\] and for games when the goalkeeper’s team is not behind as: \\[\\begin{equation} p_0=\\frac{e^{\\beta_0}}{1+e^{\\beta_0}} \\tag{6.9} \\end{equation}\\] We use likelihood methods to estimate \\(\\beta_0\\) and \\(\\beta_1\\). As we had done in Chapter 2, we can write the likelihood for this example in the following form: \\[Lik(p_1, p_0) = {28 \\choose 22}p_1^{22}(1-p_1)^{2} {180 \\choose 141}p_0^{141}(1-p_0)^{39}\\] Our interest centers on estimating \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\), not \\(p_1\\) or \\(p_0\\). So we replace \\(p_1\\) in the likelihood with an expression for \\(p_1\\) in terms of \\(\\beta_0\\) and \\(\\beta_1\\) as in Equation (6.8). Similarly, \\(p_0\\) in Equation (6.9) involves only \\(\\beta_0\\). After removing constants, the new likelihood looks like: \\[\\begin{eqnarray} Lik(\\beta_0,\\beta_1)&amp;=&amp; \\left( \\frac{e^{\\beta_0+\\beta_1}}{1+e^{\\beta_0+\\beta_1}}\\right)^{22}\\left(1- \\frac{e^{\\beta_0+\\beta_1}}{1+e^{\\beta_0+\\beta_1}}\\right)^{2}\\\\ \\nonumber &amp; &amp; *\\left(\\frac{e^{\\beta_0}}{1+e^{\\beta_0}}\\right)^{141}\\left(1-\\frac{e^{\\beta_0}}{1+e^{\\beta_0}}\\right)^{39} \\tag{6.10} \\end{eqnarray}\\] Now what? Fitting the model means finding estimates of \\(\\beta_0\\) and \\(\\beta_1\\), but familiar methods from calculus for maximizing the likelihood don’t work here. Instead, we could consider all sorts of possible combinations of \\(\\beta_0\\) and \\(\\beta_1\\). That is, we will pick that pair of values for \\(\\beta_0\\) and \\(\\beta_1\\) that will yield the largest likelihood for our data. Trial and error to find the best pair is tedious at best, but more efficient numerical methods are available. The MLEs for the coefficients in the soccer goalkeeper study (provided by most statistical software packages) are \\(\\hat{\\beta_0}= 1.2852\\) and \\(\\hat{\\beta_1}=1.1127\\). ## # A tibble: 2 x 4 ## behind scores shots prop1 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Yes 22.0 24.0 0.917 ## 2 No 141 180 0.783 ## glm(formula = prop1 ~ behind, family = binomial(link = &quot;logit&quot;), ## data = soccer, weights = shots) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.2852 0.1809 7.104 1.22e-12 *** ## behindYes 1.1127 0.7604 1.463 0.143 Exponentiating \\(\\hat{\\beta_1}\\) provides an estimate of the odds ratio (the odds of scoring when the goalkeeper’s team is behind compared to the odds of scoring when the team is not behind) of 3.04, which is consistent with our calculations using the 2 \\(\\times\\) 2 table. We estimate that the odds of scoring when the goalkeeper’s team is behind is over 3 times that of when the team is not behind or, in other words, the odds a shooter is successful in a penalty kick shootout are 3.04 times higher when his team is leading. Time out (Optional). Discuss the following quote from the study abstract: “Because penalty takers shot the two sides of the goal equally often, the goalkeepers’ right-oriented bias was dysfunctional, allowing more goals to be scored.” Construct an argument for why the greater success observed when the goalkeeper’s team was behind might be better explained from the shooter’s perspective. Before we go on, you may be curious as to why there is no error term in our model statements for logistic or Poisson regression. One way to look at it is to consider that all models describe how observed values are generated. With the logistic model we assume that the observations are generated as a binomial variables. Each observation or realization of \\(Y\\) = number of successes in n independent and identical trials with a probability of success on any one trial of \\(p\\) is produced by Y \\(\\sim\\) Binomial\\((n,p)\\). So the randomness in this model is not introduced by an added error term but rather by appealing to a Binomial probability distribution, where variability depends only on \\(n\\) and \\(p\\) through \\(Var(Y)=np(1-p)\\), where \\(n\\) is usually considered fixed and \\(p\\) is the parameter of interest. 6.6 Case Study: Reconstructing Alabama You probably are aware that statistics are used in science experiments, surveys, and sports. This case study demonstrates how wide ranging applications of statistics can be. Many would not associate statistics with historical research, but this case study shows that it can be done. US Census data from 1870 helped historian Michael Fitzgerald of St. Olaf College gain insight into important questions about how railroads were supported during the Reconstruction Era. In a paper entitled “Reconstructing Alabama: Reconstruction Era Demographic and Statistical Research,” Ben Bayer performs an analysis of data from 1870 to explain influences on voting on referendums related to railroad subsidies. Positive votes are hypothesized to be inversely proportional to the distance a voter is from the proposed railroad, but the racial composition of a community (as measured by the percentage of blacks) is hypothesized to be associated with voting behavior as well. Separate analyses of three counties in Alabama—Hale, Clarke, and Dallas—were performed; we discuss Hale County here. This example differs from the soccer example in that it includes continuous covariates. 6.6.1 Research Question Was voting on railroad referenda related to distance from the proposed railroad line and the racial composition of a community? 6.6.2 Data Organization The unit of observation for this data is a location or community in Hale County, of which there are 11 without missing data. We will focus on the following variables collected for each community: YesVotes = the number of “Yes” votes in favor of the proposed railroad line (our primary response variable) NumVotes = total number of votes cast in the election pctBlack = the percentage of blacks in the community distance = the distance, in miles, the proposed railroad is from the community To get a feel for the data, several rows are reproduced below. Table 6.2: Sample of the data for the Hale County, Alabama, railroad subsidy vote. community pctBlack distance YesVotes NumVotes Carthage 58.4 17 61 110 Cederville 92.4 7 0 15 Greensboro 59.4 0 1790 1804 Havana 58.4 12 16 68 6.6.3 Exploratory Data Analysis We use a coded scatterplot to get a look at our data. Figure 6.2 portrays the relationship between distance and pctBlack coded by the InFavor status (whether a community supported the referendum with over 50% Yes votes). All of those communities in favor of the railroad referendum are over 60% black. All of those opposed are 7 miles or farther from the proposed line. The overall percentage of voters in Hale County in favor of the railroad is 87.9%, although that percentage drops to 58.3% if Greensboro is excluded. Figure 6.2: Scatterplot of distance from a proposed rail line and percent black in the community coded by whether the community was in favor of the referendum or not. Recall that for a model with two covariates, the model has the form: \\[log(odds) =log(\\frac{p}{1-p}) = \\beta_0+\\beta_1X_1+\\beta_2X_2.\\] where \\(p\\) is the proportion of Yes votes in a community. In logistic regression, we expect the logits, the log(odds), to be a linear function of X, the predictors. To assess the linearity assumption, we construct empirical logit plots, where “empirical” just means “based on sample data.” Empirical logits are computed for each community by taking \\(log(\\frac{\\textrm{number of successes}}{\\textrm{number of failures}})\\). In Figure 6.3, we see that a plot of empirical logits versus distance produces a plot that looks linear, which is the relationship specified by the logistic regression model. In contrast, the empirical logits by percent black reveal that Greensboro is quite a distance from the otherwise linear pattern; this suggests that Greensboro is an outlier and possibly an influential point. Greensboro has 99.2% voting yes with only 59.4% black. As a side note, it is important to realize that the empirical logits are based on the sample odds, and are not what is modeled when performing logistic regression. Figure 6.3: Empirical logit plots for the Railroad Referendum data. Source: Bayer and Fitzgerald. In addition to examining how the response correlates with the predictors, it is a good idea to determine whether the predictors correlate with one another. Here the correlation between distance and percent black is moderately high at -0.49. We’ll watch to see if the correlation affects the stability of our odds ratio estimates. 6.6.4 Modeling Overview We begin by fitting a logistic regression model with distance alone. Then the covariate pctBlack is added. The Wald-type test and the drop-in-deviance provide strong support for the addition of percent Black to the model. The model with distance and percent Black has a large residual deviance suggesting an ill-fitting model. Possible reasons for the lack-of-fit are (1) omitting important covariates, (2) extreme observations, or (3) overdispersion. We have used the covariates at hand, and it may well be that the model is inadequate, but we will proceed nonetheless. A look at the residuals indicates that Greensboro is an extreme observation. Models without Greensboro are fit and compared to our initial models. Seeing no appreciable improvement or differences with Greensboro removed, we leave it in the model. There remains a large residual deviance so we attempt to account for it by using an estimated dispersion parameter similar to Section 4.12 with Poisson regression. The final model includes distance and percent black, and it adjusts for overdispersion. 6.6.5 Results 6.6.5.1 Initial Models The first model includes only one covariate, distance. glm(formula = cbind(YesVotes, NumVotes - YesVotes) ~ distance, family = binomial, data = rrHale.df) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 3.30927 0.11313 29.25 &lt;2e-16 *** distance -0.28758 0.01302 -22.09 &lt;2e-16 *** --- Null deviance: 988.45 on 10 degrees of freedom Residual deviance: 318.44 on 9 degrees of freedom AIC: 360.73 Our estimated binomial regression model is: \\[log\\frac{\\hat{p}_i}{1-\\hat{p}_i}=3.309-0.288*distance_i\\] where \\(\\hat{p}_i\\) is the estimated proportion of Yes votes in community \\(i\\). The estimated odds ratio for distance, that is the exponentiated coefficient for distance, in this model is \\(e^{-0.288}=0.750\\). It can be interpreted as follows: for each additional mile from the proposed railroad, the support (odds of a Yes vote) declines by 25.0%. The covariate pctBlack is then added to the first model. Despite the somewhat high correlation between the percent black and distance, the estimated odds ratio for distance remains approximately the same in this new model (OR = 0.747); controlling for percent black does little to change our estimate of the effect of distance. For each additional mile from the proposed railroad, odds of a Yes vote declines by 25.3% after adjusting for the racial composition of a community. glm(formula = cbind(YesVotes, NumVotes - YesVotes) ~ distance + pctBlack, family = binomial, data = rrHale.df) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 4.222021 0.296963 14.217 &lt; 2e-16 *** distance -0.291735 0.013100 -22.270 &lt; 2e-16 *** pctBlack -0.013227 0.003897 -3.394 0.000688 *** --- Null deviance: 988.45 on 10 degrees of freedom Residual deviance: 307.22 on 8 degrees of freedom AIC: 351.51 exp(coef(model.HaleBD)) (Intercept) distance pctBlack 68.1711286 0.7469668 0.9868600 exp(confint(model.HaleBD)) 2.5 % 97.5 % (Intercept) 38.2284603 122.6115988 distance 0.7276167 0.7659900 pctBlack 0.9793819 0.9944779 anova(model.HaleD, model.HaleBD, test = &quot;Chisq&quot;) Analysis of Deviance Table Model 1: cbind(YesVotes, NumVotes - YesVotes) ~ distance Model 2: cbind(YesVotes, NumVotes - YesVotes) ~ distance + pctBlack Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) 1 9 318.44 2 8 307.22 1 11.222 0.0008083 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 6.6.5.2 Tests for significance of model coefficients Do we have statistically significant evidence that support for the railroad referendum decreases with higher proportions of black residents in a community, after accounting for the distance a community is from the railroad line? As discussed in Section 4.7.2 with Poisson regression, there are two primary approaches to testing signficance of model coefficients. Drop-in-deviance test to compare models Compute the deviance for each model, then calculate: drop-in-deviance = residual deviance for reduced model - residual deviance for the larger model. When the reduced model is true, the drop-in-deviance \\(\\sim \\chi^2_d\\) where d = the difference in the degrees of freedom associated with the two models (that is, the difference in the number of terms / coefficients). A large drop-in-deviance favors the larger model. Wald test for a single coefficient Wald-type statistic = estimated coefficient / standard error When the true coefficient is 0, for sufficiently large \\(n\\), the test statistic \\(\\sim\\) N(0,1). If the magnitude of the test statistic is large, there is evidence that the true coefficient is not 0. If we consider our larger model to be \\(log\\frac{p_i}{1-p_i} = \\beta_0+\\beta_1*distance_i+\\beta_2*pctBlack_i\\), then the Wald test evaluates \\(H_O:\\beta_2=0\\) vs. \\(H_A:\\beta_2 \\neq 0\\) with the test statistic \\(Z=\\frac{-0.0132}{.0039}=-3.394\\). This produces a highly significant p-value \\(p=.00069\\), indicating significant evidience that support for the railroad referendum decreases with higher proportions of black residents in a community, after adjusting for the distance a community is from the railroad line. The drop in deviance test would compare the larger model above to the reduced model \\(log\\frac{p_i}{1-p_i} = \\beta_0+\\beta_1*distance_i\\) by comparing residual deviances from the two models. The drop-in-deviance test statistic is \\(318.44 - 307.22 = 11.22\\) on \\(9 - 8 = 1\\) df, producing a p-value of .00081, in close agreement with the Wald test. A third approach to determining significance of \\(\\beta_2\\) would be to generate a 95% confidence interval and then see if 0 falls within the interval, or if 1 falls within a 95% confidence interval for \\(e^{\\beta_2}\\). The next section describes two approaches to producing a confidence interval for coefficients in logistic regression models. 6.6.5.3 Confidence intervals for model coefficients Since the Wald statistic follows a normal distribution with \\(n\\) large, we could generate a Wald-type (normal-based) confidence interval for \\(\\beta_2\\) using: \\[\\hat\\beta_2 \\pm 1.96*SE(\\hat\\beta_2)\\] and then exponentiating endpoints if we prefer a confidence interval for the odds ratio \\(e^{\\beta_2}\\). In this case, \\[\\begin{eqnarray} 95\\% \\textrm{ CI for } \\beta_2 &amp; = &amp; \\hat\\beta_2 \\pm 1.96*SE(\\hat\\beta_2) \\\\ &amp; = &amp; -0.0132 \\pm 1.96*0.0039 \\\\ &amp; = &amp; -0.0132 \\pm 0.00764 \\\\ &amp; = &amp; (-0.0208, -0.0056) \\\\ 95\\% \\textrm{ CI for } e^{\\beta_2} &amp; = &amp; (e^{-0.0208}, e^{-0.0056}) \\\\ &amp; = &amp; (.979, .994) \\\\ 95\\% \\textrm{ CI for } e^{10*\\beta_2} &amp; = &amp; (e^{-0.208}, e^{-0.056}) \\\\ &amp; = &amp; (.812, .946) \\end{eqnarray}\\] Thus, we can be 95% confident that every 10% increase in the proportion of black residents is associated with between a 5.4% and 18.8% decrease in the odds of a Yes vote for the railroad referendum. This same relationship could be expressed as between a 0.6% and a 2.1% decrease in odds for each 1% increase in the black population, or between a 5.7% (\\(1/e^{-.056}\\)) and a 23.1% (\\(1/e^{-.208}\\)) increase in odds for each 10% derease in the black population, after adjusting for distance. Of course, with \\(n=11\\), we should be cautious about relying on a Wald-type interval in this example. Another approach available in many software package in the profile likelihood method. In fact, this approach is generally preferable (and it’s what is automatically performed using confint() in R). When there are multiple parameters to be estimated (like \\(\\beta_0, \\beta_1, \\beta_2\\)), the profile likelihood will maximize the likelihood with respect to one parameter after first fixing that parameter to find estimates of the others. The profile likelihood is based on an asymptotic \\(\\chi^2\\) distribution of the log likelihood ratio test; one implication is that this approach can handle cases when the likelihood is not symmetric around the maximum likelihood estimate. In the model with distance and pctBlack, the profile likelihood 95% confidence interval for \\(e^{\\beta_2}\\) is (.979, .994), which (to 3 decimal places) is exactly equal to the Wald-based interval despite the small sample size. We can also confirm the statistically significant association between percent black and odds of voting Yes (after controlling for distance) since 1 is not a plausible value of \\(e^{\\beta_2}\\) (and an odds ratio of 1 implies that the odds of voting Yes does not change with percent black). 6.6.5.4 Testing for goodness of fit As in Section 4.7.10, we can evaluate the goodness of fit for our model by comparing the residual deviance to a \\(\\chi^2\\) distribution with \\(n-p\\) degrees of freedom. The model with pctBlack and distance has statistically significant evidence of lack of fit (p&lt;.001). 1-pchisq(model.HaleBD$deviance, model.HaleBD$df.residual) # GOF test [1] 0 As is Poisson regression models, this lack of fit could result from (a) missing covariates, (b) outliers, or (c) overdispersion. We will first attempt to address (a) by fitting a model with an interaction between distance and percent black to determine whether the effect of racial composition differs depending on how far a community is from the proposed railroad. glm(formula = cbind(YesVotes, NumVotes - YesVotes) ~ distance + pctBlack + distance:pctBlack, family = binomial, data = rrHale.df) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 7.5509017 0.6383697 11.828 &lt; 2e-16 *** distance -0.6140052 0.0573808 -10.701 &lt; 2e-16 *** pctBlack -0.0647308 0.0091723 -7.057 1.70e-12 *** distance:pctBlack 0.0053665 0.0008984 5.974 2.32e-09 *** --- Null deviance: 988.45 on 10 degrees of freedom Residual deviance: 274.23 on 7 degrees of freedom AIC: 320.53 exp(coef(model.HaleBxD)) (Intercept) distance pctBlack distance:pctBlack 1902.4574769 0.5411790 0.9373197 1.0053810 exp(confint(model.HaleBxD)) 2.5 % 97.5 % (Intercept) 544.4758958 6692.7238802 distance 0.4831871 0.6053654 pctBlack 0.9207984 0.9545967 distance:pctBlack 1.0035958 1.0071444 anova(model.HaleBD, model.HaleBxD, test = &quot;Chisq&quot;) Analysis of Deviance Table Model 1: cbind(YesVotes, NumVotes - YesVotes) ~ distance + pctBlack Model 2: cbind(YesVotes, NumVotes - YesVotes) ~ distance + pctBlack + distance:pctBlack Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) 1 8 307.22 2 7 274.23 1 32.984 9.294e-09 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We have statistically significant evidence (Wald test: \\(Z = 5.974, p&lt;.001\\); Drop in deviance test: \\(\\chi^2=32.984, p&lt;.001\\)) that the effect of the proportion of the community that is black on the odds of voting Yes depends on the distance of the community from the proposed railroad. To interpret the interaction coefficient in context, we can compare two cases, such as a community right on the proposed railroad (distance = 0) and one 10 miles away (distance = 15), since a significant interaction implies that the effect of pctBlack should differ in these two cases. In the first case, the coefficient for pctBlack is -0.0647, while in the second case, the relevant coefficient is -0.0647+15(.00537) = 0.0158. Thus, for a community right on the proposed railroad, a 1% increase in percent black is associated with a 6.3% (\\(e^{-.0647}=.937\\)) decrease in the odds of voting Yes, while for a community 15 miles away, a 1% (\\(e^{.0158}=1.016\\)) increase in percent black is associated with a 1.6% increase in the odds of voting Yes. A significant interaction term doesn’t always imply a change in the direction of the association, but it does here. Since our interaction model still exhibits significant lack of fit (residual deviance of 274.23 on just 7 df), and since we’ve used the covariates at our disposal, we will assess this model for potential outliers and overdispersion by examining its residuals. 6.6.5.5 Residuals for Binomial Regression With OLS, residuals were used to assess model assumptions and identify outliers. For binomial regression, two different types of residuals are typically used. One residual, the Pearson residual, has a form similar to that used with OLS. Specifically, the Pearson residual is calculated using: \\[\\begin{equation} \\textrm{Pearson residual}_i = \\frac{\\textrm{actual count}-\\textrm{predicted count}}{\\textrm{SD of count}} = \\frac{Y_i-m_i\\hat{p_i}}{\\sqrt{m_i\\hat{p_i}(1-\\hat{p_i})}} \\tag{6.11} \\end{equation}\\] where \\(m_i\\) is the number of trials for the \\(i^{th}\\) observation and \\(\\hat{p}_i\\) is the estimated probability of success for that same observation. A deviance residual is an alternative residual for binomial regression based on the discrepancy between the observed values and those estimated using the likelihood. A deviance residual can be calculated for each observation using: \\[\\begin{equation} \\textrm{deviance residual}_i = d_i = sign(Y_i-m_i\\hat{p_i})\\sqrt{2[Y_i log(\\frac{Y_i}{m_i \\hat{p_i}})+ (m_i - Y_i) log(\\frac{m_i - Y-i}{m_i - m_i \\hat{p_i}})]} \\tag{6.12} \\end{equation}\\] When the number of trials is large for all of the observations and the models are appropriate, both sets of residuals should follow a standard normal distribution. The sum of the individual deviance residuals is referred to as the deviance or residual deviance. The deviance is used to assess the model. As the name suggests, a model with a small deviance is preferred. In the case of binomial regression, when the denominators, \\(m_i\\), are large and a model fits, the residual deviance follows a \\(\\chi^2\\) with \\(n-p\\) degrees of freedom (the residual degrees of freedom). Thus for a good fitting model the residual deviance should be approximately equal to its corresponding degrees of freedom. When binomial data meets these conditions, the deviance can be used for a goodness-of-fit test. The p-value for lack-of-fit is the proportion of values from a \\(\\chi_{n-p}^2\\) that are greater than the observed residual deviance. We begin a residual analysis of our interaction model by plotting the residuals against the fitted values in Figure 6.4. This kind of plot for binomial regression would produce two linear trends with slope -1 with equal sample sizes \\(m_i\\) for each observation. From this residual plot, Greensboro does not stand out as an outlier. If it did, we could remove Greensboro and refit our interaction model, checking to see if model coefficients changed in a noticeable way. Instead, we will continue to include Greensboro in our modeling efforts Figure 6.4: Fitted values by residuals for the interaction model for the Railroad Referendum data. Source: Bayer and Fitzgerald. Because the large residual deviance cannot be explained by outliers, and given we have included all of the covariates at hand as well as an interaction term, the observed binomial counts are overdispersed, exhibiting more variation than the model would suggest, and we must consider ways to handle this overdispersion. 6.6.5.6 Overdispersion Community members vote Yes (1) or No (0) on the railroad referendum. Votes are summed in each community and recorded as the number of Yes votes. The number of Yes votes is a binomial variable if the community members’ votes are independent of one another. However, there is some concern that individual votes in a community are not independent of one another. For example, when there exist like-minded clusters within a community it may lead to overdispersion, a situation where the variation is actually greater than the binomial model might predict. If overdispersion is suspected, it is best to take it into account. Some statisticians would recommend that you always assume and correct for overdispersion, but there are differences of opinion on that point. Recall that the binomial model implies that the variance of a binomial variable is \\(np(1-p)\\). With overdispersion there is extra-binomial variation, so the actual variance will be greater than this. One way to adjust for overdispersion is to estimate a multiplier (dispersion parameter), \\(\\hat{\\phi}\\), for the variance that will inflate it and reflect the reduction in the amount of information we would otherwise have with independent observations. We used a similar approach to adjust for overdispersion in a Poisson regression model in Section 4.12, and we will use the same estimate here: \\(\\hat\\phi=\\frac{\\sum(\\textrm{Pearson residuals})^2}{n-p}\\). When overdispersion is adjusted for in this way, we can no longer use maximum likelihood to fit our regression model; instead we use a quasi-likelihood approach. Quasi-likelihood is similar in spirit to likelihood-based inference, but because the model uses the dispersion parameter it is not a binomial model with a true likelihood. Most statistical packages offer quasi-likelihood as an option when model fitting. The quasi-likelihood approach will yield the same coefficient point estimates as maximum likelihood; the variances, however, will be larger in the presence of overdispersion (assuming \\(\\phi&gt;1\\)). We will see other ways in which to deal with overdispersion and clusters in the remaining chapters in the book, but the following describes how overdispersion is accounted for using \\(\\hat{\\phi}\\): Summary: Accounting for Overdispersion Use the dispersion parameter \\(\\hat\\phi=\\frac{\\sum(\\textrm{Pearson residuals})^2}{n-p}\\) to inflate standard errors of model coefficients Wald test statistics: multiply the standard errors by \\(\\sqrt{\\hat{\\phi}}\\) so that \\(SE_Q(\\hat\\beta)=\\sqrt{\\hat\\phi}*SE(\\hat\\beta)\\) and conduct tests using the t-distribution CIs which use the adjusted standard errors and are thereby wider \\(\\hat\\beta \\pm t_{n-p}*SE_Q(\\hat\\beta)\\) Drop-in-deviance test statistic comparing Model 1 (larger model with \\(p\\) parameters) to Model 2 (smaller model with \\(q&lt;p\\) parameters) = \\[\\begin{equation} F=\\frac{1}{\\hat\\phi} \\frac{D_2 - D_1}{p-q} \\tag{4.8} \\end{equation}\\] where \\(D_1\\) and \\(D_2\\) are the residual deviances for models 1 and 2 respectively and \\(p-q\\) is the difference in the number of parameters for the two models. Note that both \\(D_2-D_1\\) and \\(p-q\\) are positive. This test statistic is compared to an F-distribution with \\(p-q\\) and \\((n-p)\\) degrees of freedom. Output for a model that adjusts our interaction model for overdispersion appears below, where \\(\\hat{\\phi}=51.6\\) is used to adjust the standard errors for the coefficients and the drop-in-deviance tests during model building. Standard errors, for example, will be inflated by a factor of \\(\\sqrt{51.6}=7.2\\). As a results, there are no significant terms in the adjusted interaction model below. glm(formula = cbind(YesVotes, NumVotes - YesVotes) ~ distance + pctBlack + distance:pctBlack, family = quasibinomial, data = rrHale.df) Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 7.550902 4.585464 1.647 0.144 distance -0.614005 0.412171 -1.490 0.180 pctBlack -0.064731 0.065885 -0.982 0.359 distance:pctBlack 0.005367 0.006453 0.832 0.433 (Dispersion parameter for quasibinomial family taken to be 51.5967) Null deviance: 988.45 on 10 degrees of freedom Residual deviance: 274.23 on 7 degrees of freedom Therefore, we remove the interaction term and refit the model, adjusting for the extra-binomial variation that still exists. glm(formula = cbind(YesVotes, NumVotes - YesVotes) ~ distance + pctBlack, family = quasibinomial, data = rrHale.df) Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.22202 1.99031 2.121 0.0667 . distance -0.29173 0.08780 -3.323 0.0105 * pctBlack -0.01323 0.02612 -0.506 0.6262 --- (Dispersion parameter for quasibinomial family taken to be 44.9194) Null deviance: 988.45 on 10 degrees of freedom Residual deviance: 307.22 on 8 degrees of freedom exp(coef(model.HaleBDq)) (Intercept) distance pctBlack 68.1711286 0.7469668 0.9868600 exp(confint(model.HaleBDq)) 2.5 % 97.5 % (Intercept) 1.3608623 5006.7224182 distance 0.6091007 0.8710322 pctBlack 0.9365625 1.0437861 In our latest model, we see that distance is significantly associated with support, but percent black is no longer significant after adjusting for distance. Because quasi-likelihood methods do not change estimated coefficients, we still estimate a 25% decline (\\(1-e^{-0.292}\\)) in support for each additional mile from the proposed railroad (oddds ratio of .75). But while we previously found a 95% confidence interval for the odds ratio of (.728, .766), our confidence interval is now much wider: (.609, .871). Appropriately accounting for overdispersion has changed both the significance of certain terms and the precision of our coefficient estimates. 6.6.6 Least Squares Regression vs. Logistic Regression \\[ \\underline{\\textrm{Response}} \\\\ \\mathbf{OLS:}\\textrm{normal} \\\\ \\mathbf{Binomial Regression:}\\textrm{ number of successes in n trials} \\\\ \\textrm{ } \\\\ \\underline{\\textrm{Variance}} \\\\ \\mathbf{OLS:}\\textrm{ Equal for each level of X} \\\\ \\mathbf{Binomial Regression:}np(1-p)\\textrm{ for each level of X} \\\\ \\textrm{ } \\\\ \\underline{\\textrm{Model Fitting}} \\\\ \\mathbf{OLS:}\\mu=\\beta_0+\\beta_1x \\textrm{ using Least Squares}\\\\ \\mathbf{Binomial Regression:}log(\\frac{p}{1-p})=\\beta_0+\\beta_1x \\textrm{ using Maximum Likelihood}\\\\ \\textrm{ } \\\\ \\underline{\\textrm{EDA}} \\\\ \\mathbf{OLS:}\\textrm{ plot X vs. Y; add line} \\\\ \\mathbf{Binomial Regression:}\\textrm{ find }log(odds)\\textrm{ for several subgroups; plot vs. X} \\\\ \\textrm{ } \\\\ \\underline{\\textrm{Comparing Models}} \\\\ \\mathbf{OLS:}\\textrm{ extra sum of squares F-tests; AIC/BIC} \\\\ \\mathbf{Binomial Regression:}\\textrm{ Drop in Deviance tests; AIC/BIC} \\\\ \\textrm{ } \\\\ \\underline{\\textrm{Interpreting Coefficients}} \\\\ \\mathbf{OLS:}\\beta_1=\\textrm{ change in }\\mu_y\\textrm{ for unit change in X} \\\\ \\mathbf{Binomial Regression:}e^{\\beta_1}=\\textrm{ percent change in odds for unit change in X} \\] 6.7 Case Study: Who wants to lose weight? Sex, Media, Sports, and BMI. The final case study uses individual-specific information so that our response, rather than the number of successes out of some number of trials, is simply a binary variable taking on values of 0 or 1 (for failure/success, no/yes, etc.). This type of problem—binary logistic regression—is exceedingly common in practice. 6.7.1 Background What are the characteristics of young people who are trying to lose weight? The prevalence of obesity among US youth suggests that wanting to lose weight is sensible and desirable for some young people such as those with a high body mass index (BMI). On the flip side, there are young people who do not not need to lose weight but make ill-advised attempts to do so nonetheless. A multitude of studies on weight loss focus specifically on youth and propose a variety of motivations for the young wanting to lose weight; athletics and the media are two commonly cited sources of motivation for losing weight for young people. Sports have been implicated as a reason for young people wanting to shed pounds, but not all studies are consistent with this idea. For example, a study by Martinsen et al. reported that, despite preconceptions to the contrary, there was a higher rate of self-reported eating disorders among controls (non-elite athletes) as opposed to elite athletes [@Martinsen2009]. Interestingly, the kind of sport was not found to be a factor, as participants in leanness sports (for example, distance running and swimming, gymnastics, dance, and diving) did not differ in the proportion with eating disorders when compared to those in non-leanness sports. So, in our analysis, we will not make a distinction between different sports. Other studies suggest that mass media is the culprit. They argue that students’ exposure to unrealistically thin celebrities may provide unhealthy motivation for some to try to slim down particularly young women. An examination and analysis of a large number of related studies (referred to as a meta-analysis) (Gradbe et al. 2008) found a strong relationship between exposure to mass media and the amount of time that adolescents spend talking about what they see in the media, deciphering what it means, and figuring out how they can be more like the celebrities. 6.7.2 Research Questions The following hypothesis are of interest. The odds that young females report trying to lose weight is greater that the odds that males do. Increasing BMI is associated with an interest in losing weight, regardless of sex. Sports participation is associated with the desire to lose weight, although this may differ between men and women. Media exposure is associated with more interest in losing weight, more so for females. 6.7.3 Data Collection We have sample of 500 teens from data collected in 2009 through the U.S. Youth Risk Behavior Surveillance System (YRBSS) [@YRBS2009]. The YRBSS is an annual national school-based survey conducted by the Center for Disease Control (CDC) and state, territorial, and local education and health agencies and tribal governments. The CDC states on their website that the “survey monitors six types of health-risk behaviors that contribute to leading causes of death and disability among youth and young adults,” including: Behaviors that contribute to unintentional injuries and violence Tobacco use Alcohol and other drug use Sexual risk behaviors Unhealthy dietary behaviors Physical inactivity More information on this survey can be found here. 6.7.4 Data Organization Here are the three questions from the YRBSS we use for our investigation: Q66. Which of the following are you trying to do about your weight? A. Lose weight B. Gain weight C. Stay the same weight D. I am not trying to do anything about my weight Q81. On an average school day, how many hours do you watch TV? A. I do not watch TV on an average school day B. Less than 1 hour per day C. 1 hour per day D. 2 hours per day E. 3 hours per day F. 4 hours per day G. 5 or more hours per day Q84. During the past 12 months, on how many sports teams did you play? (Include any teams run by your school or community groups.) A. 0 teams B. 1 team C. 2 teams D. 3 or more teams Answers to Q66 are used to define our response variable: Y = 1 corresponds to “(A) trying to lose weight”, while Y = 0 corresponds to the other non-missing values. Q84 provides information on students’ sports participation and is treated as numerical, 0 through 3, with 3 representing 3 or more. As a proxy for media exposure we use answers to Q81 as numerical values 0, 0.5, 1, 2, 3, 4, and 5, with 5 representing 5 or more. Media exposure and sports participation are also considered as categorical factors, that is, as variables with distinct levels which can be denoted by indicator variables as opposed to their numerical values. BMI is included in this study as the percentile for a given BMI for members of the same sex. This facilitates comparisons when modeling with males and females. We will use the terms BMI and BMI percentile interchangeably with the understanding that we are always referring to the percentile. With our sample, we use only the cases that include all of the data for these four questions. This is referred to as a complete case analysis. That brings our sample of 500 to 426. There are limitations of complete case analyses that we address in the Discussion. 6.7.5 Exploratory Data Analysis prop.table( tally(~ sex, data = risk2009) ) ## sex ## Female Male ## 0.4988345 0.5011655 prop.table( tally(~ sport, data = risk2009) ) ## sport ## No sports Sports ## 0.4335664 0.5664336 prop.table( tally(~ media, data = risk2009) ) ## media ## 0 0.5 1 2 3 4 ## 0.09324009 0.20046620 0.11888112 0.22843823 0.19114219 0.07459207 ## 5 ## 0.09324009 prop.table( tally(~ lose.wt, data = risk2009) ) ## lose.wt ## No weight loss Lose weight ## 0.5547786 0.4452214 Primary Outcome Nearly half (44.5%) of our sample of 429 youth report that they are trying to lose weight. Covariates 49.9% percent of the sample are females and 56.6% play on one or more sports teams. 9.3 percent report that they do not watch any TV on school days, whereas another 9.3% watched 5 or more hours each day. The relationship between the Primary Outcome and Covariates The most dramatic difference in the proportions of those who are trying to lose weight is by sex; 58% of the females want to lose weight in contrast to only 31% of the males (see Figure 6.5). This provides strong support for the inclusion of a sex term in every model considered. Figure 6.5: Weight loss plans vs. Sex ## lose.wt ## sex No weight loss Lose weight ## Female 0.4205607 0.5794393 ## Male 0.6883721 0.3116279 The next hypothesis addresses the question of whether those with higher BMI also tend to express attempting to lose weight regardless of sex. Table 6.3 displays the mean BMI of those wanting and not wanting to lose weight for males and females. The mean BMI is greater for those trying to lose weight compared to those not trying to lose weight, regardless of sex. The size of the difference is remarkably similar for the two sexes. Table 6.3: Mean BMI percentile by sex and desire to lose weight. Sex Weight loss status mean BMI percentile SD n Female No weight loss 48.2 26.2 90 Lose weight 76.3 19.5 124 Male No weight loss 54.9 28.6 148 Lose weight 84.4 17.7 67 If we consider including a BMI term in our model(s), the logit (log odds) should be linear related to BMI. We can investigate this assumption by constructing empirical logits. Because BMI is not discrete, we create BMI categories by dividing the BMI percentile variable into intervals separately for males and females. We choose intervals of equal sample size and then count up the number of respondents in each interval to determine the proportion, \\(\\hat{p}\\), that report that they want to lose weight. The empirical logit for each interval is \\(log(\\hat{p}/(1-\\hat{p}))\\). Figure 6.6 presents the empirical logits for the BMI intervals by sex. Both males and females exhibit an increasing linear trend on the logit scale indicating that increasing BMI is associated a greater desire to lose weight and that modeling log odds as a linear function of BMI is reasonable. The slope for the females appears to be slightly steeper than the males, but the difference in slope is likely not enough to consider an interaction term of BMI by sex in the model. Figure 6.6: Empirical logits for the odds of trying to lose weight by BMI deciles and Sex. Our next hypothesis involves sports participation. Of those who play sports, 43% want to lose weight whereas 46% want to lose weight among those who do not play sports. Figure 6.7 compares the proportion of respondents who want to lose weight by their sex and sport participation. The data suggest that sports participation is associated with the same or even a slightly lower desire to lose weight, contrary to what had originally been hypothesized. While the overall levels of those wanting to lose weight differ considerably between the sexes, the differences between those in and out of sports within sex appear to be very small. A term for sports participation or number of teams will be considered, but there is not compelling evidence that an interaction term will be needed. Figure 6.7: Weight loss plans vs. sex and sports participation ## , , lose.wt = No weight loss ## ## sport ## sex No sports Sports ## Female 0.4141414 0.4260870 ## Male 0.6781609 0.6953125 ## ## , , lose.wt = Lose weight ## ## sport ## sex No sports Sports ## Female 0.5858586 0.5739130 ## Male 0.3218391 0.3046875 It was posited that increased exposure to media, here measured as hours of TV daily, is associated with increased desire to lose weight, particularly for females. Overall, the percentage who want to lose weight ranges from 35% of those watching 5 hours of TV per day to 52% among those watching 3 hours daily. There is minimal variation in the proportion wanting to lose weight with both sexes combined. However, we are interested in differences between the sexes (see Figure 6.8). We create empirical logits using the proportion of students trying to lose weight for each level of hours spent watching daily and look at the trends in the logits separately for males and females. From Figure 6.9, there does not appear to be a linear relationship for males or females. Figure 6.8: Weight loss plans vs. daily hours of TV and sex. ## , , lose.wt = No weight loss ## ## media ## sex 0 0.5 1 2 3 4 ## Female 0.5000000 0.3800000 0.4074074 0.4444444 0.2972973 0.5454545 ## Male 0.5000000 0.8611111 0.7083333 0.6981132 0.6222222 0.5714286 ## media ## sex 5 ## Female 0.5416667 ## Male 0.8125000 ## ## , , lose.wt = Lose weight ## ## media ## sex 0 0.5 1 2 3 4 ## Female 0.5000000 0.6200000 0.5925926 0.5555556 0.7027027 0.4545455 ## Male 0.5000000 0.1388889 0.2916667 0.3018868 0.3777778 0.4285714 ## media ## sex 5 ## Female 0.4583333 ## Male 0.1875000 Figure 6.9: Empirical logits for the odds of trying to lose weight by sports particpation and sex. 6.7.6 Modeling Our strategy for modeling is to use our hypotheses of interest and what we have learned in the exploratory data analysis. For each model we interpret the coefficient of interest, look at the corresponding Wald test (estimated coefficient/s.e.) and, as a final step, compare the deviances for the different models we considered. 6.7.6.1 Model 1: sex The estimated coefficient in the model with sex (0=male, 1=female) as the only covariate is 1.1130 with a standard error of 0.2021 and a Wald statistic of 5.506, implying a very low p-value (3.67e-08). We can interpret the coefficient by exponentiating \\(e^{1.1130} = 3.04\\) indicating that the odds of a female trying to lose weight is over three times the odds of a male trying to lose weight (95% CI: 2.05, 4.54). We retain sex in the model and consider adding the BMI percentile. glm(formula = lose.wt.01 ~ female, family = binomial, data = risk2009) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -0.7925 0.1472 -5.382 7.36e-08 *** female 1.1130 0.2021 5.506 3.67e-08 *** --- Null deviance: 589.56 on 428 degrees of freedom Residual deviance: 558.01 on 427 degrees of freedom AIC: 562.01 exp(coef(model1)) (Intercept) female 0.4527027 3.0434494 exp(confint(model1)) 2.5 % 97.5 % (Intercept) 0.3372617 0.601313 female 2.0547758 4.541329 6.7.6.2 Model 2: sex + bmipct We have statistically significant evidence (\\(Z=9.067, p&lt;.001\\)) that BMI is positively associated with the odds of trying to lose weight, after controlling for sex. Clearly BMI percentile belongs in the model with sex. To interpret \\(\\beta_2\\) in \\(log(\\frac{p}{1-p})=\\beta_0+\\beta_1female+\\beta_2bmipct\\), we will consider a 10 unit increase in bmipct. Since \\(e^{10*0.0509}=1.664\\) (95% CI: 1.497, 1.867), there is an estimated 66.4% increase in the odds of wanting to lose weight for each additional 10 percentile points of BMI for members of the same sex. Just as we had done in other multiple regression models we need to interpret our coefficient given that the other variables remain constant. An interaction term for BMI by sex was not significant (\\(Z=-0.405, p=0.6856\\)), so the effect of BMI does not differ by sex. glm(formula = lose.wt.01 ~ female + bmipct, family = binomial, data = risk2009) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -4.458822 0.471411 -9.458 &lt; 2e-16 *** female 1.544968 0.249009 6.204 5.49e-10 *** bmipct 0.050915 0.005615 9.067 &lt; 2e-16 *** --- Null deviance: 589.56 on 428 degrees of freedom Residual deviance: 434.88 on 426 degrees of freedom AIC: 440.88 exp(10*coef(model2)[3]) bmipct 1.663883 exp(10*confint(model2)[3,]) 2.5 % 97.5 % 1.497421 1.866994 6.7.6.3 Model 3: sex + bmipct + sport Sports participation was considered for inclusion in the model in three ways. First, we tried an indicator of sports participation (0 = no teams, 1 = one or more teams), next the number of teams (0, 1, 2, or 3), and lastly, treating the number of teams as a factor. Sports teams were not significant in any of these models, nor were interaction terms (sex by sports) and (bmipct by sports). As a result, sports participation was no longer considered for inclusion in the model. 6.7.6.4 Model 4: sex + media + bmipct Because interest centers on how media may affect attempts to lose weight and how its effect might be different for females and males, we fit a model with a media term and a sex by media interaction term. Neither term was statistically significant, so we have no support in our data that media exposure as measured by hours spent watching TV is associated with the odds a teen is trying to lose weight. 6.7.6.5 Drop-in-deviance Tests ## Analysis of Deviance Table ## ## Model 1: lose.wt.01 ~ female ## Model 2: lose.wt.01 ~ female + bmipct ## Model 3: lose.wt.01 ~ female + bmipct + sport ## Model 4: lose.wt.01 ~ bmipct + female + media ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 427 558.01 ## 2 426 434.88 1 123.129 &lt;2e-16 *** ## 3 425 434.75 1 0.130 0.7188 ## 4 425 434.33 0 0.426 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## df AIC ## model1 2 562.0129 ## model2 3 440.8838 ## model3 4 442.7541 ## model4 4 442.3281 Comparing models using differences in deviances requires that the models be nested, meaning each smaller model is a simplified version of the larger model. In our case, Models 1, 2, and 4 are nested, as are Models 1, 2, and 3, but Models 3 and 4 cannot be compared using a drop-in-deviance test. There is a large drop-in-deviance adding BMI to the model with sex. (Model 1 to Model 2, 123.13) which is clearly statistically significant when compared to a \\(\\chi^2\\) distribution with 1 df. The drop-in-deviance for adding an indicator variable for sports to the model with sex and BMI is only 434.88 - 434.75 = 0.13. There is a difference of a single parameter, so the drop-in-deviance would be compared to a \\(\\chi^2\\) distribution with 1 df. The resulting \\(p\\)-value is very large (.7188) suggesting that adding an indicator for sports is not helpful once we’ve already accounted for BMI and sex. For comparing Models 3 and 4, one approach is to look at the Akaike Information Criteria (AIC). The AIC is defined using the deviances or deviations from the model along with a penalty for model complexity (for each additional predictor). Since less deviation and less complexity are both preferred, models with smaller AICs are preferred to models with larger AICs. Most statistics packages provide AICs. In this case, the AIC is (barely) smaller for the model with media, providing evidence that the latter model is slightly preferable. Note that a similar comparison can be made using Bayesian Information Criteria (BIC), which imposes a stronger penalty for model complexity. 6.7.7 Discussion We found that the odds of wanting to lose weight are considerably greater for females compared to males. In addition, respondents with greater BMI percentiles express a greater desire to lose weight for members of the same sex. Regardless of sex or BMI percentile, sports participation and TV watching are not associated with different odds for wanting to lose weight. A limitation of this analysis is that we used complete cases in place of a method of imputing responses or modeling missingness. This reduced our sample from 500 to 429, and it may have introduced bias. For example if respondents who watch a lot of TV were unwilling to reveal as much, and if they differed with respect to their desire to lose weight from those respondents who reported watching little TV, our inferences regarding the relationship between lots of TV and desire to lose weight may be biased. Other limitations may result from definitions. Trying to lose weight is self-reported and may not correlate with any action undertaken to do so. The number of sports teams may not accurately reflect sports related pressures to lose weight. For example, elite athletes may focus on a single sport and be subject to greater pressures whereas athletes who casually participate in three sports may not feel any pressure to lose weight. Hours spent watching TV is not likely to encompass the totality of media exposure, particularly because exposure to celebrities occurs often online. Furthermore, this analysis does not explore in any detail maladaptions—inappropriate motivations for wanting to lose weight. For example, we did not focus our study on subsets of respondents with low BMI who are attempting to lose weight. It would be instructive to use data science methodologies to explore the entire data set of 16,000 instead of sampling 500. However, the types of exploration and models used here could translate to the larger sample size. Finally a limitation may be introduced as a result of the acknowledged variation in the administration of the YRBS. States and local authorities are allowed to administer the survey as they see fit, which at times results in significant variation in sample selection and response. 6.7.8 Optional topics to be developed Predicted probabilities of trying to lose weight Classification 6.8 References 6.9 Exercises 6.9.1 Interpret article abstracts Interpret the odds ratios in the following abstract. Day Care Centers and Respiratory Health [@Nafstad1999] Objective. To estimate the effects of the type of day care on respiratory health in preschool children. Methods. A population-based cross-sectional study of Oslo children born in 1992 was conducted at the end of 1996. A self-administered questionnaire inquired about day care arrangements, environmental conditions, and family characteristics (n = 3853; response rate, 79%). Results. In a logistic regression controlling for confounding, children in day care centers had more often nightly cough (adjusted odds ratio, 1.89; 95%), and blocked or runny nose without common cold (1.55; 1.07, 1.61) during the past 12 months compared with children in home care. Construct a table and calculate the corresponding odds and odds ratios. Comment on the reported and calculated results. IVF and Birth Defects Data Source: CDC In November, the Centers for Disease Control and Prevention published a paper reporting that babies conceived with IVF, or with a technique in which sperm are injected directly into eggs, have a slightly increased risk of several birth defects, including a hole between the two chambers of the heart, a cleft lip or palate, an improperly developed esophagus and a malformed rectum. The study involved 9,584 babies with birth defects and 4,792 babies without. Among the mothers of babies without birth defects, 1.1 percent had used IVF or related methods, compared with 2.4 percent of mothers of babies with birth defects. The findings are considered preliminary, and researchers say they believe IVF does not carry excessive risks. There is a 3 percent chance that any given baby will have a birth defect. 6.9.2 Guided Exercises Soccer goals on target Data from an article in Psychological Science, July 2011. [@Masters2007] The example in the text referred to all shots, good or bad. This data relates to shots on target, 18 out of 20 shots were scored when the goalkeeper’s team was behind, 71 out of 90 shots were scored when the game was tied, and when the team was not behind 55 out of 75 shots were scored. Calculate the odds of scoring for games behind, games tied, and games not behind. Construct empirical odds ratio for scoring for behind versus tied and tied versus not behind. Fit a model with the categorical predictor c(“behind”,“tied”,“not behind”) and interpret the exponeniated coefficients. How do they compare to the empirical odds ratios you calculated? Would it be better to model this data using a negative binomial variable? Explain. Medical School Admissions The data for Medical School Admissions is in MedGPA.xls, taken from undergraduates from a small liberal arts school over several years. Compare the relative effects of improving your MCAT score versus improving your GPA. Are there different majors that are more likely to gain medical school admission, after controlling for MCAT and GPA? 6.9.3 Open-ended Exercises Presidential Voting in Minnesota counties Data is in MNVote.xls. Response is Obama or McCain county-wide victory for each of the 87 counties in Minnesota. Model the county-level Obama indicator as a function of the percent of households in poverty, percent of the county that is rural, unemployment rate. Interpret your coefficients. Crime on Campus Data in Campus Crime Data Set.csv. In this dataset, crimes are characterized as violent or property crimes. Of the total number of crimes, does the proportion of violent crimes differ by regions of the country? 6.9.4 Project Ideas Spam Perform a screen shot of an inbox. Identify which emails are spam. Have students come up with a model that predicts whether an email is spam or not. Try this model out on a new inbox. Trashball Great for a rainy day! A fun way to generate overdispersed binomial data. Each student crumbles an 8-1/2 by 11 inch sheet and tosses it from three prescribed distances ten times each. \\(Y_i=\\) number of baskets out of 10 tosses keeping track of the distance. GSS student generate research question(s) YRBS student generate research question(s) "],
["ch-corrdata.html", "Chapter 7 Correlated Data 7.1 Learning Objectives 7.2 Recognizing correlation 7.3 Case Study: Dams and pups, Correlated Binary Outcomes 7.4 Under Construction…", " Chapter 7 Correlated Data 7.1 Learning Objectives After finishing this chapter, you should be able to: Given a data structure, be able to recognize when there is a potential for correlation. Identify a primary sampling unit and corresponding observations for a study. Provide reasons why correlated observations may cause problems when modeling. Describe overdispersion and why it might occur. Understand how correlation in data can be taken into account using random effects models. Be able to describe the differences in the kind of inferences that can be made using fixed versus random effects models. Use software to fit a model taking correlation into account using: overdispersion parameter estimates (quasi-likelihood) or random effects. Introductory statistics courses typically require responses which are approximately normal and independent of one another. We saw from the first chapters in this book that there are models for non-normal responses, so we have already broadened the types of applications we can handle. In this chapter, we relax an additional assumption from introductory statistics, that of independence. Here we introduce methods that allow correlation to be taken into account. When modeling, correlation can be considered for normal and non-normal responses. Taken together, we can handle many more applications than the average introductory student. First, we focus on recognizing data structures that may imply correlation. Next, we consider potential problems correlated outcomes may cause and why we need to take correlation into account when modeling. Models which take correlation into account are then described and fit with the help of R. 7.2 Recognizing correlation Correlated data is encountered in nearly every field. In education, student scores from a particular teacher are more similar than scores of other students who have had a different teacher. During a study measuring depression indices weekly over the course of a month, we may find that four measures for the same patient tend to be more similar than depression indices from other patients. In political polling, opinions from members of the same household may be more similar than opinions of members from other randomly selected households. The structure of these data sets suggest inherent patterns of similarities or correlation among outcomes. This kind of correlation specifically concerns correlation of observations within the same teacher or patient or household and is referred to as intraclass correlation. To simplify our discussion, we borrow terminology from survey methodology. In our examples, teachers, patients, and households can be referred to as primary sampling units or PSUs. In sample surveys, primary sampling unit (commonly abbreviated as PSU) arises in samples in which population elements are grouped into aggregates and the aggregates become units in sample selection. The aggregates are, due to their intended usage, called “sampling units.” Primary sampling unit refers to sampling units that are selected in the first (primary) stage of a multi-stage sample ultimately aimed at selecting individual elements. — James M. Lepkowski, SAGE (2008) A PSU is sampled and multiple measurements or observations are made for each PSU; here test scores, depression indices, and political opinions are the measurements, respectively. Other examples appear in the table below. Look for this structure when analyzing data. Correlated data structures can also involve mixture models. For example, in Section ?? we saw the zero-inflated Poisson model, which is a mixture of binary and Poisson variables. Another common mixture is when a population has two subgroups described by normal distributions with potentially different means and standard deviations. We will explore methods for analyzing mixture models too. 7.3 Case Study: Dams and pups, Correlated Binary Outcomes A tertogen is a substance or exposure that can result in harm to a developing fetus. An experiment can be conducted to determine whether increasing levels of a potential teratogen results in increasing probability of defects in rat pups. For example, consider an experiment in which dams (pregnant rats) are exposed to one of two dose levels of a potential teratogen. A control group is not exposed. Dams produce litters and the number of pups with a defect is the outcome of interest. 7.3.1 Sources of Variability In order to analyze the entire data set, let’s step back and look at the big picture. Statistics is all about analyzing variability, so let’s consider what sources of variability we have in the dams and pups example. There are several reasons why the counts of the number of defective pups differ from dam to dam and it is helpful to explicitly identify what they are in order to determine how the dose levels affect the pups in addition to accommodating correlation. Dose Effect The dams and pups experiment is being carried out to determine whether different dose levels affect the development of defects differently. Of particular interest is determining whether a dose-response effect is present. A dose-response effect is evident when dams receiving higher dose levels produce higher proportions of pups with defects. Knowing defect rates at specific dose levels is typically of interest within this experiment and beyond. Publishing the defect rates for each dose level in a paper in a journal, for example, would be of interest to other teratologists. For that reason, we refer to the dose level effects as fixed effects. Dams (Litter) Effect In many settings like this, there is a litter effect as well. For example, some dams may exhibit a propensity to produce pups with defects while others rarely produce litters with defective pups. That is, observations on pups within the same litter are likely to be similar or correlated. Unlike the dose effect, teratologists reading experiment results are not interested in the estimated probability of defect for each dam in the study, and we would not report these estimated probabilities in a paper. However, there may be interest in the variability in litter-specific defect probabilities. Often this kind of effect is modeled using the idea that randomly selected dams produce random effects. This provides one way in which to model correlated data. We elaborate on this idea throughout the remainder of the text after this next section on overdispersion. Pup-to-pup variability The within litter pup-to-pup differences reflect random, unexplained variation in the model. 7.3.2 Analyzing a control group We begin by focusing on a control group, ignoring the dose group data. The control group for this study consists of four dams who each produce 15 rat pups in their litter. Each pup is examined for the presence or absence of a defect. (Dams typically produce 8 to 15 pups, so it is unlikely that every dam produces 15 pups. Here we keep it simple with fifteen pups per litter.) Before proceeding any further with the analysis, take note of the data structure. We can think of this as a repeated measures study where dams are PSUs and the elements are the binary outcomes of defect or no defect for each pup. The total number of pups with a defect for a single dam is the response, which is binomial with n=15 and probability of defect \\(p\\). This structure alerts us to the possibility that pup outcomes may be correlated within litters; thus, we may not have 60 independent observations, and we’ll want to take that into account when modeling. Each dam \\(j\\) produces a litter of 15 pups (j=1, \\(\\ldots\\), 4). Individual pup outcomes are either 0 or 1 for “no defect” or “defect” and are denoted as \\(Y_{jk}\\) for the \\(k^{th}\\) pup from the \\(j^{th}\\) dam, \\(k=1, \\ldots, 15\\) and \\(j=1, \\ldots, 4\\). The binomial counts, \\(Y_{j.}=\\sum_{k=1}^{15}Y_{jk}\\), the total number of defects for the \\(j^{th}\\) litter, are recorded for each of the four litters. If dams do not differ in their propensity to produce defects, these counts follow a binomial distribution with n=15 and a common probability \\(p\\). However, if dams’ probabilities of producing a pup with a defect differ from dam-to-dam, the number of pups with defects will not be coming from the same binomial distribution, and the counts are likely to differ more than they would with a common \\(p\\). For illustration, we generate a (hypothetical) control group data set based on the notion that different dams are producing defective pups with different probabilities. Our simulation assumes that \\(p_1\\), the true probability that dam 1 produces a pup with a defect, is 0.40. She produces 15 pups and has 8 pups with defects. Dam 2 has a true probability of \\(p_2=0.3\\) for the probability of a defective pup, and among her 15 pups only 3 have defects. Dam 3 has a very unhealthy lifestyle and as a result produces pups with defects about 50% of the time. For her litter of 15, she has 7 pups with defects. Only 20% of the pups from the healthiest dam have defects and this litter has 3 pups with defects. We have made the variation in \\(p\\) dramatic for illustration. Overall the estimated probability of a defect in the control group is (8+3+7+3)/(4x15) = 0.35. This would be a good estimate if all four dams had the same probability of producing a defective pup. If we proceeded with this naive assumption, recall that the variance of a binomial random variable is \\(np(1-p)\\), so we would expect the variation in the number of defective pups to be approximately \\(15*0.35*(1-.35)=3.41\\). However, we calculate the variance for our four litters (\\(s^2=\\sum_{j=1}^{4}(Y_j-\\bar{Y})^2/(4-1)\\)) and find 6.92. Comparing the model-based variance \\(np(1-p)\\) to the observed variance (\\(s^2\\)), we find a lot more variation in the number of defects among the control group litters than we would expect if in fact they were all coming from the same binomial distribution. If we hadn’t generated the data, making this comparison would lead us to suspect that there is a dam effect and the observations within litters are correlated. In the case of binomial data, this is referred to as extra-binomial variation and more generally as overdispersion. We found that overdispersion can be a clue that observations are correlated within PSUs. Why should we worry about it? If there is correlation within litters, there are not 60 independent pieces of information; that is, the effective sample size is less than 60. Standard errors are based on the reciprocal of the sample size, so standard errors for the naive model will be artificially small, resulting in more significant results and narrower confidence intervals than there would be with the larger standard errors based on the effective sample size. Overdispersion suggests that our naive binomial model is incorrect and may well result in significant lack-of-fit. Luckily there are ways in which we can modify the naive model to improve this situation. Had we been attentive to the data structure, we would not be surprised to find overdispersion. How can we take this correlation into account? The model-based variance of the counts for the four litters is artificially small (3.41 instead of 6.92). Using a model-based variance to calculate standard errors for Wald tests and confidence intervals will lead to too many significant results or confidence intervals which are too narrow. One way to correct for overdispersion is to inflate the model-based variance by using a factor (an estimated dispersion parameter often denoted \\(\\hat{\\phi}\\)) so the model uses \\(\\phi np(1-p)\\) as the variance instead of \\(np(1-p)\\), where \\(\\phi &gt;\\) 1. We will demonstrate this using software with an expanded data set in a subsequent section. 7.4 Under Construction… The remainder of the chapter is currently being rewritten. It will eventually include: a simulation involving dams and pups to illustrate the effects of failing to account for correlated data a case study involving real data from a tree growth experiment a preview of multilevel models and random effects a set of exercises In the meantime, here is a set of questions and associated R script that addresses the first bullet point above: 7.4.1 Correlated Data Simulation Dams and Pups. 24 dams (mother rats) are randomized to four groups; 3 groups of 6 dams each are administered a potential teratogen (an agent that can disturb the development of an embryo or fetus) in high, medium, or low doses, and the fourth group serves as the control. Each of the 24 dams produces 10 rat pups, and each pup is examined for the presence of an anomaly or deformities. We will be simulating data under various assumptions and analyzing the results. Relevant R code can be found below the list of questions, along with a template to summarize key results. Do we have \\(24*10=240\\) independent pieces of data? The number of deformed pups will differ by dam. What may be reasons behind these differences? We will use the Beta distribution to model probabilities of deformed pups for each dam. How do the possible values depend on \\(a\\) and \\(b\\)? How might you decide on values for \\(a\\) and \\(b\\) if you have run a preliminary experiment and gathered data on the number of dams with deformed pups? We will simulate counts of deformed pups for each dam under various scenarios described in the R script. First, we will consider Scenario 1, where the probability that an individual dam has a deformed pup does not depend on dose. How do differences between Scenarios 1a and 1b appear in the simulated count data (and simulated probabilities)? Describe how the quasibinomial analysis of Scenario 1b differs from the binomial analysis of the same simulated data. Why are differences between quasibinomial and binomial analyses of Scenario 1a less noticeable? Next, we will consider Scenario 2, where the probability that an individual dam has a deformed pup does depend on dose. How do differences between Scenarios 2a and 2b appear in the simulated count data (and simulated probabilities)? Describe how the quasibinomial analysis of Scenario 2b differs from the binomial analysis of the same simulated data. Why are differences between quasibinomial and binomial analyses of Scenario 2a less noticeable? For the logistic regression model “lreg1”, we created a data set with 1 observation per pup, using the responses generated under Scenario 2b. Describe how this logistic regression model compares with your binomial and quasibinomial regression models of this data in (5). In model1, the data set from (6) is analyzed using multilevel modeling. We will talk much more about this later, but a sketch of this multilevel model (with a random effect for dam) is given below. What conclusions do you seem to be able to draw from the multilevel model, and how do those conclusions compare to conclusions from binomial regression modeling? Level 1: \\[ log(\\frac{p_{ij}}{1-p_{ij}})=a_i \\] where \\(i\\) = dam and \\(j\\) = pup Level 2: \\[ a_i=\\alpha_0+\\alpha_1\\textrm{dose}_i+u_i \\] where \\(u_i \\sim N(0,\\sigma_u^2)\\) Composite: \\[ log(\\frac{p_{ij}}{1-p_{ij}})=\\alpha_0+\\alpha_1\\textrm{dose}_i+u_i \\] Template for organizing simulation results # CorrData DamsPups.R ## Explore beta distribution for modeling probabilities of deformed pups ### # Beta random variables take on values from 0 to 1. p=seq(0,1,length=1000) # To plot a beta density use dbeta. Here I selected a=5, b=1: density=dbeta(p,5,1) plot(p,density,type=&quot;l&quot;) # Try some other values for a&gt;0 and b&gt;0 # to see other forms for the Beta distribution. # Try to answer the following questions: # What values do Beta random variables take on? # What do these values represent for the dams and pups simulation? # Do the possible values depend on a or b? # What is a feature of the Beta density when a=b? # What happens to the density when a is not equal to b? # How does the magnitude of a or b affect the density? # How does the difference between a and b affect the density? # If you wanted to simulate dams with mostly low probabilities of # defects and a few with very high probabilities, how would you do it? # If you wanted to simulate dams with mostly high probabilities of # defects and a few with very low probabilities, how would you do it? # If you wanted to simulate a population of dams where half of the # probabilities of defects are very high and half are very low, # how would you do it? # How might you decide on values for a and b if you have run a # preliminary experiment and gathered data on the number of dams # with deformed pups? # model for generating pis p=seq(0,1,length=1000) density=dbeta(p,.5,.5) plot(p,density,type=&quot;l&quot;) # or just use: hist(rbeta(10000,.5,.5)) ### Beginning of simulation ### ## Scenario 1: log (pi / 1 - pi) = 0 ## a) pi fixed at 0.5 for all 24 dams ## b) pi randomly chosen from Beta (0.5, 0.5) which has mean 0.5 # Generate pi for each of 24 dams in sample under Scenarios 1a and 1b set.seed(530) pi_1a &lt;- rep(0.5, 24) pi_1b &lt;- rbeta(24,.5,.5) # generate deformed pups for each of 24 dams under Scenario 1a count_1a &lt;- rbinom(24, 10, pi_1a) # generate deformed pups for each of 24 dams under Scenario 1b count_1b &lt;- rbinom(24, 10, pi_1b) scenario_1 &lt;- data.frame(pi_1a, count_1a, pi_1b, count_1b) # Compare histograms of counts under both scenarios library(ggplot2) library(gridExtra) hist_1a &lt;- ggplot(data = scenario_1, aes(x = count_1a)) + geom_histogram(bins = 5) + coord_cartesian(xlim = c(0, 10)) hist_1b &lt;- ggplot(data = scenario_1, aes(x = count_1b)) + geom_histogram(bins = 5) + coord_cartesian(xlim = c(0, 10)) grid.arrange(hist_1a, hist_1b, ncol=1) # compare mean and variance for two scenarios library(dplyr) scenario_1 %&gt;% summarise(mean_1a = mean(count_1a), sd_1a = sd(count_1a), mean_1b = mean(count_1b), sd_1b = sd(count_1b) ) # Model Scenario 1a data without overdispersion scenario_1 &lt;- scenario_1 %&gt;% mutate(phat_1a = count_1a / 10, phat_1b = count_1b / 10) fit_1a_binom &lt;- glm(phat_1a ~ 1, family=binomial, weight=rep(10,24), data = scenario_1) summary(fit_1a_binom) exp(coef(fit_1a_binom)) # estimated odds of deformity exp(coef(fit_1a_binom)) / (1+exp(coef(fit_1a_binom))) # estimated prob of deformity mlcoef = summary(fit_1a_binom)$coefficients[,1] mlse = summary(fit_1a_binom)$coefficients[,2] lb = mlcoef - qnorm(.975,0,1)*mlse ub = mlcoef + qnorm(.975,0,1)*mlse cbind(exp(lb),exp(ub)) # CI for odds - normal-based cbind( exp(lb)/(1+exp(lb)) , exp(ub)/(1+exp(ub)) ) # CI for prob - normal-based ci.prof.noadj=exp(confint(fit_1a_binom)) ci.prof.noadj # CI for odds - profile likelihood ci.prof.noadj / (1+ci.prof.noadj) # CI for prob - profile likelihood gof &lt;- 1-pchisq(fit_1a_binom$deviance, fit_1a_binom$df.residual) gof # test for goodness of fit # Model Scenario 1a data with overdispersion fit_1a_quasi = glm(phat_1a ~ 1, family=quasibinomial, weight=rep(10,24), data=scenario_1) summary(fit_1a_quasi) exp(coef(fit_1a_quasi)) # estimated odds of deformity exp(coef(fit_1a_quasi)) / (1+exp(coef(fit_1a_quasi))) # estimated prob of deformity phihat = sum(residuals(fit_1a_quasi, type=&quot;pearson&quot;)^2) / fit_1a_quasi$df.residual phihat # overdispersion parameter qlcoef = summary(fit_1a_quasi)$coefficients[,1] qlse = summary(fit_1a_quasi)$coefficients[,2] lb = qlcoef - qt(.975,fit_1a_quasi$df.residual)*qlse ub = qlcoef + qt(.975,fit_1a_quasi$df.residual)*qlse cbind(exp(lb),exp(ub)) # CI for odds - t-based cbind( exp(lb)/(1+exp(lb)) , exp(ub)/(1+exp(ub)) ) # CI for prob - t-based ci.prof.adj=exp(confint(fit_1a_quasi)) ci.prof.adj # CI for odds - profile likelihood ci.prof.adj / (1+ci.prof.adj) # CI for prob - profile likelihood # Model Scenario 1b data without overdispersion fit_1b_binom &lt;- glm(phat_1b ~ 1, family=binomial, weight=rep(10,24), data = scenario_1) summary(fit_1b_binom) exp(coef(fit_1b_binom)) # estimated odds of deformity exp(coef(fit_1b_binom)) / (1+exp(coef(fit_1b_binom))) # estimated prob of deformity mlcoef = summary(fit_1b_binom)$coefficients[,1] mlse = summary(fit_1b_binom)$coefficients[,2] lb = mlcoef - qnorm(.975,0,1)*mlse ub = mlcoef + qnorm(.975,0,1)*mlse cbind(exp(lb),exp(ub)) # CI for odds - normal-based cbind( exp(lb)/(1+exp(lb)) , exp(ub)/(1+exp(ub)) ) # CI for prob - normal-based ci.prof.noadj=exp(confint(fit_1b_binom)) ci.prof.noadj # CI for odds - profile likelihood ci.prof.noadj / (1+ci.prof.noadj) # CI for prob - profile likelihood gof &lt;- 1-pchisq(fit_1b_binom$deviance, fit_1b_binom$df.residual) gof # test for goodness of fit # Model Scenario 1b data with overdispersion fit_1b_quasi = glm(phat_1b ~ 1, family=quasibinomial, weight=rep(10,24), data=scenario_1) summary(fit_1b_quasi) summary(fit_1b_quasi) exp(coef(fit_1b_quasi)) # estimated odds of deformity exp(coef(fit_1b_quasi)) / (1+exp(coef(fit_1b_quasi))) # estimated prob of deformity phihat = sum(residuals(fit_1b_quasi, type=&quot;pearson&quot;)^2) / fit_1b_quasi$df.residual phihat # overdispersion parameter qlcoef = summary(fit_1b_quasi)$coefficients[,1] qlse = summary(fit_1b_quasi)$coefficients[,2] lb = qlcoef - qt(.975,fit_1b_quasi$df.residual)*qlse ub = qlcoef + qt(.975,fit_1b_quasi$df.residual)*qlse cbind(exp(lb),exp(ub)) # CI for odds - t-based cbind( exp(lb)/(1+exp(lb)) , exp(ub)/(1+exp(ub)) ) # CI for prob - t-based ci.prof.adj=exp(confint(fit_1b_quasi)) ci.prof.adj # CI for odds - profile likelihood ci.prof.adj / (1+ci.prof.adj) # CI for prob - profile likelihood ## Scenario 2: log (pi / 1 - pi) = -2 + 4 * dose ## Thus pi=exp(-2+4*dose)/(1+exp(-2+4*dose)) ## Log odds linearly related to 4 doses: 0, 1/3, 2/3, 1 ## a) pi fixed at .119, .339, .661, .881 for groups of 6 dams ## b) pi randomly chosen from Beta (2*pi / (1-pi), 2) which has mean pi # Generate pi for each of 24 dams in sample under Scenarios 1a and 1b dose &lt;- c(rep(0,6),rep(1/3,6),rep(2/3,6),rep(1,6)) pi_2a &lt;- exp(-2+4*dose) / (1+exp(-2+4*dose)) b &lt;- 2 a &lt;- b*pi_2a / (1-pi_2a) pi_2b &lt;- rbeta(24, a, b) # generate deformed pups for each of 24 dams under Scenario 2a count_2a &lt;- rbinom(24, 10, pi_2a) # generate deformed pups for each of 24 dams under Scenario 2b count_2b &lt;- rbinom(24, 10, pi_2b) scenario_2 &lt;- data.frame(pi_2a, count_2a, pi_2b, count_2b) # Plot pis used in Scenario 2b hist1 &lt;- ggplot(data = scenario_2[1:6,], aes(x = pi_2b)) + geom_histogram(bins = 5) + coord_cartesian(xlim = c(0,1)) hist2 &lt;- ggplot(data = scenario_2[7:12,], aes(x = pi_2b)) + geom_histogram(bins = 5) + coord_cartesian(xlim = c(0,1)) hist3 &lt;- ggplot(data = scenario_2[13:18,], aes(x = pi_2b)) + geom_histogram(bins = 5) + coord_cartesian(xlim = c(0,1)) hist4 &lt;- ggplot(data = scenario_2[19:24,], aes(x = pi_2b)) + geom_histogram(bins = 5) + coord_cartesian(xlim = c(0,1)) grid.arrange(hist1, hist2, hist3, hist4, ncol=1) # Compare histograms of counts under both scenarios hist_2a &lt;- ggplot(data = scenario_2, aes(x = count_2a)) + geom_histogram(bins = 5) + coord_cartesian(xlim = c(0, 10)) hist_2b &lt;- ggplot(data = scenario_2, aes(x = count_2b)) + geom_histogram(bins = 5) + coord_cartesian(xlim = c(0, 10)) grid.arrange(hist_2a, hist_2b, ncol=1) # compare mean and variance for two scenarios scenario_2 %&gt;% group_by(pi_2a) %&gt;% summarise(mean_2a_pi = mean(pi_2a), sd_2a_pi = sd(pi_2a), mean_2b_pi = mean(pi_2b), sd_2b_pi = sd(pi_2b), mean_2a_cnt = mean(count_2a), sd_2a_cnt = sd(count_2a), mean_2b_cnt = mean(count_2b), sd_2b_cnt = sd(count_2b) ) %&gt;% as.data.frame() # elogit plots for model.binom and over.binom scenario_2 &lt;- scenario_2 %&gt;% mutate(dose = dose, phat_2a = count_2a / 10, phat_2b = count_2b / 10, logit_2a = log ((count_2a + 0.5) / (10 - count_2a + 0.5)), logit_2b = log ((count_2b + 0.5) / (10 - count_2b + 0.5)) ) elog1 &lt;- ggplot(data = scenario_2, aes(x = dose, y = logit_2a)) + geom_point() + geom_smooth(method=&quot;lm&quot;) + ggtitle(&quot;Scenario 2a&quot;) + labs(y = &quot;Logit of proportion of defective pups&quot;) elog2 &lt;- ggplot(data = scenario_2, aes(x = dose, y = logit_2b)) + geom_point() + geom_smooth(method=&quot;lm&quot;) + ggtitle(&quot;Scenario 2b&quot;) + labs(y = &quot;Logit of proportion of defective pups&quot;) grid.arrange(elog1, elog2, ncol=2) # Model Scenario 2a data without overdispersion fit_2a_binom = glm(phat_2a ~ dose, family=binomial, weight=rep(10,24), data=scenario_2) summary(fit_2a_binom) confint(fit_2a_binom) gof &lt;- 1-pchisq(fit_2a_binom$deviance, fit_2a_binom$df.residual) gof # Model Scenario 2a data with overdispersion fit_2a_quasi = glm(phat_2a ~ dose, family=quasibinomial, weight=rep(10,24), data=scenario_2) summary(fit_2a_quasi) confint(fit_2a_quasi) # Model Scenario 2b data without overdispersion fit_2b_binom = glm(phat_2b ~ dose, family=binomial, weight=rep(10,24), data=scenario_2) summary(fit_2b_binom) confint(fit_2b_binom) gof &lt;- 1-pchisq(fit_2b_binom$deviance, fit_2b_binom$df.residual) gof # Model Scenario 2b data with overdispersion fit_2b_quasi = glm(phat_2b ~ dose, family=quasibinomial, weight=rep(10,24), data=scenario_2) summary(fit_2b_quasi) confint(fit_2b_quasi) # Try running as logistic regression # First need to create data set with 1 obs per pup n=rep(10,24) dam_id=1:24 dose1 = rep(dose[1],n[1]) id1 = rep(dam_id[1],n[1]) def1 = c(rep(1,count_2b[1]),rep(0,n[1]-count_2b[1])) for(i in 2:24) { dose1 = c(dose1,rep(dose[i],n[i])) id1 = c(id1,rep(dam_id[i],n[i])) def1 = c(def1,rep(1,count_2b[i]),rep(0,n[i]-count_2b[i])) } cdplot(dose1,as.factor(def1)) # conditional density plot lreg1 = glm(def1 ~ dose1, family=binomial(link=&quot;logit&quot;) ) summary(lreg1) confint(lreg1) gof = 1-pchisq(lreg1$deviance, lreg1$df.residual) gof # Try running as a multilevel model library(lme4) model1 = glmer(def1 ~ dose1 + (1|id1), family=binomial) summary(model1) model0 = glmer(def1 ~ 1 + (1|id1), family=binomial) summary(model0) "],
["ch-multilevelintro.html", "Chapter 8 Introduction to Multilevel Models 8.1 Learning Objectives 8.2 Case Study: Music Performance Anxiety 8.3 Initial Exploratory Analyses 8.4 Two level modeling: preliminary considerations 8.5 Two level modeling: a unified approach 8.6 Building a multilevel model 8.7 Binary covariates at Level One and Level Two 8.8 Additional covariates: model comparison and interpretability 8.9 Center covariates 8.10 A potential final model for music performance anxiety 8.11 Modeling the multilevel structure: is it really necessary? 8.12 Notes on Using R (Optional) 8.13 Exercises", " Chapter 8 Introduction to Multilevel Models 8.1 Learning Objectives After finishing this chapter, you should be able to: Recognize when response variables and covariates have been collected at multiple (nested) levels. Apply exploratory data analysis techniques to multilevel data. Write out a multilevel statistical model, including assumptions about variance components, in both by-level and composite forms. Interpret model parameters (including fixed effects and variance components) from a multilevel model, including cases in which covariates are continuous, categorical, or centered. Understand the taxonomy of models, including why we start with an unconditional means model. Select a final model, using criteria such as AIC, BIC, and deviance. 8.2 Case Study: Music Performance Anxiety Stage fright can be a serious problem for performers, and understanding the personality underpinnings of performance anxiety is an important step in determining how to minimize its impact. [@Miller2010] studied the emotional state of musicians before performances and factors which may affect their emotional state. Data was collected by having 37 undergraduate music majors from a competitive undergraduate music program fill out diaries prior to performances over the course of an academic year. In particular, study participants completed a Positive Affect Negative Affect Schedule (PANAS) before each performance. The PANAS instrument provided two key outcome measures: negative affect (a state measure of anxiety) and positive affect (a state measure of happiness). We will focus on negative affect as our primary response measuring performance anxiety. Factors which were examined for their potential relationships with performance anxiety included: performance type (solo, large ensemble, or small ensemble); audience (instructor, public, students, or juried); if the piece was played from memory; age; gender; instrument (voice, orchestral, or keyboard); and, years studying the instrument. In addition, the personalities of study participants were assessed at baseline through the Multidimensional Personality Questionnaire (MPQ). The MPQ provided scores for one lower-order factor (absorption) and three higher-order factors: positive emotionality (PEM—a composite of well-being, social potency, achievement, and social closeness); negative emotionality (NEM—a composite of stress reaction, alienation, and aggression); and, constraint (a composite of control, harm avoidance, and traditionalism). Primary scientific hypotheses of the researchers included: Lower music performance anxiety will be associated with lower levels of a subject’s negative emotionality. Lower music performance anxiety will be associated with lower levels of a subject’s stress reaction. Lower music performance anxiety will be associated with greater number of years of study. 8.3 Initial Exploratory Analyses 8.3.1 Data Organization Our examination of the data from [@Miller2010] in this section will focus on the following key variables: id = unique musician identification number diary = cumulative total of diaries filled out by musician perf_type = type of performance (Solo, Large Ensemble, or Small Ensemble) audience = who attended (Instructor, Public, Students, or Juried) memory = performed from Memory, using Score, or Unspecified na = negative affect score from PANAS gender = musician gender instrument = Voice, Orchestral, or Piano mpqab = absorption subscale from MPQ mpqpem = positive emotionality (PEM) composite scale from MPQ mpqnem = negative emotionality (NEM) composite scale from MPQ Sample rows containing selected variables from our data set are illustrated in Table ??; note that each subject (id) has one row for each unique diary entry. Table 8.1: A snapshot of selected variables from the first three and the last three observations in the Music Performance Anxiety case study. Obs id diary perf_type memory na gender instrument mpqab mpqpem mpqnem 1 1 1 Solo Unspecified 11 Female voice 16 52 16 2 1 2 Large Ensemble Memory 19 Female voice 16 52 16 3 1 3 Large Ensemble Memory 14 Female voice 16 52 16 495 43 2 Solo Score 13 Female voice 31 64 17 496 43 3 Small Ensemble Memory 19 Female voice 31 64 17 497 43 4 Solo Score 11 Female voice 31 64 17 As with any statistical analysis, our first task is to explore the data, examining distributions of individual responses and predictors using graphical and numerical summaries, and beginning to discover relationships between variables. With multilevel models, exploratory analyses must eventually account for the level at which each variable is measured. In a two-level study such as this one, Level One will refer to variables measured at the most frequently occurring observational unit, while Level Two will refer to variables measured on larger observational units. For example, in our study on music performance anxiety, many variables are measured at every performance. These “Level One” variables include: negative affect (our response variable) performance characteristics (type, audience, if music was performed from memory) number of previous performances with a diary entry However, other variables measure characteristics of study participants that remain constant over all performances for a particular musician; these are considered “Level Two” variables and include: demographics (age and gender of musician) instrument used and number of previous years spent studying that instrument baseline personality assessment (MPQ measures of positive emotionality, negative emotionality, constraint, stress reaction, and absorption) 8.3.2 Exploratory Analyses: Univariate Summaries Because of this data structure—the assessment of some variables on a performance-by-performance basis and others on a subject-by-subject basis—we cannot treat our data set as consisting of 497 independent observations. Although negative affect measures from different subjects can reasonably be assumed to be independent (unless, perhaps, the subjects frequently perform in the same ensemble group), neagative affect measures from different performances by the same subject are not likely to be independent. For example, some subjects tend to have relatively high performance anxiety across all performances, so that knowing their score for Performance 3 was 20 makes it more likely that their score for Performance 5 is somewhere near 20 as well. Thus, we must carefully consider our exploratory data analysis, recognizing that certain plots and summary statistics may be useful but imperfect in light of the correlated observations. First, we will examine each response variable and potential covariate individually. Continuous variables can be summarized using histograms and summaries of center and spread; categorical variables can be summarized with tables and possibly bar charts. When examining Level One covariates and responses, we will begin by considering all 497 observations, essentially treating each performance by each subject as independent even though we expect observations from the same musician to be correlated. Although these plots will contain dependent points, since each musician provides data for up to 15 performances, general patterns exhibited in these plots tend to be real. Alternatively, we can calculate mean scores across all performances for each of the 37 musicians so that we can more easily consider each plotted point to be independent. The disadvantage of this approach would be lost information which, in a study such as this with a relatively small number of musicians each being observed over many performances, could be considerable. In addition, if the sample sizes varied greatly by subject, a mean based on 1 observation would be given equal weight to a mean based on 15 observations. Nevertheless, both types of exploratory plots typically illustrate similar relationships. In Figure 8.1 we see histograms for the primary response (negative affect); plot (a) shows all 497 (dependent) observations, while plot (b) shows the mean negative affect for each of the 37 musicians across all their performances. Through plot (a), we see that performance anxiety (negative affect) across all performances follows a right skewed distribution with a lower bound of 10 (achieved when all 10 questions are answered with a 1). Plot (b) shows that mean negative affect is also right-skewed (although not as smoothly decreasing in frequency), with range 12 to 23. Figure 8.1: Histogram of the continuous Level One response (negative effect). Plot (a) contains all 497 performances across the 37 musicians, while plot (b) contains one observation per musician (the mean negative affect across all performances). We can also summarize categorical Level One covariates across all (possibly correlated) observations to get a rough relative comparison of trends. 56.1% of the 497 performances in our data set were solos, while 27.3% were large ensembles and 16.5% were small emsembles. The most common audience type was a public performance (41.0%), followed by instructors (30.0%), students (20.1%), and finally juried recitals (8.9%). In 30.0% of performances, the musician played by memory, while 55.1% used the score and 14.9% of performances were unspecified. To generate an initial examination of Level Two covariates, we consider a data set with just one observation per subject, since Level Two variables are constant over all performances from the same subject. Then, we can proceed as we did with Level One covariates—using histograms to illustrate the distributions of continuous covariates (see Figure 8.2) and tables to summarize categorical covariates. For example, we learn that the majority of subjects have positive emotionality scores between 50 and 60, but that several subjects fall into a lengthy lower tail with scores between 20 and 50. A summary of categorical Level Two covariates reveals that among the 37 subjects (26 female and 11 male), 17 play an orchestral instrument, 15 are vocal performers, and 5 play a keyboard instrument. Figure 8.2: Histograms of the 3 continuous Level Two covariates (negative emotionality (NEM), positive emotionality (PEM), and absorption). Each plot contains one observation per musician. 8.3.3 Exploratory Analyses: Bivariate Summaries The next step in an initial exploratory analysis is the examination of numerical and graphical summaries of relationships between model covariates and responses. In examining these bivariate relationships, we hope to learn: (1) if there is a general trend suggesting that as the covariate increases the response either increases or decreases, (2) if subjects at certain levels of the covariate tend to have similar mean responses (low variability), and (3) if the variation in the response differs at different levels of the covariate (unequal variability). As with individual variables, we will begin by treating all 497 performances recorded as independent observations, even though blocks of 15 or so performances were performed by the same musician. For categorical Level One covariates, we can generate boxplots against negative affect as in Figure 8.3, plots (a) and (b). From these boxplots, we see that lower levels of performance anxiety seem to be associated with playing in large ensembles and playing in front of an instructor. For our lone continuous Level One covariate (number of previous performances), we can generate a scatterplot against negative affect as in plot (c) from Figure 8.3, adding a fitted line to illustrate general trends upward or downward. From this scatterplot, we see that negative affect seems to decrease slightly as a subject has more experience. To avoid the issue of dependent observations in our three plots from Figure 8.3, we could generate separate plots for each subject and examine trends within and across subjects. These “lattice plots” are illustrated in Figures 8.4, 8.5, and 8.6; we discuss such plots more thoroughly in the next section. While general trends are difficult to discern from these lattice plots, we can see the variety in subjects in sample size distributions and overall level of performance anxiety. In particular, in Figure 8.6, we notice that linear fits for many subjects illustrate the same same slight downward trend displayed in the overall scatterplot in Figure 8.3, although some subjects experience increasing anxiety and others exhibit non-linear trends. Having an idea of the range of individual trends will be important when we begin to draw overall conclusions from this study. Figure 8.3: Boxplots of two categorical Level One covariates (performance type (a) and audience type (b)) vs. model response, and scatterplot of one continuous Level One covariate (number of previous diary entries (c)) vs. model response (negative affect). Each plot contains one observation for each of the 497 performances. Figure 8.4: Lattice plot of performance type vs. negative affect, with separate dotplots by subject. Figure 8.5: Lattice plot of audience type vs. negative affect, with separate dotplots by subject. Figure 8.6: Lattice plot of previous performances vs. negative affect, with separate scatterplots with fitted lines by subject. In Figure 8.7, we use boxplots to examine the relationship between our primary categorical Level Two covariate (instrument) and our continuous model response. Plot (a) uses all 497 performances, while plot (b) uses one observation per subject (the mean performance anxiety across all performances) regardless of how many performances that subject had. Naturally, plot (b) has a more condensed range of values, but both plots seem to support the notion that performance anxiety is slightly lower for vocalists and maybe a bit higher for keyboardists Figure 8.7: Boxplots of the categorical Level Two covariate (instrument) vs. model response (negative affect). Plot (a) is based on all 497 observations from all 37 subjects, while plot (b) uses only one observation per subject. In Figure 8.8, we use scatterplots to examine the relationships between continuous Level Two covariates and our model response. Performance anxiety appears to vary little with a subject’s positive emotionality, but there is some evidence to suggest that performance anxiety increases with increasing negative emotionality and absorption level. Plots based on mean negative affect, with one observation per subject, support conclusions based on plots with all observations from all subjects; indeed the overall relationships are in the same direction and of the same magnitude. Figure 8.8: Scatterplots of continuous Level Two covariates (positive emotionality (PEM), negative emotionality (NEM), and absorption) vs. model response (negative affect). The top plots (a1, b1, c1) are based on all 497 observations from all 37 subjects, while the bottom plots (a2, b2, c2) use only one observation per subject. Of course, any graphical analysis is exploratory, and any notable trends at this stage should be checked through formal modeling. At this point, a statistician begins to ask familiar questions such as: which characteristics of individual performances are most associated with performance anxiety? which characteristics of study participants are most associated with performance anxiety? are any of these associations statistically significant? does the significance remain after controlling for other covariates? how do we account for the lack of independence in performances by the same musician? As you might expect, answers to these questions will arise from proper consideration of variability and properly identified statistical models. 8.4 Two level modeling: preliminary considerations 8.4.1 Ignoring the two level structure (not recommended) Armed with any statistical software package, it would be relatively simple to take our complete data set of 497 observations and run an OLS multiple linear regression model seeking to explain variability in negative affect with a number of performance-level or musician-level covariates. As an example, output from a model with two binary covariates (Does the subject play an orchestral instrument? and, Was the performance a large ensemble?) is presented below. Do you see any problems with this approach? lm(formula = na ~ orch + large + orch:large, data = music) Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 15.7212 0.3591 43.778 &lt; 2e-16 *** orch 1.7887 0.5516 3.243 0.00126 ** large -0.2767 0.7910 -0.350 0.72662 orch:large -1.7087 1.0621 -1.609 0.10831 --- Residual standard error: 5.179 on 493 degrees of freedom Multiple R-squared: 0.02782, Adjusted R-squared: 0.0219 F-statistic: 4.702 on 3 and 493 DF, p-value: 0.003012 Other than somewhat skewed residuals, residual plots (not shown) do not indicate any major problems with the OLS multiple regression model. However, another key assumption in these models is the independence of all observations. While we might reasonably conclude that responses from different study participants are independent (although possibly not if they are members of the same ensemble group), it is not likely that the 15 or so observations taken over multiple performances from a single subject are similarly independent. If a subject begins with a relatively high level of anxiety (compared to other subjects) before her first performance, chances are good that she will have relatively high anxiety levels before subsequent performances. Thus, OLS multiple linear regression using all 497 observations is not advisable for this study (or multilevel data sets in general). 8.4.2 A two-stage modeling approach (better but imperfect) If we assume that the 37 study participants can reasonably be considered to be independent, we could use traditional OLS regression techniques to analyze data from this study if we could condense each subject’s set of responses to a single meaningful outcome. Candidates for this meaningful outcome include a subject’s last performance anxiety measurement, average performance anxiety, minimum anxiety level, etc. For example, in clinical trials, data is often collected over many weekly or monthly visits for each patient, except that many patients will drop out early for many reasons (e.g., lack of efficacy, side effects, personal reasons). In these cases, treatments are frequently compared using “last-value-carried-forward” methods—the final visit of each patient is used as the primary outcome measure, regardless of how long they remained in the study. However, “last-value-carried-forward” and other summary measures feel inadequate, since we end up ignoring much of the information contained in the multiple measures for each individual. A more powerful solution is to model performance anxiety at multiple levels. We will begin by considering all performances by a single individual. For instance, consider the 15 performances for which Musician #22 recorded a diary, illustrated in Table ??. Table 8.2: Data from the 15 performances of Musician 22 id diary perform_type audience na instrument 240 22 1 Solo Instructor 24 orchestral instrument 241 22 2 Large Ensemble Public Performance 21 orchestral instrument 242 22 3 Large Ensemble Public Performance 14 orchestral instrument 243 22 4 Large Ensemble Public Performance 15 orchestral instrument 244 22 5 Large Ensemble Public Performance 10 orchestral instrument 245 22 6 Solo Instructor 24 orchestral instrument 246 22 7 Solo Student(s) 24 orchestral instrument 247 22 8 Solo Instructor 16 orchestral instrument 248 22 9 Small Ensemble Public Performance 34 orchestral instrument 249 22 10 Large Ensemble Public Performance 22 orchestral instrument 250 22 11 Large Ensemble Public Performance 19 orchestral instrument 251 22 12 Large Ensemble Public Performance 18 orchestral instrument 252 22 13 Large Ensemble Public Performance 12 orchestral instrument 253 22 14 Large Ensemble Public Performance 19 orchestral instrument 254 22 15 Solo Instructor 25 orchestral instrument Does this musician tend to have higher anxiety levels when he is playing in a large ensemble or playing in front of fellow students? Which factor is the biggest determinant of anxiety for a performance by Musician #22? We can address these questions through OLS multiple linear regression applied to only Musician #22’s data, using appropriate indicator variables for factors of interest. Let \\(Y_{j}\\) be the performance anxiety score of Musician #22 before performance \\(j\\). Consider the observed performances for Musician #22 to be a random sample of all conceivable performances by that subject. If we are initially interested in examining the effect of playing in a large ensemble, we can model the performance anxiety for Musician #22 according to the model: \\[\\begin{equation} Y_{22j}=a_{22}+b_{22}\\textstyle{LargeEns}_{22j}+\\epsilon_{22j} \\textrm{ where } \\epsilon_{22j}\\sim N(0,\\sigma^2) \\textrm{ and } \\tag{8.1} \\end{equation}\\] \\[ \\textstyle{LargeEns}_{j} = \\left\\{ \\begin{array}{ll} 1 &amp; \\mbox{if `perf_type` = Large Ensemble} \\\\ 0 &amp; \\mbox{if `perf_type` = Solo or Small Ensemble.} \\end{array} \\right. \\] The parameters in this model (\\(a_{22}\\), \\(b_{22}\\), and \\(\\sigma^2\\)) can be estimated through OLS methods. \\(a_{22}\\) represents the true intercept for Musician #22—the expected anxiety score for Musician #22 when performance type is a Solo or Small Ensemble (\\(\\textstyle{LargeEns}=0\\)), or the true average anxiety for Musician #22 over all Solo or Small Ensemble performances he may conceivably give. \\(b_{22}\\) represents the true slope for Musician #22—the expected increase in performance anxiety for Musician #22 when performing as part of a Large Ensemble rather than in a Small Ensemble or as a Solo, or the true average difference in anxiety scores for Musician #22 between Large Ensemble performances and other types. Finally, the \\(\\epsilon_{22j}\\) terms represent the deviations of Musician #22’s actual performance anxiety scores from the expected scores under this model—the part of Musician #22’s anxiety before performance \\(j\\) that is not explained by performance type. The variability in these deviations from the regression model is denoted by \\(\\sigma^2\\). For Subject 22, we estimate \\(\\hat{a}_{22}=24.5\\), \\(\\hat{b}_{22}=-7.8\\), and \\(\\hat{\\sigma}=4.8\\). Thus, according to our simple linear regression model, Subject 22 had an estimated anxiety score of 24.5 before Solo and Small Ensemble performances, and 16.7 (7.8 points lower) before Large Ensemble performances. With an \\(R^2\\) of 0.425, the regression model explains a moderate amount (42.5%) of the performance-to-performance variability in anxiety scores for Subject 22, and the trend toward lower scores for large ensemble performances is statistically significant at the 0.05 level (t(13)=-3.10, p=.009). lm(formula = na ~ large, data = id22) Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 24.500 1.960 12.503 1.28e-08 *** large -7.833 2.530 -3.097 0.0085 ** --- Residual standard error: 4.8 on 13 degrees of freedom Multiple R-squared: 0.4245, Adjusted R-squared: 0.3802 F-statistic: 9.588 on 1 and 13 DF, p-value: 0.008504 We could continue trying to build a better model for Subject 22, adding indicators for audience and memory, and even adding a continuous variable representing the number of previous performance where a diary was kept. As our model R-square value increased, we would be explaining a larger proportion of Subject 22’s performance-to-performance variability in anxiety. It would not, however, improve our model to incorporate predictors for ag, gender, or even negative emotionality based on the MPQ—why is that? For the present time, we will model Subject 22’s anxiety scores for his 15 performances using the model given by Equation (8.1), with a lone indicator variable for performing in a Large Ensemble. We can then proceed to fit the OLS regression model in Equation (8.1) to examine the effect of performing in a Large Ensemble for each of the 37 subjects in this study. These are called Level One models. As displayed in Figure 8.9, there is considerable variability in the fitted intercepts and slopes among the 37 subjects. Mean performance anxiety scores for Solos and Small Ensembles range from 11.6 to 24.5, with a median score of 16.7, while mean differences in performance anxiety scores for Large Ensembles compared to Solos and Small Ensembles range from -7.9 to 5.0, with a median difference of -1.7. Can these differences among individual musicians be explained by (performance-invariant) characteristics associated with each individual, such as gender, age, instrument, years studied, or baseline levels of personality measures? Questions like these can be addressed through further statistical modeling. Figure 8.9: Histograms of intercepts and slopes from fitting simple regression models by subject, where each model contained a single binary predictor indicating if a performance was part of a large ensemble. As an illustration, we can consider whether or not there are significant relationships between individual regression parameters (intercepts and slopes) and instrument played. From a modeling perspective, we would build a system of two Level Two models to predict the fitted intercept (\\(a_{i}\\)) and fitted slopes (\\(b_{i}\\)) for Subject \\(i\\): \\[\\begin{eqnarray} a_{i} &amp; = &amp; \\alpha_{0}+\\alpha_{1}\\textstyle{Orch}_{i}+u_{i} \\tag{8.2} \\\\ b_{i} &amp; = &amp; \\beta_{0}+\\beta_{1}\\textstyle{Orch}_{i}+v_{i} \\tag{8.3} \\end{eqnarray}\\] where \\(\\textstyle{Orch}_{i}=1\\) if Subject \\(i\\) plays an orchestral instrument and \\(\\textstyle{Orch}_{i}=0\\) if Subject \\(i\\) plays keyboard or is a vocalist. Note that, at Level Two, our response variables are not observed measurements such as performance anxiety scores, but rather the fitted regression coefficients from the Level One models fit to each subject. (Well, in our theoretical model, the responses are actually the true intercepts and slopes from Level One models for each subject, but in reality, we have to use our estimated slopes and intercepts.) Exploratory data analysis (see boxplots by instrument in Figure 8.10 suggests that subjects playing orchestral instruments have higher intercepts than vocalists or keyboardists, and that orchestral instruments are associated with slight lower (more negative) slopes, although with less variability that the slopes of vocalists and keyboardists. These trends are borne out in regression modeling. If we fit Equations (8.2) and (8.3) using fitted intercepts and slopes as our response variables, we obtain the following estimated parameters: \\(\\hat{\\alpha}_{0}=16.3\\), \\(\\hat{\\alpha}_{1}=1.4\\), \\(\\hat{\\beta}_{0}=-0.8\\), and \\(\\hat{\\beta}_{1}=-1.4\\). Thus, the intercept (\\(a_{i}\\)) and slope (\\(b_{i}\\)) for Subject \\(i\\) can be modeled as: \\[\\begin{eqnarray} \\hat{a}_{i} &amp; = &amp; 16.3+1.4\\textstyle{Orch}_{i}+u_{i} \\tag{8.4} \\\\ \\hat{b}_{i} &amp; = &amp; -0.8-1.4\\textstyle{Orch}_{i}+v_{i} \\tag{8.5} \\end{eqnarray}\\] where \\(a_{i}\\) is the true mean negative affect when Subject \\(i\\) is playing solos or small ensembles, and \\(b_{i}\\) is the true mean difference in negative affect for Subject \\(i\\) between large ensembles and other performance types. Based on these models, average performance anxiety before solos and small ensembles is 16.3 for vocalists and keyboardists, but 17.7 (1.4 points higher) for orchestral instrumentalists. Before playing in large ensembles, vocalists and instrumentalists have performance anxiety (15.5) which is 0.8 points lower, on average, than before solos and small ensembles, while subjects playing orchestral instruments experience an average difference of 2.2 points, producing an average performance anxiety of 15.5 before playing in large ensembles just like subjects playing other instruments. However, the difference between orchestral instruments and others does not appear to be statistically significant for either intercepts (t=1.424, p=0.163) or slopes (t=-1.168, p=0.253). Figure 8.10: Boxplots of fitted intercepts (plot (a)) and slopes (plot (b)) by orchestral instrument (1) vs. keyboard or vocalist (0). This two stage modeling process does have some drawbacks. Among other things, (1) it weights every subject the same regardless of the number of diary entries we have, (2) it responds to missing individual slopes (from 7 subjects who never performed in a large ensemble) by simply dropping those subjects, and (3) it does not share strength effectively across individuals. These issues can be better handled through a unified multilevel modeling framework which we will develop over the remainder of this section. 8.5 Two level modeling: a unified approach 8.5.1 Our framework For the unified approach, we will still envision two levels of models as in section 8.4.2, but we will use likelihood-based methods for parameter estimation rather than ordinary least squares to address the drawbacks associated with the two-stage approach. To illustrate the unified approach, we will first generalize the models presented in section 8.4.2. Let \\(Y_{ij}\\) be the performance anxiety score of the \\(i^{th}\\) subject before performance \\(j\\). If we are initially interested in examining the effects of playing in a large ensemble and playing an orchestral instrument, then we can model the performance anxiety for Subject \\(i\\) in performance \\(j\\) with the following system of equations: Level One: \\[\\begin{equation} Y_{ij} = a_{i}+b_{i}\\textstyle{LargeEns}_{ij}+\\epsilon_{ij} \\tag{8.6} \\end{equation}\\] Level Two: \\[\\begin{eqnarray*} a_{i} &amp; = &amp; \\alpha_{0}+\\alpha_{1}\\textstyle{Orch}_{i}+u_{i} \\\\ b_{i} &amp; = &amp; \\beta_{0}+\\beta_{1}\\textstyle{Orch}_{i}+v_{i}, \\end{eqnarray*}\\] In this system, there are 4 key fixed effects to estimate: \\(\\alpha_{0}\\), \\(\\alpha_{1}\\), \\(\\beta_{0}\\) and \\(\\beta_{1}\\). Fixed effects are the fixed but unknown population effects associated with certain covariates. The intercepts and slopes for each subject from Level One, \\(a_{i}\\) and \\(b_{i}\\), don’t need to be formally estimated as we did in section 8.4.2; they serve to conceptually connect Level One with Level Two. In fact, by substituting the two Level Two equations into the Level One equation, we can view this two-level system of models as a single Composite Model without \\(a_{i}\\) and \\(b_{i}\\): \\[\\begin{eqnarray*} Y_{ij} &amp; = &amp; [\\alpha_{0}+\\alpha_{1}\\textstyle{Orch}_{i}+\\beta_{0}\\textstyle{LargeEns}_{ij}+\\beta_{1}\\textstyle{Orch}_{i}\\textstyle{LargeEns}_{ij}] \\\\ &amp; &amp; \\mbox{} + [u_{i}+v_{i}\\textstyle{LargeEns}_{ij}+\\epsilon_{ij}] \\end{eqnarray*}\\] From this point forward, when building multilevel models, we will use Greek letters (such as \\(\\alpha_{0}\\)) to denote final fixed effects model parameters to be estimated empirically, and Roman letters (such as \\(a_{0}\\)) to denote preliminary fixed effects parameters at lower levels. Variance components that will be estimated empirically will be denoted with \\(\\sigma\\) or \\(\\rho\\), while terms such as \\(\\epsilon\\) and \\(u_{i}\\) represent error terms. In our framework, we can estimate final parameters directly without first estimating preliminary parameters, which can be seen with the Composite Model formulation (although we can obtain estimates of preliminary parameters in those occasional cases when they are of interest to us). Note that when we model a slope term like \\(b_{i}\\) from Level One using Level Two covariates like \\(\\textstyle{Orch}_{i}\\), the resulting Composite Model contains an interaction term, denoting that the effect of \\(\\textstyle{LargeEns}_{ij}\\) depends on the instrument played. Furthermore, with a binary predictor at Level Two such as instrument, we can write out what our Level Two model looks like for those who play keyboard or are vocalists (\\(\\textstyle{Orch}_{i}=0\\)) and those who play orchestral instruments (\\(\\textstyle{Orch}_{i}=1\\)): Keyboardists and Vocalists (\\(\\textstyle{Orch}_{i}=0\\)) \\[\\begin{eqnarray*} a_{i} &amp; = &amp; \\alpha_{0}+u_{i} \\\\ b_{i} &amp; = &amp; \\beta_{0}+v_{i} \\end{eqnarray*}\\] Orchestral instrumentalists (\\(\\textstyle{Orch}_{i}=1\\)) \\[\\begin{eqnarray*} a_{i} &amp; = &amp; (\\alpha_{0}+\\alpha_{1})+u_{i} \\\\ b_{i} &amp; = &amp; (\\beta_{0}+\\beta_{1})+v_{i} \\label{eq:level2byorch} \\end{eqnarray*}\\] Writing the Level Two model in this manner helps us interpret the model parameters from our two-level model. In this case, even the Level One covariate is binary, so that we can write out expressions for mean performance anxiety based on our model for four different combinations of instrument played and performance type: Keyboardists or vocalists playing solos or small ensembles: \\(\\alpha_{0}\\) Keyboardists or vocalists playing large ensembles: \\(\\alpha_{0}+\\beta_{0}\\) Orchestral instrumentalists playing solos or small ensembles: \\(\\alpha_{0}+\\alpha_{1}\\) Orchestral instrumentalists playing large ensembles: \\(\\alpha_{0}+\\alpha_{1}+\\beta_{0}+\\beta_{1}\\) 8.5.2 Random vs. fixed effects Before we can use likelihood-based methods to estimate our model parameters, we still must define the distributions of our error terms. The error terms \\(\\epsilon_{ij}\\), \\(u_{i}\\), and \\(v_{i}\\) represent random effects in our model. In multilevel models, it is important to distinguish between fixed and random effects. Typically, fixed effects describe levels of a factor that we are specifically interested in drawing inferences about, and which would not change in replications of the study. For example, in our music performance anxiety case study, the levels of performance type will most likely remain as solos, small ensembles, and large ensembles even in replications of the study, and we wish to draw specific conclusions about differences between these three types of performances. Thus, performance type would be considered a fixed effect. On the other hand, random effects describe levels of a factor which can be thought of as a sample from a larger population of factor levels; we are not typically interested in drawing conclusions about specific levels of a random effect, although we are interested in accounting for the influence of the random effect in our model. For example, in our case study the different musicians included can be thought of as a random sample from a population of performing musicians. Although our goal is not to make specific conclusions about differences between any two musicians, we do want to account for inherent differences among musicians in our model, and by doing so, we will be able to draw more precise conclusions about our fixed effects of interest. Thus, musician would be considered a random effect. 8.5.3 Distribution of errors: the multivariate normal distribution As part of our multilevel model, we must provide probability distributions to describe the behavior of random effects. Typically, we assume that random effects follow a normal distribution with mean 0 and a variance parameter which must be estimated from the data. For example, at Level One, we will assume that the errors associated with each performance of a particular musician can be described as: \\(\\epsilon_{ij}\\sim N(0,\\sigma^2)\\). At Level Two, we have one error term (\\(u_{i}\\)) associated with subject-to-subject differences in intercepts, and one error term (\\(v_{i}\\)) associated with subject-to-subject differences in slopes. That is, \\(u_{i}\\) represents the deviation of Subject \\(i\\) from the mean performance anxiety before solos and small ensembles after accounting for his or her instrument, and \\(v_{i}\\) represents the deviation of Subject \\(i\\) from the mean difference in performance anxiety between large ensembles and other performance types after accounting for his or her instrument. In modeling the random behavior of \\(u_{i}\\) and \\(v_{i}\\), we must also account for the possibility that random effects at the same level might be correlated. Subjects with higher baseline performance anxiety have a greater capacity for showing decreased anxiety in large ensembles as compared to solos and small ensembles, so we might expect that subjects with larger intercepts (performance anxiety before solos and small ensembles) would have smaller slopes (indicating greater decreases in anxiety before large ensembles). In fact, our fitted Level One intercepts and slopes in this example actually show evidence of a fairly strong negative correlation (\\(r=-0.525\\), see Figure 8.11). Figure 8.11: Scatterplot with fitted regression line for estimated intercepts and slopes (one point per subject). To allow for this correlation, the error terms at Level Two can be assumed to follow a multivariate normal distribution in our unified multilevel model. Mathematically, we can express this as: \\[ \\left[ \\begin{array}{c} u_{i} \\\\ v_{i} \\end{array} \\right] \\sim N \\left( \\left[ \\begin{array}{c} 0 \\\\ 0 \\end{array} \\right], \\left[ \\begin{array}{cc} \\sigma_{u}^{2} &amp; \\\\ \\rho_{uv}\\sigma_{u}\\sigma_v &amp; \\sigma_{v}^{2} \\end{array} \\right] \\right) \\] Note that the correlation \\(\\rho_{uv}\\) between the error terms is simply the covariance \\(\\sigma_{uv}=\\rho_{uv}\\sigma_{u}\\sigma_{v}\\) converted to a \\([-1,1]\\) scale through the relationship: \\[\\begin{equation} \\rho_{uv} = \\frac{\\rho\\sigma_{u}\\sigma_{v}}{\\sigma_{u}\\sigma_{v}} \\tag{8.7} \\end{equation}\\] With this expression, we are allowing each error term to have its own variance (around a mean of 0) and each pair of error terms to have its own covariance (or correlation). Thus, if there are \\(n\\) equations at Level Two, we can have \\(n\\) variance terms and \\(n(n-1)/2\\) covariance terms for a total of \\(n + n(n-1)/2\\) variance components. These variance components are organized in matrix form, with variance terms along the diagonal and covariance terms in the off-diagonal. In our small example, we have \\(n=2\\) equations at Level Two, so we have 3 variance components to estimate—2 variance terms (\\(\\sigma_{u}^{2}\\) and \\(\\sigma_{v}^{2}\\)) and 1 covariance term (\\(\\sigma_{uv}\\)). The multivariate normal distribution with \\(n=2\\) is illustrated in Figure 8.12 for two cases: (a) the error terms are uncorrelated (\\(\\sigma_{uv}=\\rho_{uv}=0\\)), and (b) the error terms are positively correlated (\\(\\sigma_{uv}&gt;0\\) and \\(\\rho_{uv} &gt; 0\\)). In general, if the errors in intercepts (\\(u_{i}\\)) are placed on the x-axis and the errors in slopes (\\(v_{i}\\)) are placed on the y-axis, then \\(\\sigma_{u}^{2}\\) measures spread in the x-direction and \\(\\sigma_{v}^{2}\\) measures spread in the y-direction, while \\(\\sigma_{uv}\\) measures tilt. Positive tilt (\\(\\sigma_{uv}&gt;0\\)) indicates a tendency for errors from the same subject to both be positive or both be negative, while negative tilt (\\(\\sigma_{uv}&lt;0\\)) indicates a tendency for one error from a subject to be positive and the other to be negative. In Figure 8.12, \\(\\sigma_{u}^{2}=4\\) and \\(\\sigma_{v}^{2}=1\\), so both contour plots show a greater range of errors in the x-direction than the y-direction. Internal ellipses in the contour plot indicate pairs of \\(u_{i}\\) and \\(v_{i}\\) that are more likely. In Figure 8.12 (a) \\(\\sigma_{uv}=\\rho_{uv}=0\\), so the axes of the contour plot correspond to the x- and y-axes, but in Figure 8.12 (b) \\(\\sigma_{uv}=1.5\\), so the contour plot tilts up, reflecting a tendency for high values of \\(u_{i}\\) to be associated with high values of \\(v_{i}\\). Figure 8.12: Contour plots illustrating multivariate normal density with (a) no correlation between error terms, and (b) positive correlation between error terms. 8.5.4 Technical issues when estimating and testing parameters (Optional) Now, our two-level model has 8 parameters that need to be estimated: 4 fixed effects (\\(\\alpha_{0}\\), \\(\\alpha_{1}\\), \\(\\beta_{0}\\), and \\(\\beta_{1}\\)), and 4 variance components (\\(\\sigma^{2}\\), \\(\\sigma_{u}^{2}\\), \\(\\sigma_{v}^{2}\\), and \\(\\sigma_{uv}\\)). Note that we use the term variance components to signify model parameters that describe the behavior of random effects. We can use statistical software, such as the lmer() function from the lme4 package in R, to obtain parameter estimates using our 497 observations. The most common methods for estimating model parameters—both fixed effects and variance components—are maximum likelihood (ML) and restricted maximum likelihood (REML). The method of maximum likelihood (ML) was introduced in Chapter 2, where parameter estimates are chosen to maximize the value of the likelihood function based on observed data. Restricted maximum likelihood (REML) is conditional on the fixed effects, so that the part of the data used for estimating variance components is separated from that used for estimating fixed effects. Thus REML, by accounting for the loss in degrees of freedom from estimating the fixed effects, provides an unbiased estimate of variance components, while ML estimators for variance components are biased under assumptions of normality, since they use estimated fixed effects rather than the true values. REML is preferable when the number of parameters is large or the primary interest is obtaining estimates of model parameters, either fixed effects or variance components associated with random effects. ML should be used if nested fixed effects models are being compared using a likelihood ratio test, although REML is fine for nested models of random effects (with the same fixed effects model). In this text, we will typically report REML estimates unless we are specifically comparing nested models with the same random effects. In most case studies and most models we consider, there is very little difference between ML and REML parameter estimates. Additional details are beyond the scope of this book (Singer and Willett, 2003). Note that the multilevel output shown beginning in the next section contains no p-values for performing hypothesis tests. This is primarily because the exact distribution of the test statistics under the null hypothesis (no fixed effect) is unknown, primarily because the exact degrees of freedom is not known (Bates 2015). Finding good approximate distributions for test statistics (and thus good approximate p-values) in multilevel models is an area of active research. In most cases, we can simply conclude that t-values (ratios of parameter estimates to estimated standard errors) with absolute value above 2 indicate significant evidence that a particular model parameter is different than 0. Certain software packages will report p-values corresponding to hypothesis tests for parameters of fixed effects; these packages are typically using conservative assumptions, large-sample results, or approximate degrees of freedom for a t-distribution. In section 1.6.4, we introduced the bootstrap as a non-parametric, computational approach for producing confidence intervals for model parameters. In addition, in section 10.6, we will introduce a method called the parametric bootstrap which is being used more frequently by researchers to better approximate the distribution of the likelihood test statistic and produce more accurate p-values by simulating data under the null hypothesis (Efron 2011). 8.5.5 An initial model with parameter interpretations The output below contains likelihood-based estimates of our 8 parameters from a two-level model applied to the music performance anxiety data: Linear mixed model fit by REML [&#39;lmerMod&#39;] A) Formula: na ~ orch + large + orch:large + (large | id) Data: music B) REML criterion at convergence: 2987 B2) AIC BIC logLik deviance df.resid 3007.2 3040.9 -1495.6 2991.2 489 Random effects: Groups Name Variance Std.Dev. Corr C) id (Intercept) 5.655 2.3781 D) large 0.452 0.6723 -0.63 E) Residual 21.807 4.6698 F) Number of obs: 497, groups: id, 37 Fixed effects: Estimate Std. Error t value G) (Intercept) 15.9297 0.6415 24.833 H) orch 1.6926 0.9452 1.791 I) large -0.9106 0.8452 -1.077 J) orch:large -1.4239 1.0992 -1.295 This output (except for the capital letters along the left column) was specifically generated by the lmer() function in R; multilevel modeling results from other packages will contain similar elements. Because we will use lmer() output to summarize analyses of case studies in this and following sections, we will spend a little time now orienting ourselves to the most important features in this output. How our multilevel model is written in R, based on the composite model formulation. For more details, see section 8.12. Measures of model performance. Since this model was fit using REML, this line only contains the REML criterion. B2) If the model is fit with ML instead of REML, the measures of performance will contain AIC, BIC, deviance, and the log-likelihood. Estimated variance components (\\(\\hat{\\sigma}_{u}^2\\) and \\(\\hat{\\sigma}_{u}\\)) associated with the intercept equation in Level Two. Estimated variance components (\\(\\hat{\\sigma}_{v}^2\\) and \\(\\hat{\\sigma}_{v}\\)) associated with the large ensemble effect equation in Level Two, along with the estimated correlation (\\(\\hat{\\rho}_{uv}\\)) between the two Level Two error terms. Estimated variance components (\\(\\hat{\\sigma}^2\\) and \\(\\hat{\\sigma}\\)) associated with the Level One equation. Total number of performances where data was collected (Level One observations = 497) and total number of subjects (Level Two observations = 37). Estimated fixed effect (\\(\\hat{\\alpha}_{0}\\)) for intercept term, along with its standard error and t-value (which is the ratio of the estimated coefficient to its standard error). As described in section 8.5.4, no p-value testing the significance of the coefficient is provided because the exact null distribution of the t-value is unknown. Estimated fixed effect (\\(\\hat{\\alpha}_{1}\\)) for the orchestral instrument effect, along with its standard error and t-value . Estimated fixed effect (\\(\\hat{\\beta}_{0}\\)) for the large ensemble effect, along with its standard error and t-value. Estimated fixed effect (\\(\\hat{\\beta}_{1}\\)) for the interaction between orchestral instruments and large ensembles, along with its standard error and t-value. Assuming the 37 musicians in this study are representative of a larger population of musicians, parameter interpretations for our 8 model parameters are given below: Fixed effects: \\(\\hat{\\alpha}_{0} = 15.9\\). The estimated mean performance anxiety for solos and small ensembles (Large=0) for keyboard players and vocalists (Orch=0) is 15.9. \\(\\hat{\\alpha}_{1} = 1.7\\). Orchestral instrumentalists have an estimated mean performance anxiety for solos and small ensembles which is 1.7 points higher than keyboard players and vocalists. \\(\\hat{\\beta}_{0} = -0.9\\). Keyboard players and vocalists have an estimated mean decrease in performance anxiety of 0.9 points when playing in large ensembles instead of solos or small ensembles. \\(\\hat{\\beta}_{1} = -1.4\\). Orchestral instrumentalists have an estimated mean decrease in performance anxiety of 2.3 points when playing in large ensembles instead of solos or small ensembles, 1.4 points greater than the mean decrease among keyboard players and vocalists. Variance components \\(\\hat{\\sigma}_{u} = 2.4\\). The estimated standard deviation of performance anxiety levels for solos and small ensembles is 2.4 points, after controlling for instrument played. \\(\\hat{\\sigma}_{v} = 0.7\\). The estimated standard deviation of differences in performance anxiety levels between large ensembles and other performance types is 0.7 points, after controlling for instrument played. \\(\\hat{\\rho}_{uv} = -0.64\\). The estimated correlation between performance anxiety scores for solos and small ensembles and increases in performance anxiety for large ensembles is -0.64, after controlling for instrument played. Those subjects with higher performance anxiety scores for solos and small ensembles tend to have greater decreases in performance anxiety for large ensemble performances. \\(\\hat{\\sigma} = 4.7.\\) The estimated standard deviation in residuals for the individual regression models is 4.7 points. Table 8.3 shows a side-by-side comparison of estimated coefficients from the approaches described to this point. Underlying assumptions, especially regarding the error and correlation structure, differ, and differences in estimated effects are potentially meaningful. Note that some standard errors are greatly underestimated under independence, and that no Level One covariates (such as performance type) can be analyzed under a method such as last-visit-carried-forward which uses one observation per subject. Moving forward, we will employ the unified multilevel approach to maximize the information being used to estimate model parameters and to remain faithful to the structure of the data. Table 8.3: Comparison of estimated coefficients and standard errors from the approaches mentioned in this section. Variable Independence TwoStage LVCF Multilevel Intercept 15.72(0.36) 16.28(0.67) 15.20(1.25) 15.93(0.64) Orch 1.79(0.55) 1.41(0.99) 1.45(1.84) 1.69(0.95) Large -0.28(0.79) -0.77(0.85) - -0.91(0.85) Orch*Large -1.71(1.06) -1.41(1.20) - -1.42(1.10) Two level modeling as done with the music performance anxiety data usually involves fitting a number of models. Subsequent sections will describe a process of starting with the simplest two-level models and building toward a final model which addresses the research questions of interest. 8.6 Building a multilevel model 8.6.1 Model building strategy Initially, it is advisable to first fit some simple, preliminary models, in part to establish a baseline for evaluating larger models. Then, we can build toward a final model for description and inference by attempting to add important covariates, centering certain variables, and checking model assumptions. In this study, we are particularly interested in Level Two covariates—those subject-specific variables that provide insight into why individuals react differently in anxiety-inducing situations. To get more precise estimates of the effect of Level Two covariates, we also want to control for Level One covariates that describe differences in individual performances. Our strategy for building multilevel models will begin with extensive exploratory data analysis at each level. Then, after examining models with no predictors to assess variability at each level, we will first focus on creating a Level One model, starting simple and adding terms as necessary. Next, we will move to Level Two models, again starting simple and adding terms as necessary, beginning with the equation for the intercept term. Finally, we will examine the random effects and variance components, beginning with a full set of error terms and then removing covariance terms and variance terms where advisable (for instance, when parameter estimates are failing to converge or producing impossible or unlikely values). This strategy follows closely with that described by Raudenbush and Byrk (2002) and used by Singer and Willett (2003). Singer and Willett further find that the modeled error structure rarely matters in practical contexts. Other model building approaches are certainly possible. Diggle et al. (2002), for example, begins with a saturated fixed effects model, determines variance components based on that, and then simplifies the fixed part of the model after fixing the random part. 8.6.2 An initial model: unconditional means or random intercepts The first model fit in almost any multilevel context should be the unconditional means model, also called a random intercepts model. In this model, there are no predictors at either level; rather, the purpose of the unconditional means model is to assess the amount of variation at each level—to compare variability within subject to variability between subjects. Expanded models will then attempt to explain sources of between and within subject variability. The unconditional means (random intercepts) model, which we will denote as Model A, can be specified either using formulations at both levels: Level One: \\[\\begin{equation} Y_{ij} = a_{i}+\\epsilon_{ij} \\textrm{ where } \\epsilon_{ij}\\sim N(0,\\sigma^2) \\tag{8.8} \\end{equation}\\] Level Two: \\[\\begin{equation} a_{i} = \\alpha_{0}+u_{i} \\textrm{ where } u_{i}\\sim N(0,\\sigma_{u}^{2}) \\tag{8.9} \\end{equation}\\] or as a composite model: \\[\\begin{equation} Y_{ij}=\\alpha_{0}+u_{i}+\\epsilon_{ij} \\tag{8.10} \\end{equation}\\] In this model, the performance anxiety scores of subject \\(i\\) are not a function of performance type or any other Level One covariate, so that \\(a_{i}\\) is the true mean response of all observations for subject \\(i\\). On the other hand, \\(\\alpha_{0}\\) is the grand mean – the true mean of all observations across the entire population. Our primary interest in the unconditional means model is the variance components – \\(\\sigma^2\\) is the within-person variability, while \\(\\sigma_{u}^{2}\\) is the between-person variability. The name random intercepts model then arises from the Level Two equation for \\(a_{i}\\): each subject’s intercept is assumed to be a random value from a normal distribution centered at \\(\\alpha_{0}\\) with variance \\(\\sigma_{u}^{2}\\). Using the composite model specification, the unconditional means model can be fit to the music performance anxiety data using statistical software: Formula: na ~ 1 + (1 | id) Random effects: Groups Name Variance Std.Dev. id (Intercept) 4.95 2.225 Residual 22.46 4.739 Number of obs: 497, groups: id, 37 Fixed effects: Estimate Std. Error t value (Intercept) 16.2370 0.4279 37.94 From this output, we obtain estimates of our three model parameters: \\(\\hat{\\alpha}_{0}=16.2=\\) the estimated mean performance anxiety score across all performances and all subjects. \\(\\hat{\\sigma}^2=22.5=\\) the estimated variance in within-person deviations. \\(\\hat{\\sigma}_{u}^{2}=5.0=\\) the estimated variance in between-person deviations. The relative levels of between- and within-person variabilities can be compared through the intraclass correlation coefficient: \\[\\begin{equation} \\hat{\\rho}=\\frac{\\textrm{Between-person variability}}{\\textrm{Total variability}} = \\frac{\\hat{\\sigma}_{u}^{2}}{\\hat{\\sigma}_{u}^{2}+\\hat{\\sigma}^2} = \\frac{5.0}{5.0+22.5} = .182. \\tag{8.11} \\end{equation}\\] Thus, 18.2% of the total variability in performance anxiety scores are attributable to differences among subjects. In this particular model, we can also say that the average correlation for any pair of responses from the same individual is a moderately low .182. As \\(\\rho\\) approaches 0, responses from an individual are essentially independent and accounting for the multilevel structure of the data becomes less crucial. However, as \\(\\rho\\) approaches 1, repeated observations from the same individual essentially provide no additional information and accounting for the multilevel structure becomes very important. With \\(\\rho\\) near 0, the effective sample size (the number of independent pieces of information we have for modeling) approaches the total number of observations, while with \\(\\rho\\) near 1, the effective sample size approaches the number of subjects in the study. 8.7 Binary covariates at Level One and Level Two 8.7.1 Random slopes and intercepts model The next step in model fitting is to build a good model for predicting performance anxiety scores at Level One (within subject). We will add potentially meaningful Level One covariates—those that vary from performance-to-performance for each individual. In this case, mirroring our model from section 8.4, we will include a binary covariate for performance type: \\[ \\textstyle{LargeEns}_{ij} = \\left\\{ \\begin{array}{ll} 1 &amp; \\mbox{if `perf_type` = Large Ensemble} \\\\ 0 &amp; \\mbox{if `perf_type` = Solo or Small Ensemble.} \\end{array} \\right. \\] and no other Level One covariates (for now). (Note that we may later also want to include an indicator variable for “Small Ensemble” to separate the effects of Solo performances and Small Ensemble performances.) The resulting model, which we will denote as Model B, can be specified either using formulations at both levels: Level One: \\[\\begin{equation} Y_{ij} = a_{i}+b_{i}\\textstyle{LargeEns}_{ij}+\\epsilon_{ij} \\tag{8.12} \\end{equation}\\] Level Two: \\[\\begin{eqnarray*} a_{i} &amp; = &amp; \\alpha_{0}+u_{i} \\\\ b_{i} &amp; = &amp; \\beta_{0}+v_{i} \\end{eqnarray*}\\] or as a composite model: \\[\\begin{equation} Y_{ij}=[\\alpha_{0}+\\beta_{0}\\textstyle{LargeEns}_{ij}]+[u_{i}+v_{i}\\textstyle{LargeEns}_{ij}+\\epsilon_{ij}] \\tag{8.13} \\end{equation}\\] where \\(\\epsilon_{ij}\\sim N(0,\\sigma^2)\\) and \\[ \\left[ \\begin{array}{c} u_{i} \\\\ v_{i} \\end{array} \\right] \\sim N \\left( \\left[ \\begin{array}{c} 0 \\\\ 0 \\end{array} \\right], \\left[ \\begin{array}{cc} \\sigma_{u}^{2} &amp; \\\\ \\rho\\sigma_{u}\\sigma_{v} &amp; \\sigma_{v}^{2} \\end{array} \\right] \\right). \\] as discussed in section 8.5.3. In this model, performance anxiety scores for subject \\(i\\) are assumed to differ (on average) for Large Ensemble performances as compared with Solos and Small Ensemble performances; the \\(\\epsilon_{ij}\\) terms capture the deviation between the true performance anxiety levels for subjects (based on performance type) and their observed anxiety levels. \\(\\alpha_{0}\\) is then the true mean performance anxiety level for Solos and Small Ensembles, and \\(\\beta_{0}\\) is the true mean difference in performance anxiety for Large Ensembles compared to other performance types. As before, \\(\\sigma^2\\) quantifies the within-person variability (the scatter of points around individuals’ means by performance type), while now the between-person variability is partitioned into variability in Solo and Small Ensemble scores (\\(\\sigma_{u}^{2}\\)) and variability in differences with Large Ensembles (\\(\\sigma_{v}^{2}\\)). Using the composite model specification, Model B can be fit to the music performance anxiety data, producing the following output: Formula: na ~ large + (large | id) Random effects: Groups Name Variance Std.Dev. Corr id (Intercept) 6.3330 2.5165 large 0.7429 0.8619 -0.76 Residual 21.7712 4.6660 Number of obs: 497, groups: id, 37 Fixed effects: Estimate Std. Error t value (Intercept) 16.7297 0.4908 34.09 large -1.6762 0.5425 -3.09 From this output, we obtain estimates of our six model parameters (2 fixed effects and 4 variance components): \\(\\hat{\\alpha}_{0}=16.7=\\) the mean performance anxiety level before solos and small ensemble performances. \\(\\hat{\\beta}_{0}=-1.7=\\) the mean decrease in performance anxiety before large ensemble performances. \\(\\hat{\\sigma}^2=21.8=\\) the variance in within-person deviations. \\(\\hat{\\sigma}_{u}^{2}=6.3=\\) the variance in between-person deviations in performance anxiety scores before solos and small ensembles. \\(\\hat{\\sigma}_{v}^{2}=0.7=\\) the variance in between-person deviations in increases (or decreases) in performance anxiety scores before large ensembles. \\(\\hat{\\rho}_{uv}=-0.76=\\) the correlation in subjects’ anxiety before solos and small ensembles and their differences in anxiety between large ensembles and other performance types. We see that, on average, subjects had a performance anxiety level of 16.7 before solos and small ensembles, and their anxiety levels were 1.7 points lower, on average, before large ensembles, producing an average performance anxiety level before large ensembles of 15.0. According to the t-value listed in R, the difference between large ensembles and other performance types is statistically significant (t=-3.09). This random slopes and intercepts model is illustrated in Figure 8.13. The thicker black line shows the overall trends given by our estimated fixed effects: an intercept of 16.7 and a slope of -1.7. Then, each subject is represented by a gray line. Not only do the subjects’ intercepts differ (with variance 6.3), but their slopes differ as well (with variance 0.7). Additionally, subjects’ slopes and intercepts are negatively associated (with correlation -0.76), so that subjects with greater intercepts tend to have steeper negative slopes. We can compare this model with the random intercepts model from section 8.6.2, pictured in Figure 8.14. With no effect of large ensembles, each subject is represented by a gray line with the identical slope (0), but with varying intercepts (with variance 5.0). Figure 8.13: The random slopes and intercepts model fitted to the music performance anxiety data. Each gray line represents one subject, and the thicker black line represents the trend across all subjects. Figure 8.14: The random intercepts model fitted to the music performance anxiety data. Each gray line represents one subject, and the thicker black line represents the trend across all subjects. Figures 8.13 and 8.14 use empirical Bayes estimates for the intercepts (\\(a_{i}\\)) and slopes (\\(b_{i}\\)) of individual subjects. Empirical Bayes estimates are sometimes called “shrinkage estimates” since they combine individual-specific information with information from all subjects, thus “shrinking” the individual estimates toward the group averages. Empirical Bayes estimates are often used when a term such as \\(a_{i}\\) involves both fixed and random components; further detail can be found in Raudenbush and Byrk (2002) and Singer and Willett (2003). 8.7.2 Pseudo \\(R^2\\) values The estimated within-person variance \\(\\hat{\\sigma}^2\\) decreased by 3.1% (from 22.5 to 21.8) from the unconditional means model, implying that only 3.1% of within-person variability in performance anxiety scores can be explained by performance type. This calculation is considered a pseudo R-square value: \\[\\begin{equation} \\textrm{Pseudo }R^2_{L1} = \\frac{\\hat{\\sigma}^{2}(\\textrm{Model A})-\\hat{\\sigma}^{2}(\\textrm{Model B})}{\\hat{\\sigma}^{2}(\\textrm{Model A})} = \\frac{22.5-21.8}{22.5} = 0.031 \\tag{8.14} \\end{equation}\\] Values of \\(\\hat{\\sigma}_{u}^{2}\\) and \\(\\hat{\\sigma}_{v}^{2}\\) cannot be compared to between-person variability from Model A, since the inclusion of performance type has changed the interpretation of these values, although they can provide important benchmarks for evaluating more complex Level Two predictions. Finally, \\(\\hat{\\rho}_{uv}=-0.76\\) indicates a strong negative relationship between a subject’s performance anxiety before solos and small ensembles and their (typical) decrease in performance anxiety before large ensembles. As might be expected, subjects with higher levels of performance anxiety before solos and small ensembles tend to have smaller increases (or greater decreases) in performance anxiety before large ensembles; those with higher levels of performance anxiety before solos and small ensembles have more opportunity for decreases before large ensembles. Pseudo \\(R^2\\) values are not universally reliable as measures of model performance. Because of the complexity of estimating fixed effects and variance components at various levels of a multilevel model, it is not unusual to encounter situations in which covariates in a Level Two equation for, say, the intercept remain constant (while other aspects of the model change), yet the associated pseudo \\(R^2\\) values differ or are negative. For this reason, pseudo \\(R^2\\) values in multilevel models should be interpreted cautiously. 8.7.3 Adding a covariate at Level Two The initial two-level model described in section 8.5.5 essentially expands upon the random slopes and intercepts model by adding a binary covariate for instrument played at Level Two. We will denote this as Model C: Level One: \\[\\begin{equation} Y_{ij} = a_{i}+b_{i}\\textstyle{LargeEns}_{ij}+\\epsilon_{ij} \\tag{8.15} \\end{equation}\\] Level Two: \\[\\begin{eqnarray*} a_{i} &amp; = &amp; \\alpha_{0}+\\alpha_{1}\\textstyle{Orch}_{i}+u_{i} \\\\ b_{i} &amp; = &amp; \\beta_{0}+\\beta_{1}\\textstyle{Orch}_{i}+v_{i}, \\end{eqnarray*}\\] where \\(\\epsilon_{ij}\\sim N(0,\\sigma^2)\\) and \\[ \\left[ \\begin{array}{c} u_{i} \\\\ v_{i} \\end{array} \\right] \\sim N \\left( \\left[ \\begin{array}{c} 0 \\\\ 0 \\end{array} \\right], \\left[ \\begin{array}{cc} \\sigma_{u}^{2} &amp; \\\\ \\rho\\sigma_{u}\\sigma_{v} &amp; \\sigma_{v}^{2} \\end{array} \\right] \\right). \\] We found that there are no highly significant fixed effects in Model C (other than the intercept). In particular, we have no significant evidence that musicians playing orchestral instruments reported different performance anxiety scores, on average, for solos and small ensembles than keyboardists and vocalists, no evidence of a difference in performance anxiety by performance type for keyboard players and vocalists, and no evidence of an instrument effect in difference between large ensembles and other types. Since no terms were added at Level One when expanding from the random slopes and intercepts model (Model B), no discernable changes should occur in explained within-person variability (although small changes could occur due to numerical estimation procedures used in likelihood-based parameter estimates). However, Model C expanded Model B by using the instrument which a subject plays to model both intercepts and slopes at Level Two. We can use pseudo R-square values for both intercepts and slopes to evaluate the impact on between-person variability of adding instrument to Model B. \\[\\begin{equation} \\textrm{Pseudo }R^2_{L2_u} = \\frac{\\hat{\\sigma}_{u}^{2}(\\textrm{Model B})-\\hat{\\sigma}_{u}^{2}(\\textrm{Model C})}{\\hat{\\sigma}_{u}^{2}(\\textrm{Model B})} = \\frac{6.33-5.66}{6.33} = 0.106 \\tag{8.16} \\end{equation}\\] \\[\\begin{equation} \\textrm{Pseudo }R^2_{L2_v} = \\frac{\\hat{\\sigma}_{v}^{2}(\\textrm{Model B})-\\hat{\\sigma}_{v}^{2}(\\textrm{Model C})}{\\hat{\\sigma}_{v}^{2}(\\textrm{Model B})} = \\frac{0.74-0.45}{0.74} = 0.392 \\tag{8.17} \\end{equation}\\] \\(\\textrm{Pseudo }R^2_{L2_u}\\) describes the improvement in Model C over Model B in explaining subject-to-subject variability in intercepts, and \\(\\textrm{Pseudo }R^2_{L2_v}\\) describes the improvement in Model C over Model B in explaining subject-to-subject variability in slopes. Thus, the addition of instrument at Level Two has decreased the between-person variability in mean performance anxiety before solos and small ensembles by 10.6%, and it has decreased the between-person variability in the effect of large ensembles on performance anxiety by 39.2%. We could also run a “random intercepts” version of Model C, with no error term in the equation for the slope at Level Two (and thus no covariance between errors at Level Two as well): Level One: \\[\\begin{equation} Y_{ij} = a_{i}+b_{i}\\textstyle{LargeEns}_{ij}+\\epsilon_{ij} \\tag{8.18} \\end{equation}\\] Level Two: \\[\\begin{eqnarray*} a_{i} &amp; = &amp; \\alpha_{0}+\\alpha_{1}\\textstyle{Orch}_{i}+u_{i} \\\\ b_{i} &amp; = &amp; \\beta_{0}+\\beta_{1}\\textstyle{Orch}_{i}, \\end{eqnarray*}\\] where \\(\\epsilon_{ij}\\sim N(0,\\sigma^2)\\) and \\(u_{i}\\sim N(0,\\sigma_{u}^{2})\\). The output below contains REML estimates of our 6 parameters from this simplified version of Model C: Formula: na ~ orch + large + orch:large + (1 | id) Random effects: Groups Name Variance Std.Dev. id (Intercept) 5.131 2.265 Residual 21.882 4.678 Number of obs: 497, groups: id, 37 Fixed effects: Estimate Std. Error t value (Intercept) 15.9026 0.6187 25.703 orch 1.7100 0.9131 1.873 large -0.8918 0.8415 -1.060 orch:large -1.4650 1.0880 -1.347 df AIC model.c 8 3002.981 model.c2 6 2999.384 df BIC model.c 8 3036.650 model.c2 6 3024.635 Note that parameter estimates for the remaining 6 fixed effects and variance components closely mirror the corresponding parameter estimates from Model C. In fact, removing the error term on the slope has improved (reduced) both the AIC and BIC measures of overall model performance. Instead of assuming that the large ensemble effects, after accounting for instrument played, vary by individual, we are assuming that large ensemble effect is fixed across subjects. It is not unusual to run a two-level model like this, with an error term on the intercept equation to account for subject-to-subject differences, but with no error terms on other Level Two equations unless there is an a priori reason to allow effects to vary by subject or if the model performs better after building in those additional error terms. 8.8 Additional covariates: model comparison and interpretability Recall that we are particularly interested in this study in Level Two covariates—those subject-specific variables that provide insight into why individuals react differently in anxiety-inducing situations. In section 1.5, we saw evidence that subjects with higher baseline levels of negative emotionality tend to have higher performance anxiety levels prior to performances. Thus, in our next step in model building, we will add negative emotionality as a Level Two predictor to Model C. With this addition, our new model can be expressed as a system of Level One and Level Two models: Level One: \\[\\begin{equation} Y_{ij} = a_{i}+b_{i}\\textstyle{LargeEns}_{ij}+\\epsilon_{ij} \\tag{8.19} \\end{equation}\\] Level Two: \\[\\begin{eqnarray*} a_{i} &amp; = &amp; \\alpha_{0}+\\alpha_{1}\\textstyle{Orch}_{i}+\\alpha_{2}\\textstyle{MPQnem}_{i}+u_{i} \\\\ b_{i} &amp; = &amp; \\beta_{0}+\\beta_{1}\\textstyle{Orch}_{i}+\\beta_{2}\\textstyle{MPQnem}_{i}+v_{i}, \\end{eqnarray*}\\] or as a composite model: \\[\\begin{eqnarray*} Y_{ij} &amp; = &amp; [\\alpha_{0}+\\alpha_{1}\\textstyle{Orch}_{i}+\\alpha_{2}\\textstyle{MPQnem}_{i}+\\beta_{0}\\textstyle{LargeEns}_{ij} \\\\ &amp; &amp; \\mbox{} + \\beta_{1}\\textstyle{Orch}_{i}\\textstyle{LargeEns}_{ij}+\\beta_{2}\\textstyle{MPQnem}_{i}\\textstyle{LargeEns}_{ij}] \\\\ &amp; &amp; \\mbox{} + [u_{i}+v_{i}\\textstyle{LargeEns}_{ij}+\\epsilon_{ij}] \\end{eqnarray*}\\] where error terms are defined as in Model C. From the R output below, we see that, as our exploratory analyses suggested, subjects with higher baseline levels of stress reaction, alienation, and aggression (as measured by the MPQ negative emotionality scale) had significantly higher levels of performance anxiety before solos and small ensembles (t=3.893). They also had somewhat greater differences between large ensembles and other performance types, controlling for instrument (t=-0.575), although this interaction was not statistically significant. Linear mixed model fit by REML [&#39;lmerMod&#39;] Formula: na ~ orch + mpqnem + large + orch:large + mpqnem:large + (large | id) Data: music REML criterion at convergence: 2982.1 Scaled residuals: Min 1Q Median 3Q Max -2.0544 -0.6364 -0.1584 0.4826 4.0530 Random effects: Groups Name Variance Std.Dev. Corr id (Intercept) 3.2857 1.813 large 0.5565 0.746 -0.38 Residual 21.8114 4.670 Number of obs: 497, groups: id, 37 Fixed effects: Estimate Std. Error t value (Intercept) 11.56801 1.22058 9.477 orch 1.00069 0.81713 1.225 mpqnem 0.14823 0.03808 3.892 large -0.28019 1.83412 -0.153 orch:large -0.94927 1.10620 -0.858 mpqnem:large -0.03018 0.05246 -0.575 Correlation of Fixed Effects: (Intr) orch mpqnem large orch:l orch -0.074 mpqnem -0.898 -0.239 large -0.341 0.006 0.321 orch:large -0.006 -0.424 0.144 -0.252 mpqnem:larg 0.357 0.131 -0.422 -0.888 -0.110 Formula: -2.0544 -0.6364 -0.1584 0.4826 4.0530 Random effects: Groups Name Variance Std.Dev. Corr id (Intercept) 3.2857 1.813 large 0.5565 0.746 -0.38 Residual 21.8114 4.670 Number of obs: 497, groups: id, 37 Fixed effects: Estimate Std. Error t value (Intercept) 11.56801 1.22058 9.477 orch 1.00069 0.81713 1.225 mpqnem 0.14823 0.03808 3.892 large -0.28019 1.83412 -0.153 orch:large -0.94927 1.10620 -0.858 8.8.1 Interpretation of parameter estimates Compared to Model C, the directions of the effects of instrument and performance type are consistent, but the effect sizes and levels of significance are reduced because of the relative importance of the negative emotionality term. Interpretations will also change slightly to acknowledge that we have controlled for a covariate. In addition, interpretations of fixed effects involving negative emotionality must acknowledge that this covariate is a continuous measure and not binary like instrument and performance type: \\(\\hat{\\alpha}_{0} = 11.57\\). The estimated mean performance anxiety for solos and small ensembles (large=0) is 11.57 for keyboard players and vocalists (orch=0) with negative emotionality of 0 at baseline (mpqnem=0). Since the minimum negative emotionality score in this study was 11, this interpretation, while technically correct, is not practically meaningful. \\(\\hat{\\alpha}_{1} = 1.00\\). Orchestral instrument players have an estimated mean anxiety level before solos and small ensembles which is 1.00 point higher than keyboardists and vocalists, controlling for the effects of baseline negative emotionality. \\(\\hat{\\alpha}_{2} = 0.15\\). A one point increase in baseline negative emotionality is associated with an estimated 0.15 mean increase in anxiety levels before solos and small ensembles, after controlling for instrument. \\(\\hat{\\beta}_{0} = -0.28\\). Keyboard players and vocalists (orch=0) with baseline negative emotionality levels of 0 (mpqnem=0) have an estimated mean decrease in anxiety level of 0.28 points before large ensemble performances compared to other performance types. \\(\\hat{\\beta}_{1} = -0.95\\). Orchestral instrument players have an estimated mean difference in anxiety levels between large ensembles and other performance types that is 0.95 points lower than the difference for keyboard players and vocalists, controlling for the effects of baseline negative emotionality. For example, orchestral instrumentalists with mpqnem=0 have a larger estimated mean decrease in anxiety level of 1.23 points before large ensemble performances, rather than the mean decrease of 0.28 in those who don’t play orchestral instruments. \\(\\hat{\\beta}_{2} = -0.03\\). A one point increase in baseline negative emotionality is associated with an estimated .03 mean decrease in the difference in anxiety levels for large ensembles and other performance types, after controlling for instrument. Some of the detail in these parameter interpretations can be tricky—describing interaction terms, deciding if a covariate must be fixed at 0 or merely held constant, etc. Often it helps to write out models for special cases to isolate the effects of specific fixed effects. We will consider a few parameter estimates from above and see why the interpretations are written as they are. \\(\\hat{\\alpha}_{0}\\). For solos and small ensembles (LargeEns=0), the following equations describe the fixed effects portion of the composite model for negative affect score for vocalists and keyboardsists (Orch=0) and orchestral instrumentalists (Orch=1): \\[\\begin{eqnarray*} \\textstyle{Orch}=0: &amp; &amp; \\\\ Y_{ij} &amp; = &amp; \\alpha_{0}+\\alpha_{2}\\textstyle{MPQnem}_{i} \\\\ \\textstyle{Orch}=1: &amp; &amp; \\\\ Y_{ij} &amp; = &amp; (\\alpha_{0}+\\alpha_{1})+\\alpha_{2}\\textstyle{MPQnem}_{i} \\end{eqnarray*}\\] Regardless of the subjects’ baseline negative emotionality (MPQnem), \\(\\hat{\\alpha}_{1}\\) represents the estimated difference in performance anxiety between those playing orchestral instruments and others. This interpretation, however, only holds for solos and small ensembles. For large ensembles, the difference between those playing orchestral instruments and others is actually given by \\(\\hat{\\alpha}_{1}+\\hat{\\beta}_{1}\\), holding MPQnem constant (Show!). \\(\\hat{\\beta}_{0}\\). Because LargeEns interacts with both Orch and MPQnem in Model C, \\(\\hat{\\beta}_{0}\\) only describes the estimated difference between large ensembles and other performance types when both Orch=0 and MPQnem=0, thus removing the effects of the interaction terms. If, for instance, Orch=1 and MPQnem=20, then the difference between large ensembles and other performance types is given by \\(\\hat{\\beta}_{0}+\\hat{\\beta}_{1}+20\\hat{\\beta}_{2}\\). \\(\\hat{\\beta}_{1}\\). As with \\(\\hat{\\alpha}_{0}\\), we consider equations describing the fixed effects portion of the composite model for negative affect score for vocalists and keyboardsists (Orch=0) and orchestral instrumentalists (Orch=1), except here we leave LargeEns as an unknown rather than restricting the model to solos and small ensembles: \\[\\begin{eqnarray*} \\textstyle{Orch}=0: &amp; &amp; \\\\ Y_{ij} &amp; = &amp; \\alpha_{0}+\\alpha_{2}\\textstyle{MPQnem}_{i}+\\beta_{0}\\textstyle{LargeEns}_{ij} \\\\ &amp; &amp; \\mbox{} +\\beta_{2}\\textstyle{MPQnem}_{i}\\textstyle{LargeEns}_{ij} \\\\ \\textstyle{Orch}=1: &amp; &amp; \\\\ Y_{ij} &amp; = &amp; (\\alpha_{0}+\\alpha_{1})+\\alpha_{2}\\textstyle{MPQnem}_{i}+(\\beta_{0}+\\beta_{1})\\textstyle{LargeEns}_{ij} \\\\ &amp; &amp; \\mbox{} +\\beta_{2}\\textstyle{MPQnem}_{i}\\textstyle{LargeEns}_{ij} \\end{eqnarray*}\\] As long as baseline negative emotionality is held constant (at any level, not just 0), then \\(\\hat{\\beta}_{1}\\) represents the estimated difference in the large ensemble effect between those playing orchestral instruments and others. 8.8.2 Model comparisons At this point, we might ask: do the two extra fixed effects terms in Model D provide a significant improvement over Model C? Nested models such as these can be tested using a likelihood ratio (drop in deviance) test, as we’ve used in Sections 4.7.2 and 6.6.5.2 with certain generalized linear models. Since we are comparing models nested in their fixed effects, we use full maximum likelihood methods to estimate model parameters, as discussed in section 8.5.4. As expected, the likelihood is larger (and the log-likelihood is less negative) under the larger model (Model D); our test statistic (14.734) is then -2 times the difference in log-likelihood between Models C and D. Comparing the test statistic to a chi-square distribution with 2 degrees of freedom (signifying the number of additional terms in Model D), we obtain a p-value of .0006. Thus, Model D significantly outperforms Model C. # anova() automatically uses ML for LRT tests anova(model.d,model.c) Data: music Models: model.c: na ~ orch + large + orch:large + (large | id) model.d: na ~ orch + mpqnem + large + orch:large + mpqnem:large + (large | model.d: id) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) model.c 8 3007.2 3040.8 -1495.6 2991.2 model.d 10 2996.4 3038.5 -1488.2 2976.4 14.734 2 0.0006319 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Two models, whether they are nested or not, can be compared using AIC and BIC measures, which were first seen in Chapter 1 and later used in evaluating generalized linear models. In this case, the AIC clearly favors Model D (2996.7) over Model C (3007.3), whereas the BIC favors Model D (3038.8) only slightly over Model C (3041.0), since the BIC imposes a stiffer penalty on additional terms and additional model complexity. However, the likelihood ratio test is a more reliable method for comparing nested models. Finally, we note that Model D could be further improved by dropping the negative emotionality by large ensemble interaction term. Not only is the t-value (-0.575) associated with this term of low magnitude, but a likelihood ratio test comparing Model D to a model without mpqnem:large produces an insignificant p-value of 0.5534. model.d1 &lt;- lmer(na ~ orch + mpqnem + large + orch:large + (large|id), data=music) anova(model.d,model.d1) Data: music Models: model.d1: na ~ orch + mpqnem + large + orch:large + (large | id) model.d: na ~ orch + mpqnem + large + orch:large + mpqnem:large + (large | model.d: id) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) model.d1 9 2994.8 3032.7 -1488.4 2976.8 model.d 10 2996.4 3038.5 -1488.2 2976.4 0.3513 1 0.5534 8.9 Center covariates As we observed above, the addition of baseline negative emotionality in Model D did not always produce sensible interpretations of fixed effects. It makes no sense to draw conclusions about performance anxiety levels for subjects with MPQNEM scores of 0 at baseline (as in \\(\\hat{\\beta}_{0}\\)), since the minimum NEM composite score among subjects in this study was 11. In order to produce more meaningful interpretations of parameter estimates and often more stable parameter estimates, it is often wise to center explanatory variables. Centering involves subtracting a fixed value from each observation, where the fixed value represents a meaningful anchor value (e.g., last grade completed = 12; GPA = 3.0). Often, when there’s no pre-defined anchor value, the mean is used to represent a typical case. With this in mind, we can create a new variable \\[\\begin{eqnarray*} \\textstyle{centeredbaselineNEM} = \\textstyle{cmpqnem} = \\textstyle{mpqnem - mean(mpqnem)} = \\textstyle{mpqnem} - 31.63 \\end{eqnarray*}\\] and replace baseline NEM in Model D with its centered version: Formula: Min 1Q Median 3Q Max -2.0544 -0.6364 -0.1584 0.4826 4.0530 Random effects: Groups Name Variance Std.Dev. Corr id (Intercept) 3.2857 1.813 large 0.5565 0.746 -0.38 Residual 21.8114 4.670 Number of obs: 497, groups: id, 37 Fixed effects: Estimate Std. Error t value (Intercept) 16.25679 0.54756 29.689 orch 1.00069 0.81713 1.225 cmpqnem 0.14823 0.03808 3.892 large -1.23484 0.84320 -1.464 As you compare Model D to Model E, you should notice that only two things change – \\(\\hat{\\alpha}_{0}\\) and \\(\\hat{\\beta}_{0}\\). All other parameter estimates—both fixed effects and variance components—remain identical; the basic model is essentially unchanged as well as the amount of variability in anxiety levels explained by the model. \\(\\hat{\\alpha}_{0}\\) and \\(\\hat{\\beta}_{0}\\) are the only two parameter estimates whose interpretations in Model D refer to a specific level of baseline NEM. In fact, the interpretations that held true where NEM=0 (which isn’t possible) now hold true for cmpqnem=0 or when NEM is at its average value of 31.63, which is possible and quite meaningful. Now, parameter estimates using centered baseline NEM in Model E change in value from Model D and produce more useful interpretations: \\(\\hat{\\alpha}_{0} = 16.26\\). The estimated mean performance anxiety for solos and small ensembles (large=0) is 16.26 for keyboard players and vocalists (orch=0) with an average level of negative emotionality at baseline (mpqnem=31.63). \\(\\hat{\\beta}_{0} = -1.23\\). Keyboard players and vocalists (orch=0) with an average level of baseline negative emotionality levels (mpqnem=31.63) have an estimated mean decrease in anxiety level of 1.23 points before large ensemble performances compared to other performance types. 8.10 A potential final model for music performance anxiety We now begin iterating toward a “final model” for these data, on which we will base conclusions. Typical features of a “final multilevel model” include: fixed effects allow one to address primary research questions fixed effects control for important covariates at all levels potential interactions have been investigated variables are centered where interpretations can be enhanced important variance components have been included unnecessary terms have been removed the model tells a “persuasive story parsimoniously” Although the process of reporting and writing up research results often demands the selection of a sensible final model, it’s important to realize that (a) statisticians typically will examine and consider an entire taxonomy of models when formulating conclusions, and (b) different statisticians sometimes select different models as their “final model” for the same set of data. Choice of a “final model” depends on many factors, such as primary research questions, purpose of modeling, tradeoff between parsimony and quality of fitted model, underlying assumptions, etc. So you should be able to defend any final model you select, but you should not feel pressured to find the one and only “correct model”, although most good models will lead to similar conclusions. As we’ve done in previous sections, we can use (a) t-statistics for individual fixed effects when considering adding a single term to an existing model, (b) likelihood ratio tests for comparing nested models which differ by more than one parameter, and (c) model performance measures such as AIC and BIC to compare non-nested models. Below we offer one possible final model for this data—Model F: Level One: \\[\\begin{equation} Y_{ij} = a_{i}+b_{i}\\textstyle{previous}_{ij}+c_{i}\\textstyle{students}_{ij}+ d_{i}\\textstyle{juried}_{ij}+e_{i}\\textstyle{public}_{ij}+f_{i}\\textstyle{solo}_{ij}+\\epsilon_{ij} \\tag{8.20} \\end{equation}\\] Level Two: \\[\\begin{eqnarray*} a_{i} &amp; = &amp; \\alpha_{0}+\\alpha_{1}\\textstyle{mpqpem}_{i}+\\alpha_{2}\\textstyle{mpqab}_{i} + \\alpha_{3}\\textstyle{orch}_{i}+\\alpha_{4}\\textstyle{mpqnem}_{i}+u_{i} \\\\ b_{i} &amp; = &amp; \\beta_{0}+v_{i}, \\\\ c_{i} &amp; = &amp; \\gamma_{0}+w_{i}, \\\\ d_{i} &amp; = &amp; \\delta_{0}+x_{i}, \\\\ e_{i} &amp; = &amp; \\varepsilon_{0}+y_{i}, \\\\ f_{i} &amp; = &amp; \\zeta_{0}+\\zeta_{1}\\textstyle{mpqnem}_{i}+z_{i}, \\end{eqnarray*}\\] where previous is the number of previous diary entries filled out by that individual (diary-1); students, juried, and public are indicator variables created from the audience categorical variable (so that “Instructor” is the reference level in this model); and, solo is 1 if the performance was a solo and 0 if the performance was either a small or large ensemble. In addition, we assume the following variance-covariance structure at Level Two: \\[ \\left[ \\begin{array}{c} u_{i} \\\\ v_{i} \\\\ w_{i} \\\\ x_{i} \\\\ y_{i} \\\\ z_{i} \\end{array} \\right] \\sim N \\left( \\left[ \\begin{array}{c} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{array} \\right], \\left[ \\begin{array}{cccccc} \\sigma_{u}^{2} &amp; &amp; &amp; &amp; &amp; \\\\ \\sigma_{uv} &amp; \\sigma_{v}^{2} &amp; &amp; &amp; &amp; \\\\ \\sigma_{uw} &amp; \\sigma_{vw} &amp; \\sigma_{w}^{2} &amp; &amp; &amp; \\\\ \\sigma_{ux} &amp; \\sigma_{vx} &amp; \\sigma_{wx} &amp; \\sigma_{x}^{2} &amp; &amp; \\\\ \\sigma_{uy} &amp; \\sigma_{vy} &amp; \\sigma_{wy} &amp; \\sigma_{xy} &amp; \\sigma_{y}^{2} &amp; \\\\ \\sigma_{uz} &amp; \\sigma_{vz} &amp; \\sigma_{wz} &amp; \\sigma_{xz} &amp; \\sigma_{yz} &amp; \\sigma_{z}^{2} \\end{array} \\right] \\right). \\] Being able to write out these mammoth variance-covariance matrices is less important than recognizing the number of variance components that must be estimated by our intended model. In this case, we must use likelihood-based methods to obtain estimates for 6 variance terms and 15 correlation terms at Level Two, along with 1 variance term at Level One. Note that the number of correlation terms is equal to the number of unique pairs among Level Two random effects. In later sections we will consider ways to reduce the number of variance components in cases where the number of terms is exploding or the statistical software is struggling to simultaneously find estimates for all model parameters to maximize the likelihood function. From the signs of fixed effects estimates in the R output below, we see that performance anxiety is higher when a musician is performing in front of students, a jury, or the general public rather than their instructor, and it is lower for each additional diary the musician previously filled out. In addition, musicians with lower levels of positive emotionality and higher levels of absorption tend to experience greater performance anxiety, and those who play orchestral instruments experience more performance anxiety than those who play keyboards or sing. Addressing the researchers’ primary hypothesis, after controlling for all these factors, we have significant evidence that musicians with higher levels of negative emotionality experience higher levels of performance anxiety, and that this association is even more pronounced when musicians are performing solos rather than as part of an ensemble group. Here are how a couple of key fixed effects would be interpreted in this final model: \\(\\hat{\\alpha}_{4} = 0.11\\). A one point increase in baseline level of negative emotionality is associated with an estimated 0.11 mean increase in performance anxiety for musicians performing in an ensemble group (solo=0), after controlling for previous diary entries, audience, positive emotionality, absorption, and instrument. \\(\\hat{\\zeta}_{1} = 0.08\\). When musicians play solos, a one point increase in baseline level of negative emotionality is associated with an estimated 0.19 mean increase in performance anxiety, 0.08 points (73%) higher than musicians playing in ensemble groups, controlling for the effects of previous diary entries, audience, positive emotionality, absorption, and instrument. Formula: na ~ previous + students + juried + public + solo + mpqpem + mpqab + orch + mpqnem + mpqnem:solo + (previous + students + juried + public + solo | id) Random effects: Groups Name Variance Std.Dev. Corr id (Intercept) 14.46555 3.8034 previous 0.07078 0.2660 -0.65 students 8.20975 2.8653 -0.63 0.00 juried 18.30280 4.2782 -0.64 -0.12 0.84 public 12.79089 3.5764 -0.83 0.33 0.66 0.57 solo 0.76405 0.8741 -0.67 0.48 0.49 0.21 0.90 Residual 15.28477 3.9096 Number of obs: 497, groups: id, 37 Fixed effects: Estimate Std. Error t value (Intercept) 8.36946 1.91373 4.373 previous -0.14304 0.06248 -2.289 students 3.61116 0.76786 4.703 juried 4.07337 1.03109 3.951 public 3.06425 0.89240 3.434 solo 0.51408 1.39644 0.368 mpqpem -0.08309 0.02408 -3.451 mpqab 0.20379 0.04740 4.299 orch 1.53071 0.58387 2.622 mpqnem 0.11460 0.03591 3.191 solo:mpqnem 0.08302 0.04159 1.996 8.11 Modeling the multilevel structure: is it really necessary? Before going too much further, we should really consider if this multilevel structure has gained us anything over ordinary least squares regression. Sure, multilevel modeling seems more faithful to the inherent structure of the data—performances from the same musician should be more highly correlated than performances from different musicians—but do our conclusions change in a practical sense? Some authors have expressed doubts. For instance Robert Bickel, in his 2007 book, states, “When comparing OLS and multilevel regression results, we may find that differences among coefficient values are inconsequential, and tests of significance may lead to the same decisions. A great deal of effort seems to have yielded precious little gain.” Others, especially economists, advocate simply accounting for the effect of different Level Two observational units (like musicians) with a sequence of indicator variables for those observational units. We contend that (1) fitting multilevel models is a small extension of OLS regression that is not that difficult to conceptualize and fit, especially with the software available today, and (2) using multilevel models to remain faithful to the data structure can lead to different coefficient estimates and often leads to different (and larger) standard error estimates and thus smaller test statistics. Hopefully you’ve seen evidence of (1) in this section already; the rest of this section introduces two small examples to help illustrate (2). Figure 8.15: Hypothetical data from 4 subjects relating number of previous performances to negative affect. The solid black line depicts the overall relationship between previous performances and negative affect as determined by a multilevel model, while the dashed black line depicts the overall relationship as determined by an OLS regression model. Figure 8.15 is based on a simulated data set containing 10 observations from each of 4 subjects. For each subject, the relationship between previous performances and negative affect is linear and negative, with slope approximately -0.5 but different intercepts. The multilevel model (a random intercepts model as described in section 8.7) shows an overall relationship (the solid black line) that’s consistent with the individual subjects—slope around -0.5 with an intercept that appears to average the 4 subjects. Fitting an OLS regression model, however, produces an overall relationship (the dashed black line) that is strongly positive. In this case, by naively fitting the 40 observations as if they were all independent and ignoring subject effects, the OLS regression analysis has gotten the estimated slope of the overall relationship backwards, producing a continuous data version of Simpson’s Paradox. Figure 8.16: Density plots of parameter estimates for the four fixed effects of Model C under both a multilevel model and OLS regression. 1000 sets of simulated data for the 37 subjects in our study were produced using estimated fixed and random effects from Model C. For each set of simulated data, estimates of (a) \\(\\alpha_{0}\\), (b) \\(\\alpha_{1}\\), (c) \\(\\beta_{0}\\), and (d) \\(\\beta_{1}\\) were obtained using both a multilevel and an OLS regression model. Each plot then shows a density plot for the 1000 estimates of the corresponding fixed effect using multilevel modeling vs. a similar density plot for the 1000 estimates using OLS regression. Figure 8.17: Density plots of standard errors of parameter estimates for the four fixed effects of Model C under both a multilevel model and OLS regression. 1000 sets of simulated data for the 37 subjects in our study were produced using estimated fixed and random effects from Model C. For each set of simulated data, estimates of (a) SE(\\(\\alpha_{0}\\)), (b) SE(\\(\\alpha_{1}\\)), (c) SE(\\(\\beta_{0}\\)), and (d) SE(\\(\\beta_{1}\\)) were obtained using both a multilevel and an OLS regression model. Each plot then shows a density plot for the 1000 estimates of the corresponding standard error term using multilevel modeling vs. a similar density plot for the 1000 estimates using OLS regression. Our second example is based upon Model C from section 8.7.3, with single binary predictors at both Level One and Level Two. Using the estimated fixed effects coefficients and variance components from random effects produced in Model C, we generated 1000 sets of simulated data. Each set of simulated data contained 497 observations from 37 subjects just like the original data, with relationships between negative affect and large ensembles and orchestral instruments (along with associated variability) governed by the estimated parameters from Model C. Each set of simulated data was used to fit both a multilevel model and an OLS regression model, and the estimated fixed effects (\\(\\hat{\\alpha}_{0}\\), \\(\\hat{\\alpha}_{1}\\), \\(\\hat{\\beta}_{0}\\), and \\(\\hat{\\beta}_{1}\\)) and their standard errors were saved. Figure 8.16 shows density plots comparing the 1000 estimated fixed effects for each fixed effect from the two modeling approaches; in general, estimates from multilevel modeling and OLS regression tend to agree pretty well, without noticeable bias. Based on coefficient estimates alone, there appears to be no reason to favor multilevel modeling over OLS regression in this example, but Figure 8.17 tells a different story. Figure 8.17 shows density plots comparing the 1000 estimated standard errors associated with each fixed effect from the two modeling approaches; in general, standard errors are markedly larger with multilevel modeling than OLS regression. This is not unusual, since OLS regression assumes all 497 observations are independent while multilevel modeling acknowledges that, with correlated data within subject, there are fewer than 497 independent pieces of data. Therefore, OLS regression can overstate precision, producing t-statistics for each fixed effect that tend to be larger than they should be; the number of significant results in OLS regression are then not reflective of the true structure of the data. 8.12 Notes on Using R (Optional) Initial examination of the data for Case Study 8.2 shows a couple of features that must be noted. First, there are 37 unique study participants, but they are not numbered successively from 1 to 43. The majority of participants filled out 15 diaries, but several filled out fewer (with a minimum of 2); as with participant IDs, diary numbers within participant are not always successively numbered. Finally, missing data is not an issue in this data set, since researchers had already removed participants with only 1 diary entry and performances for which the type was not recorded (of which there were 11). The R code below runs the initial multilevel model in section 8.5.5. Multilevel model notation in R is based on the composite model formulation. Here, the response variable is na, while orch, large, and orch:large represent the fixed effects \\(\\alpha_{1}\\), \\(\\beta_{0}\\), and \\(\\beta_{1}\\), along with the intercept \\(\\alpha_{0}\\) which is included automatically. Note that a colon is used to denote an interaction between two variables. Error terms and their associated variance components are specified in (large|id), which is equivalent to (1+large|id). This specifies two error terms at Level Two (the id level): one corresponding to the intercept (\\(u_{i}\\)) and one corresponding to the large ensemble effect (\\(v_{i}\\)); the multilevel model will then automatically include a variance for each error term in addition to the covariance between the two error terms. A variance associated with a Level One error term is also automatically included in the multilevel model. Note that there are ways to override the automatic inclusion of certain variance components; for example, (0+large|id) would not include an error term for the intercept (and therefore no covariance term at Level Two either). model0 &lt;- lmer(na ~ orch + large + orch:large + (large|id), REML=T, data=music) summary(model0) 8.13 Exercises 8.13.1 Conceptual Exercises Housing Prices. Brown and Uyar (2004) describe “A Hierarchical Linear Model Approach for Assessing the Effects of House and Neighborhood Characteristics on Housing Prices”. Based on the title of their paper: (a) give the observational units at Level One and Level Two, and (b) list potential explanatory variables at both Level One and Level Two. In the preceding problem, why can’t we assume all houses in the data set are independent? What would be the potential implications to our analysis of assuming independence among houses? In the preceding problem, for each of the following sets of predictors: (a) write out the two-level model for predicting housing prices, (b) write out the corresponding composite model, and (c) determine how many model parameters (fixed effects and variance components) must be estimated. Square footage, number of bedrooms Median neighborhood income, rating of neighborhood schools Square footage, number of bedrooms, age of house, median neighborhood housing price Square footage, median neighborhood income, rating of neighborhood schools, median neighborhood housing price Music Performance Anxiety. Describe a situation in which the two plots in Figure 8.7 might tell different stories. Explain the difference between \\(a_{i}\\) in Equation (8.2) and \\(\\hat{a}_{i}\\) in Equation (8.4). Why is the contour plot for multivariate normal density in Figure 8.12 (b) tilted from southwest to northeast, but the contour plot in Figure 8.12 (a) is not tilted? In Table 8.3, note that the standard errors associated with estimated coefficients under independence are lower than standard errors under alternative analysis methods. Why is that often the case? Why is Model A (Section 8.6.2) sometimes called the “unconditional means model”? Why is it also sometimes called the “random intercepts model”? Are these two labels consistent with each other? Consider adding an indicator variable in Model B (Section 8.7.1) for Small Ensemble performances. Write out the two-level model for performance anxiety, Write out the corresponding composite model, Determine how many model parameters (fixed effects and variance components) must be estimated, and Explain how the interpretation for the coefficient in front of Large Ensembles would change. Give a short rule in your own words describing when an interpretation of an estimated coefficient should “hold constant” another covariate or “set to 0” that covariate (see Section 8.8.1). The interpretation of \\(\\hat{\\alpha}_{0}\\) in Section 8.8.1 claims that “This interpretation, however, only holds for solos and small ensembles. For large ensembles, the difference between those playing orchestral instruments and others is actually given by \\(\\hat{\\alpha}_{1}+\\hat{\\beta}_{1}\\), holding MPQNEM constant.” Show that this claim is true. Explain how the interpretations of the following parameter estimates change (or don’t change) as we change our model: \\(\\hat{\\alpha}_{0}\\) from Model A to B to C to D to E \\(\\hat{\\beta}_{1}\\) from Model B to C to D to E \\(\\hat{\\alpha}_{1}\\) from Model C to D to E \\(\\hat{\\beta}_{1}\\) from Model C to D to E \\(\\hat{\\sigma}_{u}\\) from Model A to B to C to D to E \\(\\hat{\\sigma}_{v}\\) from Model B to C to D to E When moving from Model B to Model C in Section 8.7.3, \\(\\hat{\\sigma}_{u}^{2}\\) increases slightly. Why might this have occurred? Interpret other estimated parameters from Model F beyond those interpreted in Section 8.10: \\(\\hat{\\alpha}_{0}\\), \\(\\hat{\\alpha}_{2}\\), \\(\\hat{\\alpha}_{3}\\), \\(\\hat{\\beta}_{0}\\), \\(\\hat{\\gamma}_{0}\\), \\(\\hat{\\zeta}_{0}\\), \\(\\hat{\\rho}_{wx}\\), \\(\\hat{\\sigma}^{2}\\), \\(\\hat{\\sigma}_{u}^{2}\\), and \\(\\hat{\\sigma}_{z}^{2}\\). Explain Figure 8.15 in your own words. Why would OLS regression produce a misleading analysis in this case, but multilevel models would not? Summarize Figures 8.16 and 8.17 in your own words. 8.13.2 Guided Exercise Music Performance Joy. In this chapter, we studied models for predicting music performance anxiety, as measured by the negative affect scale from the PANAS instrument. Now we will examine models for predicting the happiness of musicians prior to performances, as measured by the positive affect scale from the PANAS instrument. To begin, run the following models: Model A = unconditional means model Model B = indicator for instructor audience type and indicator for student audience type at Level One; no Level Two predictors Model C = indicator for instructor audience type and indicator for student audience type at Level One; centered MPQ absorption subscale as Level Two predictor for intercept and all slope terms Model D = indicator for instructor audience type and indicator for student audience type at Level One; centered MPQ absorption subscale and a male indicator as Level Two predictors for intercept and all slope terms Perform an exploratory data analysis by comparing positive affect (happiness) to Level One and Level Two covariates using appropriate graphs. Comment on interesting trends, supporting your comments with appropriate summary statistics. Report estimated fixed effects and variance components from Model A, using proper notation from this section (no interpretations required). Also report and interpret an intraclass correlation coefficient. Report estimated fixed effects and variance components from Model B, using proper notation from this section. Interpret your MLE estimates for \\(\\hat{\\alpha}_{0}\\) (the intercept), \\(\\hat{\\beta}_{1}\\) (the instructor indicator), and \\(\\hat{\\sigma}_{u}\\) (the Level Two standard deviation for the intercept). Also report and interpret an appropriate pseudo-Rsquare value. Write out Model C, using both separate Level One and Level Two models as well as a composite model. Be sure to express distributions for error terms. How many parameters must be estimated in Model C? Report and interpret the following parameter estimates from Model C: \\(\\hat{\\alpha}_{0}\\), \\(\\hat{\\alpha}_{1}\\), \\(\\hat{\\gamma}_{0}\\), \\(\\hat{\\beta}_{1}\\), \\(\\hat{\\sigma}_{u}\\), \\(\\hat{\\sigma}_{v}\\), and \\(\\hat{\\rho}_{uv}\\). Interpretations for variance components should be done in terms of standard deviations and correlation coefficients. Report and interpret the same parameter estimates listed above from Model D. In each case, the new interpretation should involve a small modification of your interpretation from Model C. Use underlines or highlights to denote the part of the Model D interpretation that differs from the Model C interpretation. Also report and interpret the following parameter estimates from Model D: \\(\\hat{\\alpha}_{2}\\) and \\(\\hat{\\beta}_{2}\\). Use a deviance statistic (likelihood ratio test) to compare Model C vs. Model D. Give a test statistic and p-value, then state a conclusion. Also compare Models C and D with appropriate pseudo-Rsquare value(s) and with AIC and BIC statistics. 8.13.3 Open-ended Exercises Project 5183. The Colorado Rockies, a Major League Baseball team, instigated a radical experiment on June 20th, 2012. Hopelessly out of contention for the playoffs and struggling again with their pitching, the Rockies decided to limit their starting pitchers to 75 pitches from June 20th until the end of the year with the hope of improving a struggling starting rotation, teaching pitchers how to pitch to contact (which results in low pitch counts), and at the same time trying to conserve young arms. Data has shown that, as a game progresses, fatigue becomes a big factor in a pitcher’s performance; if a pitcher has to tweak his mechanics to try to make up for a fatigued body, injuries can often occur. In addition, pitchers often struggle as they begin facing the same batters over again later in games. The Rockies called their experiment “Project 5183” to acknowledge the altitude at Coors Field, their home ballpark, and the havoc that high altitude can wreak on pitchers. A team of students (Lampert, Friedrich, Sturtz) collected 2012 data on Rockies pitchers from FanGraphs to evaluate Project 5183. In a successful experiment, Colorado pitchers on a strict limit of 75 pitches would throw more strikes and yet record fewer strikeouts (pitching to contact rather than taking more pitches to attempt to strike batters out). Different theories explain whether these pitchers would throw harder (since they don’t have to save themselves) or throw slower (in order to throw more strikes). But the end result the Rockies hoped to observe was that their pitchers pitch better (allow fewer runs to the opponent) with a pitch limit. The data set contains information for 7 starting pitchers who started at least one game before June 20th (without a pitch limit) and at least one game after June 20th (with a limit of 75 pitches). Key response variables include: vFA = average fastball velocity K.9 = strikeouts per nine innings ERA = earned runs per nine innings Pitpct = percentage of strikes thrown The primary explanatory variable of interest is PCL (an indicator variable for if a pitch count limit is in effect). Other potential confounding variables that may be important to control for include Coors (whether or not the game was played in Coors Field, where more runs are often scored because of the high altitude and thin air) and Age of the pitcher. Write a short report summarizing the results of Project 5183. (You may notice a few variance components with unusual estimates, such as an estimated variance of 0 or an estimated correlation of 1. These estimates have encountered boundary constraints; we will learn how to deal with these situations in Section 10.5. For now ignore these variance components; the fixed effects coefficients are still reliable and their interpretations valid.) "],
["ch-lon.html", "Chapter 9 Two Level Longitudinal Data 9.1 Learning objectives 9.2 Case study: Charter schools 9.3 Initial Exploratory Analyses 9.4 Preliminary two-stage modeling 9.5 Initial models 9.6 Building to a final model 9.7 Covariance structure among observations 9.8 Notes on Using R (Optional) 9.9 Exercises", " Chapter 9 Two Level Longitudinal Data 9.1 Learning objectives After finishing this chapter, you should be able to: Recognize longitudinal data as a special case of multilevel data, with time at Level One. Consider patterns of missingness and implications of that missing data on multilevel analyses. Apply exploratory data analysis techniques specific to longitudinal data. Build and understand a taxonomy of models for longitudinal data. Interpret model parameters in multilevel models with time at Level One. Compare models, both nested and not, with appropriate statistical tests and summary statistics. Consider different ways of modeling the variance-covariance structure in longitudinal data. 9.2 Case study: Charter schools Charter schools were first introduced in the state of Minnesota in 1991. Since then, charter schools have begun appearing all over the United States. While publicly funded, a unique feature of charter schools is their independence from many of the regulations that are present in the public school systems of their respective city or state. Thus, charters will often extend the school days or year and tend to offer non-traditional techniques and styles of instruction and learning. One example of this unique schedule structure is the KIPP (Knowledge is Power Program) Stand Academy in Minneapolis, MN. KIPP stresses longer days and better partnerships with parents, and they claim that 80% of their students go to college from a population where 87% qualify for free and reduced lunch and 95% are African-American or Latino. However, the larger question is whether or not charter schools are out-performing non-charter public schools in general. Because of the relative youthfulness of charter schools, data has just begun to be collected to evaluate the performance of charter versus non-charter schools and some of the factors that influence a school’s performance. Along these lines, we will examine data collected by the Minnesota Department of Education for all Minnesota schools during the years 2008-2010. Comparisons of student performance in charter schools versus public schools have produced conflicting results, potentially as a result of the strong differences in the structure and population of the student bodies that represent the two types of schools. A study by the Policy and Program Studies Service of five states found that charter schools are less likely to meet state performance standards than conventional public schools (US Department of Education, 2004). However, Witte (2007) performed a statistical analysis comparing Wisconsin charter and non-charter schools and found that average achievement test scores were significantly higher in charter schools compared to non-charter schools, after controlling for demographic variables such as the percentage of white students. In addition, a study of California students who took the Stanford 9 exam from 1998 through 2002 found that charter schools, on average, were performing at the same level as conventional public schools (Buddin and Zimmer 2005). Although school performance is difficult to quantify with a single measure, for illustration purposes in this chapter, we will focus on that aspect of school performance measured by the math portion of the Minnesota Comprehensive Assessment (MCA-II) data for 6th grade students enrolled in 618 different Minnesota schools during the years 2008, 2009, and 2010. Similar comparisons could obviously be conducted for other grade levels or modes of assessment. As described in Green et al. (2003), it is very challenging to compare charter and public non-charter schools, as charter schools are often designed to target or attract specific populations of students. Without accounting for differences in student populations, comparisons lose meaning. With the assistance of multiple school-specific predictors, we will attempt to model sixth grade math MCA-II scores of Minnesota schools, focusing on the differences between charter and public non-charter school performances. In the process, we hope to answer the following research questions: Which factors most influence a school’s performance in MCA testing? How do the average math MCA-II scores for 6th graders enrolled in charter schools differ from scores for students who attend non-charter public schools? Do these differences persist after accounting for differences in student populations? Are there differences in yearly improvement between charter and non-charter public schools? 9.3 Initial Exploratory Analyses 9.3.1 Data organization Key variables which we will examine to address the research questions above are: schoolid = includes district type, district number, and school number schoolName = name of school urban = is the school in an urban (1) or rural (0) location? charter = is the school a charter school (1) or a non-charter public school (0)? schPctnonw = proportion of non-white students in a school (based on 2010 figures) schPctsped = proportion of special education students in a school (based on 2010 figures) schPctfree = proportion of students who receive free or reduced lunches in a school (based on 2010 figures). This serves as a measure of poverty among school families. MathAvgScore.0 = average MCA-II math score for all sixth grade students in a school in 2008 MathAvgScore.1 = average MCA-II math score for all sixth grade students in a school in 2009 MathAvgScore.2 = average MCA-II math score for all sixth grade students in a school in 2010 This data is stored in WIDE format, with one row per school, as illustrated in Table 9.1. Table 9.1: The first six observations in the wide data set for the Charter Schools case study. Table continues below schoolid schoolName urban charter Dtype 1 Dnum 1 Snum 2 RIPPLESIDE ELEMENTARY 0 0 Dtype 1 Dnum 100 Snum 1 WRENSHALL ELEMENTARY 0 0 Dtype 1 Dnum 108 Snum 30 CENTRAL MIDDLE 0 0 Dtype 1 Dnum 11 Snum 121 SANDBURG MIDDLE 1 0 Dtype 1 Dnum 11 Snum 193 OAK VIEW MIDDLE 1 0 Dtype 1 Dnum 11 Snum 195 ROOSEVELT MIDDLE 1 0 Table continues below schPctnonw schPctsped schPctfree MathAvgScore.0 MathAvgScore.1 0 0.1176 0.3627 652.8 656.6 0.0303 0.1515 0.4242 646.9 645.3 0.07692 0.1231 0.2615 654.7 658.5 0.09774 0.08271 0.2481 656.4 656.8 0.05379 0.09535 0.1418 657.7 658.2 0.1234 0.08861 0.2405 655.9 659.1 MathAvgScore.2 652.6 651.9 659.7 659.9 659.8 660.3 Table 9.2: A frequency table of missing data patterns. The number of schools with a particular missing data pattern are listed in the left column; the remaining columns of 0’s and 1’s describe the missing data pattern, with 0 indicating a missing value. Some covariates that are present for every school are not listed. The bottom row gives the number of schools with missing values for specific variables; the last entry indicates that 121 total observations were missing. charter MathAvgScore.2 MathAvgScore.1 MathAvgScore.0 540 1 1 1 1 0 25 1 1 1 0 1 4 1 1 0 1 1 6 1 0 1 1 1 35 1 1 0 0 2 1 1 0 1 0 2 7 1 0 0 1 2 0 14 46 61 121 For most statistical analyses, it will be advantageous to convert WIDE format to LONG format, with one row per year per school. To make this conversion, we will have to create a time variable, which under the LONG format is very flexible—each school can have a different number of and differently-spaced time points, and they can even have predictors which vary over time. Details for making this conversion in R can be found in Section 9.8, and the form of the LONG data in this study is exhibited in the next section. 9.3.2 Missing data In this case, before we convert our data to LONG form, we should first address problems with missing data. Missing data is a common phenomenon in longitudinal studies. For instance, it could arise if a new school was started during the observation period, a school was shut down during the observation period, or no results were reported in a given year. Dealing with missing data in a statistical analysis is not trivial, but fortunately many multilevel packages (including the lme4 package in R) are adept at handling missing data. First, we must understand the extent and nature of missing data in our study. Table 9.2, where 1 indicates presence of a variable and 0 indicates a missing value for a particular variable, is a helpful starting point. Among our 618 schools, 540 had complete data (all covariates and math scores for all three years), 25 were missing a math score for 2008, 35 were missing math scores in both 2008 and 2009, etc. Statisticians have devised different strategies for handling missing data; a few common approaches are described briefly here: Include only schools with complete data. This is the cleanest approach analytically; however, ignoring data from 12.6% of the study’s schools (since 78 of the 618 schools had incomplete data) means that a large amount of potentially useful data is being thrown away. In addition, this approach creates potential issues with informative missingness. Informative missingness occurs when a school’s lack of scores is not a random phenomenon but provides information about the effectiveness of the school type (e.g., a school closes because of low test scores). Last observation carried forward. Each school’s last math score is analyzed as a univariate response, whether the last measurement was taken in 2008, 2009, or 2010. With this approach, data from all schools can be used, and analyses can be conducted with traditional methods assuming independent responses. This approach is sometimes used in clinical trials because it tends to be conservative, setting a higher bar for showing that a new therapy is significantly better than a traditional therapy. Of course, we must assume that a school’s 2008 score is representative of their 2010 score. In addition, information about trajectories over time is thrown away. Imputation of missing observations. Many methods have been developed for sensibly “filling in” missing observations, using imputation models which base imputed data on subjects with similar covariate profiles and on typical observed time trends. Once an imputed data set is created (or several imputed data sets), analyses can proceed with complete data methods that are easier to apply. Risks with the imputation approach include misrepresenting missing observations and overstating precision in final results. Apply multilevel methods, which use available data to estimate patterns over time by school and then combine those school estimates in a way that recognizes that time trends for schools with complete data are more precise than time trends for schools with fewer measurements. Laird (1988) demonstrates that multilevel models are valid under the fairly unrestrictive condition that the probability of missingness cannot depend on any unobserved predictors or the response. This is the approach we will follow in the remainder of the text. Now, we are ready to create our LONG data set. Fortunately, many packages (including R) have built-in functions for easing this conversion. The resulting LONG data set is shown in Table 9.3, where year08 measures the number of years since 2008: Table 9.3: The first six observations in the long data set for the Charter Schools case study; these lines correspond to the first two observations from the wide data set illustrated in Table 9.1. schoolName charter schPctsped schPctfree year08 MathAvgScore 1 RIPPLESIDE ELEMENTARY 0 0.1176471 0.3627451 0 652.8 619 RIPPLESIDE ELEMENTARY 0 0.1176471 0.3627451 1 656.6 1237 RIPPLESIDE ELEMENTARY 0 0.1176471 0.3627451 2 652.6 2 WRENSHALL ELEMENTARY 0 0.1515152 0.4242424 0 646.9 620 WRENSHALL ELEMENTARY 0 0.1515152 0.4242424 1 645.3 1238 WRENSHALL ELEMENTARY 0 0.1515152 0.4242424 2 651.9 9.3.3 Exploratory analyses for general multilevel models Notice the longitudinal structure of our data—we have up to three measurements of test scores at different time points for each of our 618 schools. With this structure, we can address questions at two levels: Within school—changes over time Between schools—effects of school-specific covariates (charter or non-charter, urban or rural, percent free and reduced lunch, percent special education, and percent non-white) on 2008 math scores and rate of change between 2008 and 2010. As with any statistical analysis, it is vitally important to begin with graphical and numerical summaries of important variables and relationships between variables. We’ll begin with initial exploratory analyses that we introduced in the previous chapter, noting that we have no Level One covariates other than time at this point (potential covariates at this level may have included measures of the number of students tested or funds available per student). We will, however, consider the Level Two variables of charter or non-charter, urban or rural, percent free and reduced lunch, percent special education, and percent non-white. Although covariates such as percent free and reduced lunch may vary slightly from year to year within a school, the larger and more important differences tend to occur between schools, so we used mean percent free and reduced lunch for a school in 2010 as a Level Two variable. As in Chapter 8, we can conduct initial investigations of relationships between Level Two covariates and test scores in two ways. First, we can use all \\(1733\\) observations to investigate relationships of Level Two covariates with test scores; although these plots will contain dependent points, since each school is represented by up to three years of test score data, general patterns exhibited in these plots tend to be real. Second, we can calculate mean scores across all years for each of the 618 schools; while we lose some information with this approach, we can more easily consider each plotted point to be independent. Typically, both types of exploratory plots illustrate similar relationships, and in this case, both approaches are so similar that we will only show plots using the second approach, with one observation per school. Figure 9.1 shows the distribution of MCA math test scores as somewhat left skewed. MCA test scores for sixth graders are scaled to fall between 600 and 700, where scores above 650 for individual students indicate “meeting standards”. Thus, schools with averages below 650 will often have increased incentive to improve their scores the following year. When we refer to the “math score” for a particular school in a particular year, we will assume that score represents the average for all sixth graders at that school. In Figure 9.2, we see that test scores are generally higher for both schools in rural areas and for public non-charter schools. Note that in this data set there are 237 schools in rural areas and 381 schools in urban areas, as well as 545 public non-charter schools and 73 charter schools. In addition, we can see in Figure 9.3 that schools tend to have lower math scores if they have higher percentages of students with free and reduced lunch, with special education needs, or who are non-white. Figure 9.1: Histogram of mean sixth grade MCA math test scores over the years 2008-2010 for 618 Minnesota schools. Figure 9.2: Boxplots of categorical Level Two covariates vs. average MCA math scores. Plot (a) shows charter vs. public non-charter schools, while plot (b) shows urban vs. rural schools. Figure 9.3: Scatterplots of average MCA math scores by (a) percent free and reduced lunch, (b) percent special education, and (c) percent non-white in a school. 9.3.4 Exploratory analyses for longitudinal data In addition to the initial exploratory analyses above, longitudinal data—multilevel data with time at Level One—calls for further plots and summaries that describe time trends within and across individuals. For example, we can examine trends over time within individual schools. Figure 9.4 provides a lattice plot illustrating trends over time for the first \\(24\\) schools in the data set. We note differences among schools in starting point (test scores in 2008), slope (change in test scores over the three year period), and form of the relationship. These differences among schools are nicely illustrated in so-called spaghetti plots such as Figure 9.5, which overlays the individual schools’ time trends (for the math test scores) from Figure 9.4 on a single set of axes. In order to illustrate the overall time trend without making global assumptions about the form of the relationship, we overlaid in bold a nonparametric fitted curve through a loess smoother. LOESS comes from “locally weighted scatterplot smoother”, in which a low-degree polynomial is fit to each data point using weighted regression techniques, where nearby points receive greater weight. LOESS is a computationally intensive method which performs especially well with larger sets of data, although ideally there would be a greater diversity of x-values than the three time points we have. In this case, the loess smoother follows very closely to a linear trend, indicating that assuming a linear increase in test scores over the three year period is probably a reasonable simplifying assumption. To further examine the hypothesis that linearity would provide a reasonable approximation to the form of the individual time trends in most cases, Figure 9.6 shows a lattice plot containing linear fits through ordinary least squares rather than connected time points as in Figure 9.4. Figure 9.4: Lattice plot by school of math scores over time for the first 24 schools in the data set. Figure 9.5: Spaghetti plot of math scores over time by school, for all the charter schools and a random sample of public non-charter schools, with overall fit using loess (bold). Figure 9.6: Lattice plot by school of math scores over time with linear fit for the first 24 schools in the data set. Figure 9.7: Spaghetti plot showing time trends for each school by school type, for a random sample of charter schools (left) and public non-charter schools (right), with overall fits using loess (bold). Figure 9.8: Spaghetti plot showing time trends for each school by quartiles of percent free and reduced lunch, with loess fits. Just as we explored the relationship between our response (average math scores) and important covariates in Section 9.3.3, we can now examine the relationships between time trends by school and important covariates. For instance, Figure 9.7 shows that charter schools had math scores that were lower on average than public non-charter schools and more variable. This type of plot is sometimes called a trellis graph, since it displays a grid of smaller charts with consistent scales, where each smaller chart represents a condition–an item in a category. Trends over time by school type are denoted by bold loess curves. Public non-charter schools have higher scores across all years; both school types show little growth between 2008 and 2009, but greater growth between 2009 and 2010, especially charter schools. Exploratory analyses like this can be repeated for other covariates, such as percent free and reduced lunch in Figure 9.8. The trellis plot automatically divides schools into four groups based on quartiles of their percent free and reduced lunch, and we see that schools with lower percentages of free and reduced lunch students tend to have higher math scores and less variability. Across all levels of free and reduced lunch, we see greater gains between 2009 and 2010 than between 2008 and 2009. 9.4 Preliminary two-stage modeling 9.4.1 Linear trends within schools Even though we know that every school’s math test scores were not strictly linearly increasing or decreasing over the observation period, a linear model for individual time trends is often a simple but reasonable way to model data. One advantage of using a linear model within school is that each school’s data points can be summarized with two summary statistics—an intercept and a slope (obviously, this is an even bigger advantage when there are more observations over time per school). For instance, we see in Figure 9.6 that sixth graders from the school depicted in the top right slot slowly increased math scores over the three year observation period, while students from the school depicted in the fourth column of the top row generally experienced decreasing math scores over the same period. As a whole, the linear model fits individual trends pretty well, and many schools appear to have slowly increasing math scores over time, as researchers in this study may have hypothesized. Another advantage of assuming a linear trend at Level One (within schools) is that we can examine summary statistics across schools. Both the intercept and slope are meaningful for each school: the intercept conveys the school’s math score in 2008, while the slope conveys the school’s average yearly increase or decrease in math scores over the three year period. Figure 9.9 shows that point estimates and uncertainty surrounding individual estimates of intercepts and slopes vary considerably. In addition, we can generate summary statistics and histograms for the 618 intercepts and slopes produced by fitting linear regression models at Level One, in addition to R-square values which describe the strength of fit of the linear model for each school (Figure 9.10). For our 618 schools, the mean math score for 2008 was 651.4 (SD=7.28), and the mean yearly rate of change in math scores over the three year period was 1.30 (SD=2.51). We can further examine the relationship between schools’ intercepts and slopes. Figure 9.11 shows a general decreasing trend, suggesting that schools with lower 2008 test scores tend to have greater growth in scores between 2008 and 2010 (potentially because those schools have more room for improvement); this trend is supported with a correlation coefficient of -0.32 between fitted intercepts and slopes. Note that, with only 3 or fewer observations for each school, extreme or intractable values for the slope and R-square are possible. For example, slopes cannot be estimated for those schools with just a single test score, R-square values cannot be calculated for those schools with no variability in test scores between 2008 and 2010, and R-square values must be 1 for those schools with only two test scores. Figure 9.9: Point estimates and 95% confidence intervals for (a) intercepts and (b) slopes by school, for the first 24 schools in the data set. Figure 9.10: Histograms for (a) intercepts, (b) slopes, and (c) R-square values from fitted regression lines by school. Figure 9.11: Scatterplot showing the relationship between intercepts and slopes from fitted regression lines by school. 9.4.2 Effects of level two covariates on linear time trends Summarizing trends over time within schools is typically only a start, however. Most of the primary research questions from this study involve comparisons among schools, such as: (a) are there significant differences between charter schools and public non-charter schools, and (b) do any differences between charter schools and public schools change with percent free and reduced lunch, percent special education, or location? These are Level Two questions, and we can begin to explore these questions by graphically examining the effects of school-level variables on schools’ linear time trends. By school-level variables, we are referring to those covariates that differ by school but are not dependent on time. For example school type (charter or public non-charter), urban or rural location, percent non-white, percent special education, and percent free and reduced lunch are all variables which differ by school but which don’t change over time, at least as they were assessed in this study. Variables which would be time-dependent include quantities such as per pupil funding and reading scores. Figure 9.12 shows differences in the average time trends by school type, using estimated intercepts and slopes to support observations from the spaghetti plots in Figure 9.7. Based on intercepts, charter schools have lower math scores, on average, in \\(2008\\) than public non-charter schools. Based on slopes, however, charter schools tend to improve their math scores at a slightly faster rate than public schools, especially at the seventy-fifth percentile and above. By the end of the three year observation period we would nevertheless expect charter schools to have lower average math scores than public schools. For another exploratory perspective on school type comparisons, we can examine differences between school types with respect to math scores in 2008 and math scores in 2010. As expected, boxplots by school type (Figure 9.13) show clearly lower math scores for charter schools in 2008, but differences are slightly less dramatic in 2010. Figure 9.12: Boxplots of (a) intercepts and (b) slopes by school type (charter vs. public non-charter). Figure 9.13: Boxplots of (a) 2008 and (b) 2010 math scores by school type (charter vs. public non-charter). Any initial exploratory analyses should also investigate effects of potential confounding variables such as school demographics and location. If we discover, for instance, that those schools with higher levels of poverty (measured by the percentage of students receiving free and reduced lunch) display lower test scores in 2008 but greater improvements between 2008 and 2010, then we might be able to use percentage of free and reduced lunch in statistical modeling of intercepts and slopes, leading to more precise estimates of the charter school effects on these two outcomes. In addition, we should also look for any interaction with school type—any evidence that the difference between charter and non-charter schools changes based on the level of a confounding variable. For example, do charter schools perform better relative to non-charter schools when there is a large percentage of non-white students at a school? With a confounding variable such as percentage of free and reduced lunch, we will treat this variable as continuous to produce the most powerful exploratory analyses. We can begin by examining boxplots of free and reduced lunch percentage against school type (Figure 9.14). We observe that charter schools tend to have greater percentages of free and reduced lunch students as well as greater school-to-school variability. Next, we can use scatterplots to graphically illustrate the relationships between free and reduced lunch percentages and significant outcomes such as intercept and slope (also Figure 9.14). In this study, it appears that schools with higher levels of free and reduced lunch (i.e., greater poverty) tend to have lower math scores in 2008, but there is little evidence of a relationship between levels of free and reduced lunch and improvements in test scores between 2008 and 2010. These observations are supported with correlation coefficients between percent free and reduced lunch and intercepts (r=-0.61) and slopes (r=-0.06). A less powerful but occasionally informative way to look at the effect of a continuous confounder on an outcome variables is by creating a categorical variable out of the confounder. For instance, we could classify any school with a percentage of free and reduced lunch students above the median as having a high percentage of free and reduced lunch students and all other schools as having a low percentage of free and reduced lunch students. Then we could examine a possible interaction between percent free and reduced lunch and school type through a series of four boxplots (Figure 9.15). In fact, these boxplots suggest that the gap between charter and public non-charter schools in 2008 was greater in schools with a high percentage of free and reduced lunch students, while the difference in rate of change in test scores between charter and public non-charter schools appeared similar for high and low levels of free and reduced lunch. We will investigate these trends more thoroughly with statistical modeling. Figure 9.14: (a) Boxplot of percent free and reduced lunch by school type (charter vs. public non-charter), along with scatterplots of (b) intercepts and (c) slopes from fitted regression lines by school vs. percent free and reduced lunch. Figure 9.15: Boxplots of (a) intercepts and (b) slopes from fitted regression lines by school vs. school type (charter vs. public non-charter), separated by high and low levels of percent free and reduced lunch. The effect of other confounding variables (e.g., percent non-white, percent special education, urban or rural location) can be investigated in a similar fashion to free and reduced lunch percentage, both in terms of main effect (variability in outcomes such as slope and intercept which can be explained by the confounding variable) and interaction with school type (ability of the confounding variable to explain differences between charter and public non-charter schools). We leave these explorations as an exercise. 9.4.3 Error structure within schools Finally, with longitudinal data it is important to investigate the error variance-covariance structure of data collected within a school (the Level Two observational unit). In multilevel data, as in the examples we introduced in Chapter ??, we suspect observations within group (like a school) to be correlated, and we strive to model that correlation. When the data within group is collected over time, we often see distinct patterns in the residuals that can be modeled—correlations which decrease systematically as the time interval increases, variances that change over time, correlation structure that depends on a covariate, etc. A first step in modeling the error variance-covariance structure is the production of an exploratory plot such as Figure 9.16. To generate this plot, we begin by modeling MCA math score as a linear function of time using all 1733 observations and ignoring the school variable. This population (marginal) trend is illustrated in Figure 9.5 and is given by: \\[\\begin{equation} \\hat{Y}_{ij}=651.69+1.20\\textstyle{Time}_{ij}, \\tag{9.1} \\end{equation}\\] where \\(\\hat{Y}_{ij}\\) is the predicted math score of the \\(i^{th}\\) school at time \\(j\\), where time \\(j\\) is the number of years since 2008. In this model, the predicted math score will be identical for all schools at a given time point \\(j\\). Residuals \\(Y_{ij}-\\hat{Y}_{ij}\\) are then calculated for each observation, measuring the difference between actual math score and the average overall time trend. Figure 9.16 then combines three pieces of information: the upper right triangle contains correlation coefficients for residuals between pairs of years, the diagonal contains histograms of residuals at each time point, and the lower left triangle contains scatterplots of residuals from two different years. In our case, we see that correlation between residuals from adjacent years is strongly positive (0.81-0.83) and does not drop off greatly as the time interval between years increases. Figure 9.16: Correlation structure within school. The upper right contains correlation coefficients between residuals at pairs of time points, the lower left contains scatterplots of the residuals at time point pairs, and the diagonal contains histograms of residuals at each of the three time points. 9.5 Initial models Throughout the exploratory analysis phase, our original research questions have guided our work, and now with modeling we return to familiar questions such as: are differences between charter and public non-charter schools (in intercept, in slope, in 2010 math score) statistically significant? are differences between school types statistically significant, even after accounting for school demographics and location? do charter schools offer any measurable benefit over non-charter public schools, either overall or within certain subgroups of schools based on demographics or location? As you might expect, answers to these questions will arise from proper consideration of variability and properly identified statistical models. As in Chapter 8, we will begin model fitting with some simple, preliminary models, in part to establish a baseline for evaluating larger models. Then, we can build toward a final model for inference by attempting to add important covariates, centering certain variables, and checking assumptions. 9.5.1 Unconditional means model In the multilevel context, we almost always begin with the unconditional means model, in which there are no predictors at any level. The purpose of the unconditional means model is to assess the amount of variation at each level, and to compare variability within school to variability between schools. Define \\(Y_{ij}\\) as the MCA-II math score from school \\(i\\) and year \\(j\\). Using the composite model specification from Chapter 8: \\[\\begin{equation} Y _{ij} = \\alpha_{0} + u_{i} + \\epsilon_{ij} \\textrm{ with } u_{i} \\sim N(0, \\sigma^2_u) \\textrm{ and } \\epsilon_{ij} \\sim N(0, \\sigma^2) \\end{equation}\\] the unconditional means model can be fit to the MCA-II data: Formula: MathAvgScore ~ 1 + (1 | schoolid) Random effects: Groups Name Variance Std.Dev. schoolid (Intercept) 41.87 6.471 Residual 10.57 3.251 Number of obs: 1733, groups: schoolid, 618 Fixed effects: Estimate Std. Error t value (Intercept) 652.7460 0.2726 2395 From this output, we obtain estimates of our three model parameters: \\(\\hat{\\alpha}_{0}\\) = 652.7 = the mean math score across all schools and all years \\(\\hat{\\sigma}^2\\)= 10.6 = the variance in within-school deviations between individual scores and the school mean across all years \\(\\hat{\\sigma}^2_u\\)= 41.9 = the variance in between-school deviations between school means and the overall mean across all schools and all years Based on the intraclass correlation coefficient: \\[\\begin{equation} \\hat{\\rho}=\\frac{\\hat{\\sigma}^2_u}{\\hat{\\sigma}^2_u + \\hat{\\sigma}^2} = \\frac{41.869}{41.869+10.571}= 0.798 \\end{equation}\\] 79.8 percent of the total variation in math scores is attributable to difference among schools rather than changes over time within schools. We can also say that the average correlation for any pair of responses from the same school is 0.798. 9.5.2 Unconditional growth model The second model in most multilevel contexts introduces a covariate at Level One (see Model B in Chapter 8). With longitudinal data, this second model introduces time as a predictor at Level One, but there are still no predictors at Level Two. This model is then called the unconditional growth model. The unconditional growth model allows us to assess how much of the within-school variability can be attributed to systematic changes over time. At the lowest level, we can consider building individual growth models over time for each of the 618 schools in our study. First, we must decide upon a form for each of our 618 growth curves. Based on our initial exploratory analyses, assuming that an individual school’s MCA-II math scores follow a linear trend seems like a reasonable starting point. Under the assumption of linearity, we must estimate an intercept and a slope for each school, based on their 1-3 test scores over a period of three years. Compared to time series analyses of economic data, most longitudinal data analyses have relatively few time periods for each subject (or school), and the basic patterns within subject are often reasonably described by simpler functional forms. Let \\(Y_{ij}\\) be the math score of the \\(i^{th}\\) school in year \\(j\\). Then we can model the linear change in math test scores over time for School \\(i\\) according to Model B: \\[\\begin{equation} Y_{ij} = a_{i} + b_{i}Year08_{ij} + \\epsilon_{ij} \\textrm{ where } \\epsilon_{ij} \\sim N(0, \\sigma^2) \\end{equation}\\] The parameters in this model \\((a_{i}, b_{i},\\) and \\(\\sigma^2)\\) can be estimated through OLS methods. \\(a_{i}\\) represents the true intercept for School \\(i\\)—i.e., the expected test score level for School \\(i\\) when time is zero (2008)—while \\(b_{i}\\) represents the true slope for School \\(i\\)—i.e., the expected yearly rate of change in math score for School \\(i\\) over the three year observation period. Here we use Roman letters rather than Greek for model parameters since models by school will eventually be a conceptual first step in a multilevel model. The \\(\\epsilon_{ij}\\) terms represent the deviation of School \\(i\\)’s actual test scores from the expected results under linear growth—the part of school \\(i\\)’s test score at time \\(j\\) that is not explained by linear changes over time. The variability in these deviations from the linear model is given by \\(\\sigma^2\\). In Figure 9.17, which illustrates a linear growth model for Norwood Central Middle School, \\(a_{i}\\) is estimated by the \\(y\\)-intercept of the fitted regression line, \\(b_{i}\\) is estimated by the slope of the fitted regression line, and \\(\\sigma^2\\) is estimated by the variability in the vertical distances between each point (the actual math score in year \\(j\\)) and the line (the predicted math score in year \\(j\\)). Figure 9.17: Linear growth model for Norwood Central Middle School In a multilevel model, we let intercepts (\\(a_{i}\\)) and slopes (\\(b_{i}\\)) vary by school and build models for these intercepts and slopes using school-level variables at Level Two. An unconditional growth model features no predictors at Level Two and can be specified either using formulations at both levels: Level One: \\[\\begin{equation} Y_{ij}=a_{i}+b_{i}Year08_{ij} + \\epsilon_{ij} \\end{equation}\\] Level Two: \\[\\begin{eqnarray*} a_{i}=\\alpha_{0} + u_{i}\\\\ b_{i}=\\beta_{0} + v_{i} \\end{eqnarray*}\\] or as a composite model: \\[\\begin{equation} Y_{ij}=\\alpha_{0} + \\beta_{0}Year08_{ij}+u_{i}+v_{i}Year08_{ij} + \\epsilon_{ij} \\end{equation}\\] where \\(\\epsilon_{ij}\\sim N(0,\\sigma^2)\\) and \\[ \\left[ \\begin{array}{c} u_{i} \\\\ v_{i} \\end{array} \\right] \\sim N \\left( \\left[ \\begin{array}{c} 0 \\\\ 0 \\end{array} \\right], \\left[ \\begin{array}{cc} \\sigma_{u}^{2} &amp; \\\\ \\sigma_{uv} &amp; \\sigma_{v}^{2} \\end{array} \\right] \\right) . \\] As before, \\(\\sigma^2\\) quantifies the within-school variability (the scatter of points around schools’ linear growth trajectories), while now the between-school variability is partitioned into variability in initial status \\((\\sigma^2_u)\\) and variability in rates of change \\((\\sigma^2_v)\\). Using the composite model specification, the unconditional growth model can be fit to the MCA-II test data: Formula: MathAvgScore ~ year08 + (year08 | schoolid) Random effects: Groups Name Variance Std.Dev. Corr schoolid (Intercept) 39.4416 6.2803 year08 0.1105 0.3324 0.72 Residual 8.8200 2.9699 Number of obs: 1733, groups: schoolid, 618 Fixed effects: Estimate Std. Error t value (Intercept) 651.40766 0.27934 2332.0 year08 1.26496 0.08997 14.1 AIC = 10351.51 BIC = 10384.26 From this output, we obtain estimates of our six model parameters: \\(\\hat{\\alpha}_{0}\\) = 651.4 = the mean math score for the population of schools in 2008. \\(\\hat{\\beta}_{0}\\) = 1.26 = the mean yearly change in math test scores for the population during the three year observation period. \\(\\hat{\\sigma}^2\\) = 8.82 = the variance in within-school deviations. \\(\\hat{\\sigma}^2_u\\) = 39.4 = the variance between schools in 2008 scores. \\(\\hat{\\sigma}^2_v\\) = 0.11 = the variance between schools in rates of change in math test scores during the three year observation period. \\(\\hat{\\rho}_{uv}\\) = 0.72 = the correlation in schools’ 2008 math score and their rate of change in scores between 2008 and 2010. We see that schools had a mean math test score of 651.4 in 2008 and their mean test scores tended to increase by 1.26 points per year over the three year observation period, producing a mean test score at the end of three years of \\(653.9\\). According to the t-value (14.1), the increase in mean test scores noted during the three year observation period is statistically significant. The estimated within-school variance \\(\\hat{\\sigma}^2\\) decreased by about 17% from the unconditional means model, implying that 17% of within-school variability in test scores can be explained by a linear increase over time: \\[\\begin{equation} \\textrm{Pseudo }R^2_{L1} = \\frac{\\hat{\\sigma}^2(\\textrm{uncond means}) - \\hat{\\sigma}^2(\\textrm{uncond growth})}{\\hat{\\sigma^2}(\\textrm{uncond means})} = \\frac{10.571-8.820}{10.571}= 0.17 \\end{equation}\\] 9.5.3 Modeling other trends over time While modeling linear trends over time is often a good approximation of reality, it is by no means the only way to model the effect of time. One alternative is to model the quadratic effect of time, which implies adding terms for both time and the square of time. Typically, to reduce the correlation between the linear and quadratic components of the time effect, the time variable is often centered first; we have already “centered” on 2008. Modifying Model B to produce an unconditional quadratic growth model would take the following form: Level One: \\[\\begin{equation} Y_{ij}=a_{i}+b_{i}Year08_{ij}+c_{i}Year08^{2}_{ij} + \\epsilon_{ij} \\end{equation}\\] Level Two: \\[\\begin{eqnarray*} a_{i} &amp; = &amp; \\alpha_{0} + u_{i}\\\\ b_{i} &amp; = &amp; \\beta_{0} + v_{i}\\\\ c_{i} &amp; = &amp; \\gamma_{0} + w_{i} \\end{eqnarray*}\\] where \\(\\epsilon_{ij}\\sim N(0,\\sigma^2)\\) and \\[ \\left[ \\begin{array}{c} u_{i} \\\\ v_{i} \\\\ w_{i} \\end{array} \\right] \\sim N \\left( \\left[ \\begin{array}{c} 0 \\\\ 0 \\\\ 0 \\end{array} \\right], \\left[ \\begin{array}{ccc} \\sigma_{u}^{2} &amp; &amp; \\\\ \\sigma_{uv} &amp; \\sigma_{v}^{2} &amp; \\\\ \\sigma_{uw} &amp; \\sigma_{vw} &amp; \\sigma_{w}^{2} \\end{array} \\right] \\right) . \\] With the extra term at Level One for the quadratic effect, we now have 3 equations at Level Two, and 6 variance components at Level Two (3 variance terms and 3 covariance terms). However, with only a maximum of 3 observations per school, we lack the data for fitting 3 equations with error terms at Level Two. Instead, we could model the quadratic time effect with fewer variance components—for instance, by only using an error term on the intercept at Level Two: \\[\\begin{eqnarray*} a_{i} &amp; = &amp; \\alpha_{0} + u_{i}\\\\ b_{i} &amp; = &amp; \\beta_{0}\\\\ c_{i} &amp; = &amp; \\gamma_{0} \\end{eqnarray*}\\] where \\(u_{i}\\sim N(0,\\sigma^2_u)\\). Models like this are frequently used in practice—they allow for a separate overall effect on test scores for each school while minimizing parameters that must be estimated. The tradeoff is that this model does not allow linear and quadratic effects to differ by school, but we have little choice here without more observations per school. Thus, using the composite model specification, the unconditional quadratic growth model with random intercept for each school can be fit to the MCA-II test data: Formula: MathAvgScore ~ yearc + yearc2 + (1 | schoolid) Random effects: Groups Name Variance Std.Dev. schoolid (Intercept) 43.050 6.561 Residual 8.523 2.919 Number of obs: 1733, groups: schoolid, 618 Fixed effects: Estimate Std. Error t value (Intercept) 651.94235 0.29229 2230.4 yearc 1.26993 0.08758 14.5 yearc2 1.06841 0.15046 7.1 AIC = 10308.16 BIC = 10335.45 From this output, we see that the quadratic effect is positive and significant (t=7.1), in this case indicating that increases in test scores are greater between 2009 and 2010 than between 2008 and 2009. Based on AIC and BIC values, the quadratic growth model outperforms the linear growth model (AIC: 10308 vs. 10352; BIC: 10335 vs. 10384) with 1 fewer parameter to estimate in the quadratic growth model (1 extra fixed effect and 2 fewer variance components). Another frequently used approach to modeling time effects is the piecewise linear model. In this model, the complete time span of the study is divided into two or more segments, with a separate slope relating time to the response in each segment. In our case study there is only one piecewise option—fitting separate slopes in 2008-09 and 2009-10. With only 3 time points, creating a piecewise linear model is a bit simplified, but this idea can be generalized to segments with more than two years each. Like the model for quadratic growth, we now have an extra equation at Level Two and must restrict our error terms to only the equation for the intercept: Level One: \\[\\begin{equation} Y_{ij}=a_{i}+b_{i}Year0809_{ij}+c_{i}Year0810_{ij} + \\epsilon_{ij} \\end{equation}\\] Level Two: \\[\\begin{eqnarray*} a_{i} &amp; = &amp; \\alpha_{0} + u_{i}\\\\ b_{i} &amp; = &amp; \\beta_{0} \\\\ c_{i} &amp; = &amp; \\gamma_{0} \\end{eqnarray*}\\] where error distributions are the same as in the unconditional quadratic growth model. Using the composite model specification, the piecewise linear growth model can be fit using statistical software: Formula: MathAvgScore ~ year0809 + year0810 + (1 | schoolid) Random effects: Groups Name Variance Std.Dev. schoolid (Intercept) 43.050 6.561 Residual 8.523 2.919 Number of obs: 1733, groups: schoolid, 618 Fixed effects: Estimate Std. Error t value (Intercept) 651.7408 0.2932 2222.7 year0809 0.2015 0.1753 1.1 year0810 2.5399 0.1752 14.5 AIC = 10306.77 BIC = 10334.06 The performance of this model is very similar to the quadratic growth model by AIC and BIC measures, and the story told by fixed effects estimates is also very similar. While the mean yearly increase in math scores was 0.2 points between 2008 and 2009, it was 2.3 points between 2009 and 2010. Despite the good performances of the quadratic growth and piecewise linear models on our three-year window of data, we will continue to use linear growth assumptions in the remainder of this chapter. Not only is a linear model easier to interpret and explain, but it’s probably a more reasonable assumption in years beyond 2010. Predicting future performance is more risky by assuming a steep one year rise or a non-linear rise will continue, rather than by using the average increase over two years. 9.6 Building to a final model 9.6.1 Uncontrolled effects of school type Initially, we can consider whether or not there are significant differences in individual school growth parameters (intercepts and slopes) based on school type. From a modeling perspective, we would build a system of two \\(Level\\) \\(Two\\) \\(models\\): \\[\\begin{eqnarray*} a_{i} &amp; = &amp; \\alpha_{0} + \\alpha_{1}Charter_i + u_{i} \\\\ b_{i} &amp; = &amp; \\beta_{0} + \\beta_{1}Charter_i + v_{i} \\end{eqnarray*}\\] where \\(Charter_i=1\\) if School \\(i\\) is a charter school and \\(Charter_i=0\\) if School \\(i\\) is a non-charter public school. In addition, the error terms at Level Two are assumed to follow a multivariate normal distribution: \\[ \\left[ \\begin{array}{c} u_{i} \\\\ v_{i} \\end{array} \\right] \\sim N \\left( \\left[ \\begin{array}{c} 0 \\\\ 0 \\end{array} \\right], \\left[ \\begin{array}{cc} \\sigma_{u}^{2} &amp; \\\\ \\sigma_{uv} &amp; \\sigma_{v}^{2} \\end{array} \\right] \\right) . \\] With a binary predictor at Level Two such as school type, we can write out what our Level Two model looks like for public non-charter schools and charter schools. Public schools \\[\\begin{eqnarray*} a_{i} &amp; = &amp; \\alpha_{0} + u_{i}\\\\ b_{i} &amp; = &amp; \\beta_{0} + v_{i}, \\end{eqnarray*}\\] Charter schools \\[\\begin{eqnarray*} a_{i} &amp; = &amp; (\\alpha_{0} + \\alpha_{1}) + u_{i}\\\\ b_{i} &amp; = &amp; (\\beta_{0}+ \\beta_{1}) + v_{i} \\end{eqnarray*}\\] Writing the Level Two model in this manner helps us interpret the model parameters from our two-level model. We can use statistical software (such as the lmer() function from the lme4 package in R) to obtain parameter estimates using our \\(1733\\) observations, after first converting our Level One and Level Two models into a composite model (Model C) with fixed effects and variance components separated: \\[\\begin{eqnarray*} Y_{ij} &amp; = &amp; a_{i} + b_{i}Year08_{ij}+ \\epsilon_{ij} \\\\ &amp; = &amp; (\\alpha_{0} + \\alpha_{1}Charter_i +u_{i}) + (\\beta_{0} + \\beta_{1}Charter_i + v_{i})Year08_{ij} + \\epsilon_{ij} \\\\ &amp; = &amp; [\\alpha_{0} + \\beta_{0}Year08_i +\\alpha_{1}Charter_i+ \\beta_{1}Charter_iYear08_{ij}] + [u_{i} + v_{i}Year08_{ij} + \\epsilon_{ij}] \\end{eqnarray*}\\] Formula: MathAvgScore ~ charter + year08 + charter:year08 + (year08 | -3.1935 -0.4710 0.0129 0.4660 3.4592 Random effects: Groups Name Variance Std.Dev. Corr schoolid (Intercept) 35.8319 5.9860 year08 0.1312 0.3621 0.88 Residual 8.7845 2.9639 Number of obs: 1733, groups: schoolid, 618 Fixed effects: Estimate Std. Error t value (Intercept) 652.05844 0.28449 2292.0 charter -6.01843 0.86562 -7.0 year08 1.19709 0.09427 12.7 charter:year08 0.85571 0.31430 2.7 AIC = 10307.6 BIC = 10351.26 Armed with our parameter estimates, we can offer concrete interpretations: Fixed effects: \\(\\hat{\\alpha}_{0} = 652.1.\\) The estimated mean test score for 2008 for non-charter public schools is 652.1. \\(\\hat{\\alpha}_{1}= -6.02.\\) Charter schools have an estimated test score in 2008 which is 6.02 points lower than public non-charter schools. \\(\\hat{\\beta}_{0}= 1.20.\\) Public non-charter schools have an estimated mean increase in test scores of 1.20 points per year. \\(\\hat{\\beta}_{1}= 0.86.\\) Charter schools have an estimated mean increase in test scores of 2.06 points per year over the three year observation period, 0.86 points higher than the mean yearly increase among public non-charter schools. Variance components: \\(\\hat{\\sigma}_u= 5.99.\\) The estimated standard deviation of 2008 test scores is 5.99 points, after controlling for school type. \\(\\hat{\\sigma}_v= 0.36.\\) The estimated standard deviation of yearly changes in test scores during the three year observation period is 0.36 points, after controlling for school type. \\(\\hat{\\rho}_{uv}= 0.88.\\) The estimated correlation between 2008 test scores and yearly changes in test scores is 0.88, after controlling for school type. \\(\\hat{\\sigma}= 2.96.\\) The estimated standard deviation in residuals for the individual growth curves is 2.96 points. Based on t-values reported by R, the effects of year08 and charter both appear to be statistically significant, and there is also significant evidence of an interaction between year08 and charter. Public schools had a significantly higher mean math score in 2008, while charter schools had significantly greater improvement in scores between 2008 and 2010 (although the mean score of charter schools still lagged behind that of public schools in 2010, as indicated in the graphical comparison of models B and C in Figure 9.18). Based on pseudo R-square values, the addition of a charter school indicator to the unconditional growth model has decreased unexplained school-to-school variability in 2008 math scores by 4.7%, while unexplained variability in yearly improvement actually increased slightly. Obviously, it makes little sense that introducing an additional predictor would reduce the amount of variability in test scores explained, but this is an example of the limitations in the pseudo \\(R^2\\) values discussed in Section 8.7.2. Figure 9.18: Fitted growth curves for Models B and C. 9.6.2 Add percent free and reduced lunch as a covariate Although we will still be primarily interested in the effect of school type on both 2008 test scores and rate of change in test scores (as we observed in Model C), we can try to improve our estimates of school type effects through the introduction of meaningful covariates. In this study, we are particularly interested in Level Two covariates—those variables which differ by school but which remain basically constant for a given school over time—such as urban or rural location, percentage of special education students, and percentage of students with free and reduced lunch. In Section 9.4, we investigated the relationship between percent free and reduced lunch and a school’s test score in 2008 and their rate of change from 2008 to 2010. Based on these analyses, we will begin by adding percent free and reduced lunch as a Level Two predictor for both intercept and slope (Model D): Level One: \\[\\begin{equation} Y_{ij}=a_{i} + b_{i}Year08_{ij} + \\epsilon_{ij} \\end{equation}\\] Level Two: \\[\\begin{eqnarray*} a_{i} &amp; = &amp; \\alpha_{0} + \\alpha_{1}Charter_i + \\alpha_{2}schpctfree_i + u_{i}\\\\ b_{i} &amp; = &amp; \\beta_{0} + \\beta_{1}Charter_i + \\beta_{2}schpctfree_i + v_{i} \\end{eqnarray*}\\] The composite model is then: \\[\\begin{equation} Y_{ij}= [\\alpha_{0}+\\alpha_{1}Charter_i +\\alpha_{2}schpctfree_i + \\beta_{0}Year08_{ij} + \\beta_{1}Charter_iYear08_{ij} + \\\\ \\beta_{2}schpctfree_iYear08_{ij}] + [u_{i} + v_{i}Year08_{ij} + \\epsilon_{ij}] \\end{equation}\\] where error terms are defined as in Model C. Formula: MathAvgScore ~ charter + SchPctFree + year08 + charter:year08 + SchPctFree:year08 + (year08 | schoolid) Random effects: Groups Name Variance Std.Dev. Corr schoolid (Intercept) 19.1319 4.3740 year08 0.1603 0.4004 0.51 Residual 8.7981 2.9662 Number of obs: 1733, groups: schoolid, 618 Fixed effects: Estimate Std. Error t value (Intercept) 659.278483 0.444688 1482.6 charter -3.439938 0.712832 -4.8 SchPctFree -0.166539 0.008907 -18.7 year08 1.641369 0.189499 8.7 charter:year08 0.980755 0.318583 3.1 SchPctFree:year08 -0.010409 0.003839 -2.7 AIC = 9988.308 BIC = 10042.88 Compared to Model C, the introduction of school-level poverty based on percentage of students receiving free and reduced lunch in Model D leads to similar conclusions about the significance of the charter school effect on both the intercept and the slope, although the magnitude of these estimates change after controlling for poverty levels. The estimated gap in test scores between charter and non-charter schools in 2008 is smaller in Model D, while estimates of improvement between 2008 and 2010 increase for both types of schools. Inclusion of free and reduced lunch reduces the unexplained variability between schools in 2008 math scores by 27%, while unexplained variability in rates of change between schools again increases slightly based on pseudo \\(R^2\\) values. A likelihood ratio test using maximum likelihood estimates illustrates that adding free and reduced lunch as a Level Two covariate significantly improves our model (\\(\\chi^2 = 341.5, df=2, p&lt;.001\\)). Specific fixed effect parameter estimates are given below: \\(\\hat{\\alpha}_{0}= 659.3.\\) The estimated mean math test score for 2008 is 659.3 for non-charter public schools with no students receiving free and reduced lunch. \\(\\hat{\\alpha}_{1}= -3.44.\\) Charter schools have an estimated mean math test score in 2008 which is 3.44 points lower than non-charter public schools, controlling for effects of school-level poverty. \\(\\hat{\\alpha}_{2}= -0.17.\\) Each 10% increase in the percentage of students at a school receiving free and reduced lunch is associated with a 1.7 point decrease in mean math test scores for 2008, after controlling for school type. \\(\\hat{\\beta}_{0}= 1.64.\\) Public non-charter schools with no students receiving free and reduced lunch have an estimated mean increase in math test score of 1.64 points per year during the three years of observation. \\(\\hat{\\beta}_{1}= 0.98.\\) Charter schools have an estimated mean yearly increase in math test scores over the three year observation period of 2.62, which is 0.98 points higher than the annual increase for public non-charter schools, after controlling for school-level poverty. \\(\\hat{\\beta}_{2}= -0.010.\\) Each 10% increase in the percentage of students at a school receiving free and reduced lunch is associated with a 0.10 point decrease in rate of change over the three years of observation, after controlling for school type. 9.6.3 A potential final model with three Level Two covariates We now begin iterating toward a “final model” for these data, on which we will base conclusions. Being cognizant of typical features of a “final model” as outlined in Chapter 8, we offer one possible final model for this data—Model F: Level One: \\[\\begin{equation} Y_{ij}= a_{i} + b_{i}Year08_{ij} + \\epsilon_{ij} \\end{equation}\\] Level Two: \\[\\begin{eqnarray*} a_{i} &amp; = &amp; \\alpha_{0} + \\alpha_{1}Charter_i + \\alpha_{2}urban_i + \\alpha_{3}schpctsped_i + \\alpha_{4}schpctfree_i + u_{i} \\\\ b_{i} &amp; = &amp; \\beta_{0} + \\beta_{1}Charter_i + \\beta_{2}urban_i + \\beta_{3}schpctsped_i + v_{i} \\end{eqnarray*}\\] where we find the effect of charter schools on 2008 test scores after adjusting for urban or rural location, percentage of special education students, and percentage of students that receive free or reduced lunch, and the effect of charter schools on yearly change between 2008 and 2010 after adjusting for urban or rural location and percentage of special education students. We can use AIC and BIC criteria to compare Model F with Model D, since the two models are not nested. By both criteria, Model F is significantly better than Model D: AIC of 9855 vs. 9988, and BIC of 9956 vs. 10043. Based on the R output below, we offer interpretations for estimates of model fixed effects: Formula: MathAvgScore ~ charter + urban + SchPctFree + SchPctSped + charter:year08 + -3.2959 -0.4966 0.0121 0.4727 4.0825 Random effects: Groups Name Variance Std.Dev. Corr schoolid (Intercept) 16.946781 4.11665 year08 0.003428 0.05855 1.00 Residual 8.823185 2.97038 Number of obs: 1733, groups: schoolid, 618 Fixed effects: Estimate Std. Error t value (Intercept) 661.010451 0.512890 1288.8 charter -3.222815 0.698546 -4.6 urban -1.113831 0.427568 -2.6 SchPctFree -0.152814 0.008096 -18.9 SchPctSped -0.117703 0.020612 -5.7 year08 2.144274 0.200852 10.7 charter:year08 1.030848 0.315137 3.3 urban:year08 -0.527497 0.186466 -2.8 SchPctSped:year08 -0.046740 0.010165 -4.6 AIC = 9884.646 BIC = 9955.595 \\(\\hat{\\alpha}_{0}= 661.0.\\) The estimated mean math test score for 2008 is 661.0 for public schools in rural areas with no students qualifying for special education or free and reduced lunch. \\(\\hat{\\alpha}_{1}= -3.22.\\) Charter schools have an estimated mean math test score in 2008 which is 3.22 points lower than non-charter public schools, after controlling for urban or rural location, percent special education, and percent free and reduced lunch. \\(\\hat{\\alpha}_{2}= -1.11.\\) Schools in urban areas have an estimated mean math score in 2008 which is 1.11 points lower than schools in rural areas, after controlling for school type, percent special education, and percent free and reduced lunch. \\(\\hat{\\alpha}_{3}= -0.118.\\) A 10% increase in special education students at a school is associated with a 1.18 point decrease in estimated mean math score for 2008, after controlling for school type, urban or rural location, and percent free and reduced lunch. \\(\\hat{\\alpha}_{4}= -0.153.\\) A 10% increase in free and reduced lunch students at a school is associated with a 1.53 point decrease in estimated mean math score for 2008, after controlling for school type, urban or rural location, and percent special education. \\(\\hat{\\beta}_{0}= 2.14.\\) Public non-charter schools in rural areas with no students qualifying for special education have an estimated increase in mean math test score of 2.14 points per year over the three year observation period, after controlling for percent of students receiving free and reduced lunch. \\(\\hat{\\beta}_{1}= 1.03.\\) Charter schools have an estimated mean annual increase in math score that is 1.03 points higher than public non-charter schools over the three year observation period, after controlling for urban or rural location, percent special education, and percent free and reduced lunch. \\(\\hat{\\beta}_{2}= -0.53.\\) Schools in urban areas have an estimated mean annual increase in math score that is 0.53 points lower than schools from rural areas over the three year observation period, after controlling for school type, percent special education, and percent free and reduced lunch. \\(\\hat{\\beta}_{3}= -0.047.\\) A 10% increase in special education students at a school is associated with an estimated mean annual increase in math score that is 0.47 points lower over the three year observation period, after controlling for school type, urban or rural location, and percent free and reduced lunch. From this model, we again see that 2008 sixth grade math test scores from charter schools were significantly lower than similar scores from public non-charter schools, after controlling for school location and demographics. However, charter schools showed significantly greater improvement between 2008 and 2010 compared to public non-charter schools, although charter school test scores were still lower than public school scores in 2010, on average. We also tested several interactions between Level Two covariates and charter schools and found none to be significant, indicating that the 2008 gap between charter schools and public non-charter schools was consistent across demographic subgroups. The faster improvement between 2008 and 2010 for charter schools was also consistent across demographic subgroups (found by testing three-way interactions). Controlling for school location and demographic variables provided more reliable and nuanced estimates of the effects of charter schools, while also providing interesting insights. For example, schools in rural areas not only had higher test scores than schools in urban areas in 2008, but the gap grew larger over the study period given fixed levels of percent special education, percent free and reduced lunch, and school type. In addition, schools with higher levels of poverty lagged behind other schools and showed no signs of closing the gap, and schools with higher levels of special education students had both lower test scores in 2008 and slower rates of improvement during the study period, again given fixed levels of other covariates. As we demonstrated in this case study, applying multilevel methods to two-level longitudinal data yields valuable insights about our original research questions while properly accounting for the structure of the data. 9.7 Covariance structure among observations Part of our motivation for framing our model for multilevel data was to account for the correlation among observations made on the same school (the Level Two observational unit). Our two-level model, through error terms on both Level One and Level Two variables, actually implies a specific within-school covariance structure among observations, yet we have not (until now) focused on this imposed structure; for example: What does our two-level model say about the relative variability of 2008 and 2010 scores from the same school? What does it say about the correlation between 2008 and 2009 scores from the same school? In this section, we will describe the within-school covariance structure imposed by our two-level model and offer alternative covariance structures that we might consider, especially in the context of longitudinal data. In short, we will discuss how we might decide if our implicit covariance structure in our two-level model is satisfactory for the data at hand. Then, in the succeeding optional section, we provide derivations of the imposed within-school covariance structure for our standard two-level model using results from probability theory. 9.7.1 Standard covariance structure We will use Model C (uncontrolled effects of school type) to illustrate covariance structure within subjects. Recall that, in composite form, Model C is: \\[\\begin{eqnarray*} Y_{ij} &amp; = &amp; a_{i}+b_{i}Year08_{ij}+ \\epsilon_{ij} \\\\ &amp; = &amp; (\\alpha_{0}+ \\alpha_{1}Charter_i + u_{i}) + (\\beta_{0}+\\beta_{1}Charter_i +v_{i}) Year08_{ij} + \\epsilon_{ij} \\\\ &amp; = &amp; [\\alpha_{0}+\\alpha_{1}Charter_i + \\beta_{0}Year08_i + \\beta_{1}Charter_iYear08_{ij}] + [u_{i} + v_{i}Year08_{ij} + \\epsilon_{ij}] \\end{eqnarray*}\\] where \\(\\epsilon_{ij}\\sim N(0,\\sigma^2)\\) and \\[ \\left[ \\begin{array}{c} u_{i} \\\\ v_{i} \\end{array} \\right] \\sim N \\left( \\left[ \\begin{array}{c} 0 \\\\ 0 \\end{array} \\right], \\left[ \\begin{array}{cc} \\sigma_{u}^{2} &amp; \\\\ \\sigma_{uv} &amp; \\sigma_{v}^{2} \\end{array} \\right] \\right) . \\] For School \\(i\\), the covariance structure for the three time points has general form: \\[ Cov(\\mathbf{Y}_i) = \\left[ \\begin{array}{cccc} Var(Y_{i1}) &amp; Cov(Y_{i1},Y_{i2}) &amp; Cov(Y_{i1},Y_{i3}) \\\\ Cov(Y_{i1},Y_{i2}) &amp; Var(Y_{i2}) &amp; Cov(Y_{i2},Y_{i3}) \\\\ Cov(Y_{i1},Y_{i3}) &amp; Cov(Y_{i2},Y_{i3}) &amp; Var(Y_{i3}) \\end{array} \\right] \\] where, for instance, \\(Var(Y_{i1})\\) is the variability in 2008 test scores (time \\(j=1\\)), \\(Cov(Y_{i1},Y_{i2})\\) is the covariance between 2008 and 2009 test scores (times \\(j=1\\) and \\(j=2\\)), etc. Since covariance measures the tendency of two variables to move together, we expect positive values for all three covariance terms in \\(Cov(\\mathbf{Y}_i)\\), since schools with relatively high test scores in 2008 are likely to also have relatively high test scores in 2009 or 2010. The correlation between two variables then scales covariance terms to values between -1 and 1, so by the same rationale, we expect correlation coefficients between two years to be near 1. If observations within school were independent—that is, knowing a school had relatively high scores in 2008 tells nothing about whether that school will have relatively high scores in 2009 or 2010—then we would expect covariance and correlation values near 0. It is important to notice that the error structure at Level Two is not the same as the within-school covariance structure among observations. That is, the relationship between \\(u_{i}\\) and \\(v_{i}\\) from the Level Two equations is not the same as the relationship between test scores from different years at the same school (e.g., the relationship between \\(Y_{i1}\\) and \\(Y_{i2}\\)). In other words, \\[ Cov(\\mathbf{Y}_i) \\neq \\left[ \\begin{array}{c} u_{i} \\\\ v_{i} \\end{array} \\right] \\sim N \\left( \\left[ \\begin{array}{c} 0 \\\\ 0 \\end{array} \\right], \\left[ \\begin{array}{cc} \\sigma_{u}^{2} &amp; \\\\ \\sigma_{uv} &amp; \\sigma_{v}^{2} \\end{array} \\right] \\right) . \\] Yet, the error structure and the covariance structure are connected to each other, as we will now explore. Using results from probability theory (see Section 9.7.5), we can show that: \\[\\begin{eqnarray*} Var(Y_{ij}) &amp; = &amp; \\sigma_{u}^{2} + t^{2}_{ij} \\sigma_{v}^{2} + \\sigma^{2} + 2t_{ij}\\sigma_{uv}, \\\\ Cov(Y_{ij},Y_{ik}) &amp; = &amp; \\sigma_{u}^{2} + t_{ij}t_{ik} \\sigma_{v}^{2} + (t_{ij}+t_{ik})\\sigma_{uv} \\end{eqnarray*}\\] for all \\(i\\), where our time variable (year08) has values \\(t_{i1}=0\\), \\(t_{i2}=1\\), and \\(t_{i3}=2\\) for every School \\(i\\). Intuitively, these formulas are sensible. For instance, \\(Var(Y_{i1})\\), the uncertainty (variability) around a school’s score in 2008, increases as the uncertainty in intercepts and slopes increases, as the uncertainty around that school’s linear time trend increases, and as the covariance between intercept and slope residuals increases (since if one is off, the other one is likely off as well). Also, \\(Cov(Y_{i1},Y_{i2})\\), the covariance between 2008 and 2009 scores, does not depend on Level One error. Thus, in the 3-by-3 within-school covariance structure of the charter schools case study, our standard two-level model determines all 6 covariance matrix elements through the estimation of four parameters (\\(\\sigma_{u}^{2}, \\sigma_{uv}, \\sigma_{v}^{2}, \\sigma^2\\)) and the imposition of a specific structure related to time. To obtain estimated variances for individual observations and covariances between two time points from the same school, we can simply plug estimated variance components from our two-level model along with time points from our data collection into the equations above. For instance, in Section ??, we obtained the following estimates of variance components: \\(\\hat{\\sigma}^{2}=8.69\\), \\(\\hat{\\sigma}^{2}_{u}=35.97\\), \\(\\hat{\\sigma}^{2}_{v}=0.23\\), and \\(\\hat{\\sigma}_{uv}=\\hat{\\rho}\\hat{\\sigma_{u}}\\hat{\\sigma_{v}}=1.81\\). Therefore, our estimated within-school variances for the three time points would be: \\[\\begin{eqnarray*} \\hat{Var}(Y_{i1}) &amp; = &amp; 35.97 + 0^{2} 0.23 + 8.69 + 2(0)1.81 = 44.66 \\\\ \\hat{Var}(Y_{i2}) &amp; = &amp; 35.97 + 1^{2} 0.23 + 8.69 + 2(1)1.81 = 48.50 \\\\ \\hat{Var}(Y_{i3}) &amp; = &amp; 35.97 + 2^{2} 0.23 + 8.69 + 2(2)1.81 = 52.80 \\end{eqnarray*}\\] and our estimated within-school covariances between different time points would be: \\[\\begin{eqnarray*} \\hat{Cov}(Y_{i1},Y_{i2}) &amp; = &amp; 35.97 + (0)(1)0.23 + (0+1)1.81 = 37.77 \\\\ \\hat{Cov}(Y_{i1},Y_{i3}) &amp; = &amp; 35.97 + (0)(2)0.23 + (0+2)1.81 = 39.58 \\\\ \\hat{Cov}(Y_{i2},Y_{i3}) &amp; = &amp; 35.97 + (1)(2)0.23 + (1+2)1.81 = 41.84 \\end{eqnarray*}\\] In fact, these values will be identical for every School \\(i\\), since scores were assessed at the same three time points. Thus, we will drop the \\(i\\) subscript moving forward. Written in matrix form, our two-level model implicitly imposes this estimated covariance structure on within-school observations for any specific School \\(i\\): \\[ \\hat{Cov}(\\mathbf{Y}) = \\left[ \\begin{array}{cccc} 44.66 &amp; &amp; \\\\ 37.77 &amp; 48.50 &amp; \\\\ 39.58 &amp; 41.84 &amp; 52.80 \\end{array} \\right] \\] and this estimated covariance matrix can be converted into an estimated within-school correlation matrix using the identity \\(Corr(Y_{1},Y_{2})=\\frac{Cov(Y_{1},Y_{2})}{Var(Y_{1}) Var(Y_{2})}\\): \\[ \\hat{Corr}(\\mathbf{Y}) = \\left[ \\begin{array}{cccc} 1 &amp; &amp; \\\\ .812 &amp; 1 &amp; \\\\ .815 &amp; .827 &amp; 1 \\end{array} \\right] \\] A couple of features of these two matrices can be highlighted that offer insights into implications of our standard two-level model on the covariance structure among observations at Level One from the same school: Many longitudinal data sets show higher correlation for observations that are closer in time. In this case, we see that correlation is very consistent between all pairs of observations from the same school; the correlation between test scores separated by two years (.815) is approximately the same as the correlation between test scores separated by a single year (.812 for 2008 and 2009 scores; .827 for 2009 and 2010 scores). Many longitudinal data sets show similar variability at all time points. In this case, the variability in 2010 (52.80) is about 18% greater than the variability in 2008 (44.66), while the variability in 2009 is in between (48.50). Our two-level model actually imposes a quadratic structure on the relationship between variance and time; note that the equation for \\(Var(Y_{j})\\) contains both \\(t^{2}_{j}\\) and \\(t_{j}\\). The variance is therefore minimized at \\(t=\\frac{-\\sigma_{uv}}{\\sigma^{2}_{v}}\\). With the charter school data, the variance in test scores is minimized when \\(t=\\frac{-\\sigma_{uv}}{\\sigma^{2}_{v}}=\\frac{-1.81}{0.23}=-8.0\\); that is, the smallest within-school variance in test scores is expected 8.0 years prior to 2008 (i.e., in 2000), and the variance increases parabolically from there. In general, cases in which \\(\\sigma^{2}_{v}\\) and \\(\\sigma_{uv}\\) are relatively small have little curvature and fairly consistent variability over time. There is no requirement that time points within school need to be evenly spaced or even that each school has an equal number of measurements over time, which makes the two-level model structure nicely flexible. 9.7.2 Alternative covariance structures The standard covariance structure that’s implied by our multilevel modeling structure provides a useful model in a wide variety of situations—it provides a reasonable model for Level One variability with a relatively small number of parameters, and it has sufficient flexibility to accommodate irregular time intervals as well as subjects with different number of observations over time. However, there may be cases in which a better fitting model requires additional parameters, or when a simpler model with fewer parameters still provides a good fit to the data. Here is an outline of a few alternative error structures: Unstructured - Every variance and covariance term for observations within a school is a separate parameter and is therefore estimated uniquely; no patterns among variances or correlations are assumed. This structure offers maximum flexibility but is most costly in terms of parameters estimated. Compound symmetry - Assume variance is constant across all time points and correlation is constant across all pairs of time points. This structure is highly restrictive but least costly in terms of parameters estimated. Autoregressive - Assume variance is constant across all time points, but correlation drops off in a systematic fashion as the gap in time increases. Autoregressive models expand compound symmetry by allowing for a common structure where points closest in time are most highly correlated. Toeplitz - Toeplitz is similar to the autoregressive model, except that it does not impose any structure on the decline in correlation as time gaps increase. Thus, it requires more parameters to be estimated than the autoregressive model while providing additional flexibility. Heterogeneous variances - The assumption that variances are equal across time points found in the compound symmetry, autoregressive, and Toeplitz models can be relaxed by introducing additional parameters to allow unequal (heterogeneous) variances. When the focus of an analysis is on stochastic parameters (variance components) rather than fixed effects, parameter estimates are typically based on restricted maximum likelihood (REML) methods; model performance statistics then reflect only the stochastic portion of the model. Models with the same fixed effects but different covariance structures can be compared as usual—with AIC and BIC measures when models are not nested and with likelihood ratio tests when models are nested. However, using a chi-square distribution to conduct a likelihood ratio test in these cases can often produce a conservative test, with p-values that are too large and not rejected enough (Raudenbush and Bryk 2002; Singer and Willett 2003; Faraway 2005). In Section 10.6, we introduce the parametric bootstrap as a potentially better way of testing models nested in their random effects. 9.7.3 Covariance structure in non-longitudinal multilevel models Careful modeling and estimation of the Level One covariance matrix is especially important and valuable for longitudinal data (with time at Level One) and as we’ve seen, our standard two-level model has several nice properties for this purpose. The standard model is also often appropriate for non-longitudinal multilevel models as discussed in Chapter 8, although we must remain aware of the covariance structure implicitly imposed. In other words, the ideas in this section generalize even if time isn’t a Level One covariate. As an example, in Case Study 8.2 where Level One observational units are musical performances rather than time points, the standard model implies the following covariance structure for Musician \\(i\\) in Model C, which uses an indicator for large ensembles as a Level One predictor: \\[\\begin{eqnarray*} Var(Y_{ij}) &amp; = &amp; \\sigma_{u}^{2} + \\textstyle{Large}^{2}_{ij} \\sigma_{v}^{2} + \\sigma^{2} + 2\\textstyle{Large}_{ij}\\sigma_{uv} \\\\ &amp; = &amp; \\left\\{ \\begin{array}{ll} \\sigma^{2} + \\sigma_{u}^{2} &amp; \\mbox{if $\\textstyle{Large}_{ij}=0$} \\\\ \\sigma^{2} + \\sigma_{u}^{2} + \\sigma_{v}^{2} + 2\\sigma_{uv} &amp; \\mbox{if $\\textstyle{Large}_{ij}=1$} \\end{array} \\right. \\end{eqnarray*}\\] and \\[\\begin{eqnarray*} Cov(Y_{ij},Y_{ik}) &amp; = &amp; \\sigma_{u}^{2} + \\textstyle{Large}_{ij}\\textstyle{Large}_{ik} \\sigma_{v}^{2} + (\\textstyle{Large}_{ij} + \\textstyle{Large}_{ik}) \\sigma_{uv} \\\\ &amp; = &amp; \\left\\{ \\begin{array}{ll} \\sigma_{u}^{2} &amp; \\mbox{if $\\textstyle{Large}_{ij}=\\textstyle{Large}_{ik}=0$} \\\\ \\sigma_{u}^{2} + \\sigma_{uv} &amp; \\mbox{if $\\textstyle{Large}_{ij}=0$ and $\\textstyle{Large}_{ik}=1$ or vice versa} \\\\ \\sigma_{u}^{2} + \\sigma_{v}^{2} + 2\\sigma_{uv} &amp; \\mbox{if $\\textstyle{Large}_{ij}=\\textstyle{Large}_{ik}=1$} \\end{array} \\right. \\end{eqnarray*}\\] Note that, in the Music Performance Anxiety case study, each subject will have a unique Level One variance-covariance structure, since each subject has a different number of performances and a different mix of large ensemble and small ensemble or solo performances. 9.7.4 Final thoughts regarding covariance structures In the charter school example, as is often true in multilevel models, the choice of covariance matrix does not greatly affect estimates of fixed effects. The choice of covariance structure could potentially impact the standard errors of fixed effects, and thus the associated test statistics, but the impact appears minimal in this particular case study. In fact, the standard model typically works very well. So is it worth the time and effort to accurately model the covariance structure? If primary interest is in inference regarding fixed effects, and if the standard errors for the fixed effects appear robust to choice of covariance structure, then extensive time spent modeling the covariance structure is not advised. However, if researchers are interested in predicted random effects and estimated variance components in addition to estimated fixed effects, then choice of covariance structure can make a big difference. For instance, if researchers are interested in drawing conclusions about particular schools rather than charter schools in general, they may more carefully model the covariance structure in this study. 9.7.5 Details of covariance structures (Optional) Using Model C as specified in Section 9.7.1, we specified the general covariance structure for School \\(i\\) as: \\[ Cov(\\mathbf{Y}_i) = \\left[ \\begin{array}{cccc} Var(Y_{i1}) &amp; Cov(Y_{i1},Y_{i2}) &amp; Cov(Y_{i1},Y_{i3}) \\\\ Cov(Y_{i1},Y_{i2}) &amp; Var(Y_{i2}) &amp; Cov(Y_{i2},Y_{i3}) \\\\ Cov(Y_{i1},Y_{i3}) &amp; Cov(Y_{i2},Y_{i3}) &amp; Var(Y_{i3}) \\end{array} \\right] \\] If \\(Y_1 = a_1 X_1 + a_2 X_2 + a_3\\) and \\(Y_2 = b_1 X_1 + b_2 X_2 + b_3\\) where \\(X_1\\) and \\(X_2\\) are random variables and \\(a_i\\) and \\(b_i\\) are constants for \\(i=1,2,3\\), then we know from probability theory that: \\[\\begin{eqnarray*} Var(Y_1) &amp; = &amp; a^{2}_{1} Var(X_1) + a^{2}_{2} Var(X_2) + 2 a_1 a_2 Cov(X_1,X_2) \\\\ Cov(Y_1,Y_2) &amp; = &amp; a_1 b_1 Var(X_1) + a_2 b_2 Var(X_2) + (a_1 b_2 + a_2 b_1) Cov(X_1,X_2) \\end{eqnarray*}\\] Applying these identities to Model C, we first see that we can ignore all fixed effects, since they do not contribute to the variability. Thus, \\[\\begin{eqnarray*} Var(Y_{ij}) &amp; = &amp; Var(u_{i}+v_{i}\\textstyle{Year08}_{ij}+\\epsilon_{ij}) \\\\ &amp; = &amp; Var(u_{i}) + \\textstyle{Year08}^{2}_{ij} Var(v_{i}) + Var(\\epsilon_{ij}) + 2\\textstyle{Year08}_{ij} Cov(u_{i},v_{i}) \\\\ &amp; = &amp; \\sigma_{u}^{2} + \\textstyle{Year08}^{2}_{ij} \\sigma_{v}^{2} + \\sigma^{2} + 2\\textstyle{Year08}_{ij}\\sigma_{uv} \\\\ &amp; = &amp; \\sigma_{u}^{2} + t^{2}_{j} \\sigma_{v}^{2} + \\sigma^{2} + 2t_{j}\\sigma_{uv} \\end{eqnarray*}\\] where the last line reflects the fact that observations were taken at the same time points for all schools. We can derive the covariance terms in a similar fashion: \\[\\begin{eqnarray*} Cov(Y_{ij},Y_{ik}) &amp; = &amp; Cov(u_{i}+ v_{i}\\textstyle{Year08}_{ij}+\\epsilon_{ij}, u_{i}+v_{i}\\textstyle{Year08}_{ik}+\\epsilon_{ik}) \\\\ &amp; = &amp; Var(u_{i}) + \\textstyle{Year08}_{ij}\\textstyle{Year08}_{ik} Var(v_{i}) + (\\textstyle{Year08}_{ij} + \\textstyle{Year08}_{ik}) Cov(u_{i},v_{i}) \\\\ &amp; = &amp; \\sigma_{u}^{2} + t_{j}t_{k} \\sigma_{v}^{2} + (t_{j}+t_{k})\\sigma_{uv} \\end{eqnarray*}\\] In Model C, we obtained the following estimates of variance components: \\(\\hat{\\sigma}^{2}=8.69\\), \\(\\hat{\\sigma}^{2}_{u}=35.97\\), \\(\\hat{\\sigma}^{2}_{v}=0.23\\), and \\(\\hat{\\sigma}_{uv}=\\hat{\\rho}\\hat{\\sigma_{u}}\\hat{\\sigma_{v}}=1.81\\). Therefore, our two level model implicitly imposes this covariance structure on within subject observations: \\[ Cov(\\mathbf{Y}_i) = \\left[ \\begin{array}{cccc} 44.66 &amp; &amp; \\\\ 37.77 &amp; 48.50 &amp; \\\\ 39.58 &amp; 41.84 &amp; 52.80 \\end{array} \\right] \\] and this covariance matrix can be converted into a within-subject correlation matrix: \\[ Corr(\\mathbf{Y}_i) = \\left[ \\begin{array}{cccc} 1 &amp; &amp; \\\\ .812 &amp; 1 &amp; \\\\ .815 &amp; .827 &amp; 1 \\end{array} \\right] \\] 9.8 Notes on Using R (Optional) The model below is our final model with \\(\\sigma_{uv}\\) set to 0—i.e., we have added the restriction that Level Two error terms are uncorrelated. Motivation for this restriction came from repeated estimates of correlation in different versions of the final model near 1, when empirically a slightly negative correlation might be expected. As we will describe in Chapter ??, inclusion of the Level Two correlation as a model parameter appears to lead to boundary constraints—maximum likelihood parameter estimates near the maximum or minimum allowable value for a parameter. A likelihood ratio test using full maximum likelihood estimates confirms that the inclusion of a correlation term does not lead to an improved model (LRT test statistic = .223 on 1 df, \\(p=.637\\)). Estimates of fixed effects and their standard errors are extremely consistent with the full model in Section 8.10; only the estimates of the variability in \\(\\sigma_{1}\\) is noticeably higher. # Modified final model model.f2a &lt;- lmer(MathAvgScore ~ charter + urban + SchPctFree + SchPctSped + charter:year08 + urban:year08 + SchPctSped:year08 + year08 + (1|schoolid) + (0+year08|schoolid), REML=T, data=chart.long) # print(summary(model.f2a), correlation=FALSE, show.resids=FALSE) tmp &lt;- capture.output(summary(model.f2a)) cat(tmp[c(2:4, 12:31)], sep=&#39;\\n&#39;) Formula: MathAvgScore ~ charter + urban + SchPctFree + SchPctSped + charter:year08 + urban:year08 + SchPctSped:year08 + year08 + (1 | schoolid) + -3.3109 -0.4960 0.0150 0.4728 4.0940 Random effects: Groups Name Variance Std.Dev. schoolid (Intercept) 17.3551 4.1659 schoolid.1 year08 0.1139 0.3375 Residual 8.7163 2.9523 Number of obs: 1733, groups: schoolid, 618 Fixed effects: Estimate Std. Error t value (Intercept) 661.017692 0.515461 1282.4 charter -3.224687 0.703173 -4.6 urban -1.116628 0.430421 -2.6 SchPctFree -0.152945 0.008096 -18.9 SchPctSped -0.117770 0.020739 -5.7 year08 2.142708 0.202090 10.6 charter:year08 1.033413 0.317175 3.3 urban:year08 -0.524420 0.187679 -2.8 SchPctSped:year08 -0.046722 0.010219 -4.6 cat(&quot; AIC = &quot;, AIC(model.f2a), &#39;\\n&#39;, &quot;BIC = &quot;, BIC(model.f2a)) AIC = 9882.823 BIC = 9948.314 # LRT comparing final model in chapter (model.f2ml) with maximum # likelihood estimates to modified final model (model.f2aml) # with uncorrelated Level Two errors. anova(model.f2ml,model.f2aml) ... Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) model.f2aml 12 9855.5 9920.9 -4915.7 9831.5 model.f2ml 13 9857.2 9928.2 -4915.6 9831.2 0.2231 1 0.6367 ... 9.9 Exercises 9.9.1 Conceptual Exercises Parenting and Gang Activity. Walker-Barnes and Mason (2001) describe “Ethnic differences in the effect of parenting on gang involvement and gang delinquency: a longitudinal, hierarchical linear modeling perspective”. In this study, 300 ninth graders from one high school in an urban southeastern city were assessed at the beginning of the school year about their gang activity, the gang activity of their peers, behavior of their parents, and their ethnic and cultural heritage. Then, information about their gang activity was collected at 7 additional occasions during the school year. For this study: (a) give the observational units at Level One and Level Two, and (b) list potential explanatory variables at both Level One and Level Two. Describe the difference between the wide and long formats for longitudinal data in this study. Describe scenarios or research questions in which a lattice plot would be more informative than a spaghetti plot, and other scenarios or research questions in which a spaghetti plot would be preferable to a lattice plot. Walker-Barnes and Mason summarize their analytic approach in the following way: The first series [of analyses] tested whether there was overall change and/or significant individual variability in gang [activity] over time, regardless of parenting behavior, peer behavior, or ethnic and cultural heritage. Second, given the well documented relation between peer and adolescent behavior . . . HLM analyses were conducted examining the effect of peer gang [activity] on [initial gang activity and] changes in gang [activity] over time. Finally, four pairs of analyses were conducted examining the role of each of the four parenting variables on [initial gang activity and] changes in gang [activity]. The last series of analyses controlled for peer gang activity and ethnic and cultural heritage, in addition to examining interactions between parenting and ethnic and cultural heritage. Although the authors examined four parenting behaviors—behavioral control, lax control, psychological control, and parental warmth—they did so one at a time, using four separate multilevel models. Based on their description, write out a sample model from each of the three steps in the series. For each model, (a) write out the two-level model for predicting gang activity, (b) write out the corresponding composite model, and (c) determine how many model parameters (fixed effects and variance components) must be estimated. The table below shows a portion of Table 2: Results of Hierarchical Linear Modeling Analyses Modeling Gang Involvement from Walker-Barnes and Mason (2001). Provide interpretations of significant coefficients in context. Table 9.4: A portion of Table 2: Results of Hierarchical Linear Modeling Analyses Modeling Gang Involvement from Walker-Barnes and Mason (2001). These columns focus on the parenting behavior of psychological control. Predictor γ SE Intercept (initial Status) Base (intercept for predicting int term) -.219 .160 Peer behavior .252** .026 Black Ethnicity .671* .289 White/Other ethnicity .149 .252 Parenting .076 .050 Black Ethnicity X Parenting -.161+ .088 White/Other ethnicity X Parenting -.026 .082 . Slope(change) . Base(intercept for predicting slope term) .028 .030 . Peer behavior -.011* .005 . Black ethnicity -.132* .054 . White/Other ethnicity -.059 .046 . Parenting -.015+ .009 . Black Ethnicity X Parenting -.048** .017 . White/Other ethnicity X Parenting .016 .015 Note: Table reports values for \\(\\gamma\\)s in the final model with all variables entered. * \\(p&lt;.05\\); ** \\(p&lt;.01\\); + \\(p&lt;.10\\) Charter Schools. Differences exist in both sets of boxplots in Figure 9.12. What do these differences imply for multilevel modeling? What implications do the scatterplots in Figure 9.14 (b) and (c) have for multilevel modeling? What implications does the boxplot in Figure 9.14 (a) have? What are the implications of Figure 9.15 for multilevel modeling? Sketch a set of boxplots to indicate an obvious interaction between percent special education and percent non-white in modeling 2008 math scores. Where would this interaction appear in the multilevel model? In Model A, \\(\\sigma^2\\) is defined as the variance in within-school deviations and \\(\\sigma^2_u\\) is defined as the variance in between-school deviations. Give potential sources of within-school and between-school deviations. In Chapter 8 Model B is called the “random slopes and intercepts model”, while in this chapter Model B is called the “unconditional growth model”. Are these models essentially the same or systematically different? Explain. In Section 8.7, why don’t we examine the pseudo \\(R^2\\) value for Level Two? If we have test score data from 2001-2010, explain how we’d create new variables to fit a piecewise model. In Section 9.6.2, could we have used percent free and reduced lunch as a Level One covariate rather than 2008 percent free and reduced lunch as a Level Two covariate? If so, explain how interpretations would have changed. What if we had used average percent free and reduced lunch over all three years or 2010 percent free and reduced lunch instead of 2008 percent free and reduced lunch - how would this have changed the interpretation of this term? In Section 9.6.2, why do we look at a 10 percent increase in the percentage of students receiving free and reduced lunch when interpreting \\(\\hat{\\alpha}_{2}\\)? In Section 9.6.3, if the gap in 2008 math scores between charter and non-charter schools differed for schools of different poverty levels (as measured by percent free and reduced lunch), how would the final model have differed? Explain in your own words why “the error structure at Level Two is not the same as the within-school covariance structure among observations”. Here is the estimated unstructured covariance matrix for Model C: \\[ Cov(\\mathbf{Y}_i) = \\left[ \\begin{array}{cccc} 41.87 &amp; &amp; \\\\ 36.46 &amp; 48.18 &amp; \\\\ 35.20 &amp; 39.84 &amp; 45.77 \\end{array} \\right] \\] Explain why this matrix cannot represent an estimated covariance matrix with a compound symmetry, autoregressive, or Toeplitz structure. Also explain why it cannot represent our standard two-level model. 9.9.2 Guided Exercise Teen Alcohol Use. Curran, Stice, and Chassin (1997) collected data on 82 adolescents at three time points starting at age 14 to assess factors that affect teen drinking behavior. Key variables in the data set (source: Singer and Willett, 2003) are as follows: id = numerical identifier for subject age = 14, 15, or 16 coa = 1 if the teen is a child of an alcoholic parent; 0 otherwise male = 1 if male; 0 if female peer = a measure of peer alcohol use, taken when each subject was 14. This is the square root of the sum of two 6-point items about the proportion of friends who drink occasionally or regularly. alcuse = the primary response. Four items—(a) drank beer or wine, (b) drank hard liquor, (c) 5 or more drinks in a row, and (d) got drunk—were each scored on an 8-point scale, from 0=“not at all” to 7=“every day”. Then alcuse is the square root of the sum of these four items. Primary research questions included: do trajectories of alcohol use differ by parental alcoholism? do trajectories of alcohol use differ by peer alcohol use? Identify Level One and Level Two predictors. Perform a quick EDA. What can you say about the shape of alcuse, and the relationship between alcuse and coa, male, and peer? Appeal to plots and summary statistics in making your statements. Generate a plot as in Figure 9.4 with alcohol use over time for all 82 subjects. Comment. Generate three spaghetti plots with loess fits similar to Figure 9.7 (one for coa, one formale, and one after creating a binary variable from peer). Comment on what you can conclude from each plot. Fit a linear trend to the data from each of the 82 subjects using age as the time variable. Generate histograms as in Figure 9.10 showing the results of these 82 linear regression lines, and generate pairs of boxplots as in Figure 9.10 for coa and male. No commentary necessary. [Hint: to produce Figure 9.10, you will need a data frame with one observation per subject.] Repeat 5 using centered age (age14=age-14) as the time variable. Also generate a pair of scatterplots as in Figure 9.14 for peer alcohol use. Comment on trends you observe in these plots. [Hint: after forming age14, append it to your current data frame.] Discuss similarities and differences between (5) and (6). Why does using age14 as the time variable make more sense in this example? (Model A) Run an unconditional means model. Report and interpret the intraclass correlation coefficient. (Model B) Run an unconditional growth model with age14 as the time variable at Level One. Report and interpret estimated fixed effects, using proper notation. Also report and interpret a pseudo-Rsquare value. (Model C) Build upon the unconditional growth model by adding the effects of having an alcoholic parent and peer alcohol use in both Level Two equations. Report and interpret all estimated fixed effects, using proper notation. (Model D) Remove the child of an alcoholic indicator variable as a predictor of slope in Model C (it will still be a predictor of intercept). Write out Model D as both a two-level and a composite model using proper notation (including error distributions); how many parameters (fixed effects and variance components) must be estimated? Compare Model D to Model C using an appropriate method and state a conclusion. 9.9.3 Open-ended Exercises UCLA Nurse Blood Pressure Study. A study by Goldstein and Shapiro (2000) collected information from 203 registered nurses in the Los Angeles area between 24 and 50 years of age on blood pressure and potential factors that contribute to hypertension. This information includes family history, including whether the subject had one or two hypertensive parents, as well as a wide range of measures of the physical and emotional condition of each nurse throughout the day. Researchers sought to study the links between blood pressure and family history, personality, mood changes, working status, and menstrual phase. Data from this study provided by Weiss (2005) includes observations (40-60 per nurse) repeatedly taken on the 203 nurses over the course of a single day. The first blood pressure measurement was taken half an hour before the subject’s normal start of work, and was measured approximately every 20 minutes for the rest of the day. At each blood pressure reading, the nurses also rate their mood on several dimensions, including how stressed they feel at the moment the blood pressure is taken. In addition, the activity of each subject during the 10 minutes before each reading was measured using an actigraph worn on the waist. Each of the variables is described below: SNUM: subject identification number SYS: systolic blood pressure (mmHg) DIA: diastolic blood pressure (mmHg) HRT: heart rate (beats per minute) MNACT5: activity level (frequency of movements in 1-minute intervals, over a 10-minute period ) PHASE: menstrual phase (follicular—beginning with the end of menstruation and ending with ovulation, or luteal—beginning with ovulation and ending with pregnancy or menstruation) DAY: workday or non-workday POSTURE: position during blood pressure measurement—either sitting, standing, or reclining STR, HAP, TIR: self-ratings by each nurse of their level of stress, happiness and tiredness at the time of each blood pressure measurement on a 5-point scale, with 5 being the strongest sensation of that feeling and 1 the weakest AGE: age in years FH123: coded as either NO (no family history of hypertension), YES (1 hypertensive parent), or YESYES (both parents hypertensive) time: in minutes from midnight timept: number of the measurement that day (approximately 50 for each subject) timepass: time in minutes beginning with 0 at time point 1 Using systolic blood pressure as the primary response, write a short report detailing factors that are significantly associated with higher systolic blood pressure. Be sure to support your conclusions with appropriate exploratory plots and multilevel models. In particular, how are work conditions—activity level, mood, and work status—related to trends in blood pressure levels? As an appendix to your report, describe your modeling process—how did you arrive at your final model, which covariates are Level One or Level Two covariates, what did you learn from exploratory plots, etc. Potential alternative directions: consider diastolic blood pressure or heart rate as the primary response variable, or even try modeling emotion rating using a multilevel model. Completion Rates at US Colleges. Education researchers wonder which factors most affect the completion rates at US colleges. Using the IPEDS database containing data from 1310 institutions over the years 2002-2009, the following variables were assembled: id = unique identification number for each college or university Response: rate = completion rate (number of degrees awarded per 100 students enrolled) Level 1 predictors: year instpct = percentage of students who receive an institutional grant instamt = typical amount of an institutional grant among recipients (in $1000s) Level 2 predictors: faculty = mean number of full-time faculty per 100 students in between 2002-2009 tuition = mean yearly tuition in between 2002 and 2009 (in $1000s) Perform exploratory analyses and run multilevel models to determine significant predictors of baseline (2002) completion rates and changes in completion rates between 2002 and 2009. In particular, is the percentage of grant recipients or the average institutional grant awarded related to completion rate? "],
["ch-3level.html", "Chapter 10 Multilevel Data With More Than Two Levels 10.1 Learning Objectives 10.2 Case Studies: Seed Germination 10.3 Initial Exploratory Analyses 10.4 Initial models: unconditional means and unconditional growth 10.5 Encountering boundary constraints 10.6 Parametric bootstrap testing 10.7 Exploding variance components 10.8 Building to a final model 10.9 Covariance structure (Optional) 10.10 Notes on Using R (Optional) 10.11 Exercises", " Chapter 10 Multilevel Data With More Than Two Levels 10.1 Learning Objectives After finishing this chapter, you should be able to: Extend the standard multilevel model to cases with more than two levels. Apply exploratory data analysis techniques specific to data from more than two levels. Formulate multilevel models including the variance-covariance structure. Build and understand a taxonomy of models for data with more than two levels. Interpret parameters in models with more than two levels. Develop strategies for handling an exploding number of parameters in multilevel models. Recognize when a fitted model has encountered boundary constraints and understand strategies for moving forward. Understand how a parametric bootstrap test of significance works and when it might be useful. 10.2 Case Studies: Seed Germination It is estimated that 82-99% of historic tallgrass prairie ecosystems have been converted to agricultural use (Baer et al. 2002). A prime example of this large scale conversion of native prairie to agricultural purposes can be seen in Minnesota, where less than 1% of the prairies that once existed in the state still remain (Camill et al. 2004). Such large scale alteration of prairie communities has been associated with numerous problems. For example, erosion and decomposition that readily take place in cultivated soils have increased atmospheric CO2 levels and increased nitrogen inputs to adjacent waterways (Baer et al. 2002, Camill et al. 2004, Knops and Tilman 2000). In addition, cultivation practices are known to affect rhizosphere composition as tilling can disrupt networks of soil microbes (Allison et al. 2005). The rhizosphere is the narrow region of soil that is directly influenced by root secretions and associated soil microorganisms; much of the nutrient cycling and disease suppression needed by plants occur immediately adjacent to roots. It is important to note that microbial communities in prairie soils have been implicated with plant diversity and overall ecosystem function by controlling carbon and nitrogen cycling in the soils (Zak et al. 2003). There have been many responses to these claims, but one response in recent years is reconstruction of the native prairie community. These reconstruction projects provide new habitat for a variety of native prairie species, yet it is important to know as much as possible about the outcomes of prairie reconstruction projects in order to ensure that a functioning prairie community is established. The ecological repercussions resulting from prairie reconstruction are not well known. For example, all of the aforementioned changes associated with cultivation practices are known to affect the subsequent reconstructed prairie community (Baer et al. 2002, Camill et al. 2004), yet there are few explanations for this phenomenon. For instance, prairies reconstructed in different years (using the same seed combinations and dispersal techniques) have yielded disparate prairie communities. Researchers at a small Midwestern college decided to experimentally explore the underlying causes of variation in reconstruction projects in order to make future projects more effective. Introductory ecology classes were organized to collect longitudinal data on native plant species grown in a greenhouse setting, using soil samples from surrounding lands. We will examine their data to compare germination and growth of two species of prairie plants—leadplants (Amorpha canescens) and coneflowers (Ratibida pinnata)—in soils taken from a remnant (natural) prairie, a cultivated (agricultural) field, and a restored (reconstructed) prairie. Additionally, half of the sampled soil was sterilized to determine if rhizosphere differences were responsible for the observed variation, so we will examine the effects of sterilization as well. The data we’ll examine was collected through an experiment run using a 3x2x2 factorial design, with 3 levels of soil type (remnant, cultivated, and restored), 2 levels of sterilization (yes or no), and 2 levels of species (leadplant and coneflower). Each of the 12 treatments (unique combinations of factor levels) was replicated in 6 pots, for a total of 72 pots. Six seeds were planted in each pot (although a few pots had 7 or 8 seeds), and initially student researchers recorded days to germination (defined as when two leaves are visible), if germination occurred. In addition, the height of each germinated plant (in mm) was measured at 13, 18, 23, and 28 days after planting. The study design is illustrated in the diagram below. The design of the seed germination study 10.3 Initial Exploratory Analyses 10.3.1 Data Organization Data for Case Study 10.2 contains the following variables: pot = Pot plant was grown in (1-72) plant = Unique plant identification number species = L for leadplant and C for coneflower soil = STP for reconstructed prairie, REM for remnant prairie, and CULT for cultivated land sterile = Y for yes and N for no germin = Y if plant germinated, N if not hgt13 = height of plant (in mm) 13 days after seeds planted hgt18 = height of plant (in mm) 18 days after seeds planted hgt23 = height of plant (in mm) 23 days after seeds planted hgt28 = height of plant (in mm) 28 days after seeds planted This data is stored in wide format, with one row per plant (see 12 sample plants in Table 10.1). As we have done in previous multilevel analyses, we will convert to long format (one observation per plant-time combination) after examining the missing data pattern and removing any plants with no growth data. In this case, we are almost assuredly losing information by removing plants with no height data at all four time points, since these plants did not germinate, and there may well be differences between species, soil type, and sterilization with respect to germination rates. We will handle this possibility by analyzing germination rates separately (see Chapter ??); the analysis in this chapter will focus on effects of species, soil type, and sterilization on initial growth and growth rate among plants that germinate. Table 10.1: A snapshot of data (Plants 231-246) from the Seed Germination case study in wide format. pot plant soil sterile species germin hgt13 hgt18 hgt23 hgt28 135 23 231 CULT N C Y 1.1 1.4 1.6 1.7 136 23 232 CULT N C Y 1.3 2.2 2.5 2.7 137 23 233 CULT N C Y 0.5 1.4 2.0 2.3 138 23 234 CULT N C Y 0.3 0.4 1.2 1.7 139 23 235 CULT N C Y 0.5 0.5 0.8 2.0 140 23 236 CULT N C Y 0.1 NA NA NA 141 24 241 STP Y L Y 1.8 2.6 3.9 4.2 142 24 242 STP Y L Y 1.3 1.7 2.8 3.7 143 24 243 STP Y L Y 1.5 1.6 3.9 3.9 144 24 244 STP Y L Y NA 1.0 2.3 3.8 145 24 245 STP Y L N NA NA NA NA 146 24 246 STP Y L N NA NA NA NA Although the experimental design called for \\(72*6=432\\) plants, the wide data set has 437 plants because a few pots had more than six plants (likely because two of the microscopically small seeds stuck together when planted). Of those 437 plants, 154 had no height data (did not germinate by the 28th day) and were removed from analysis (for example, see rows 145-146 in Table 10.1). 248 plants had complete height data (e.g., rows 135-139 and 141-143), 13 germinated later than the 13th day but had complete heights once they germinated (e.g., row 144), and 22 germinated and had measurable height on the 13th day but died before the 28th day (e.g., row 140). Ultimately, the long data set contains 1132 unique observations where plants heights were recorded; representation of plants 236-242 in the long data set can be seen in Table 10.2. Table 10.2: A snapshot of data (Plants 236-242) from the Seed Germination case study in long format. pot plant soil sterile species germin hgt time13 23 236 CULT N C Y 0.1 0 23 236 CULT N C Y NA 5 23 236 CULT N C Y NA 10 23 236 CULT N C Y NA 15 24 241 STP Y L Y 1.8 0 24 241 STP Y L Y 2.6 5 24 241 STP Y L Y 3.9 10 24 241 STP Y L Y 4.2 15 24 242 STP Y L Y 1.3 0 24 242 STP Y L Y 1.7 5 24 242 STP Y L Y 2.8 10 24 242 STP Y L Y 3.7 15 Notice the three-level structure of this data. Treatments (levels of the three experimental factors) were assigned at the pot level, then multiple plants were grown in each pot, and multiple measurements were taken over time for each plant. Our multilevel analysis must therefore account for pot-to-pot variability in height measurements (which could result from factor effects), plant-to-plant variability in height within a single pot, and variability over time in height for individual plants. In order to fit such a three-level model, we must extend the two-level model which we have used thus far. 10.3.2 Exploratory Analyses We start by taking an initial look at the effect of Level Three covariates (factors applied at the pot level: species, soil type, and sterilization) on plant height, pooling observations across pot, across plant, and across time of measurement within plant. First, we observe that the initial balance which existed after randomization of pot to treatment no longer holds. After removing plants that did not germinate (and therefore had no height data), more height measurements exist for coneflowers (n=704, compared to 428 for leadplants), soil from restored prairies (n=524, compared to 288 for cultivated land and 320 for remnant prairies), and unsterilized soil (n=612, compared to 520 for sterilized soil). This imbalance indicates possible factor effects on germination rate; we will take up those hypotheses in Chapter ??. In this chapter, we will focus on the effects of species, soil type, and sterilization on the growth patterns of plants that germinate. Because we suspect that height measurements over time for a single plant are highly correlated, while height measurements from different plants from the same pot are relatively uncorrelated, we calculate mean height per plant (over all available time points) before generating exploratory plots investigating Level Three factors. Figure 10.1 then examines the effects of soil type and sterilization separately by species. Sterilization seems to have a bigger benefit for coneflowers, while soil from remnant prairies seems to lead to smaller leadplants and taller coneflowers. Figure 10.1: Plant height comparisons of (a) soil type and (b) sterilization within species. Each plant is represented by the mean height over all measurements at all time points for that plant. We also use spaghetti plots to examine time trends within species to see (a) if it is reasonable to assume linear growth between Day 13 and Day 28 after planting, and (b) if initial height and rate of growth is similar in the two species. Figure 10.2 illustrates differences between species. While both species have similar average heights 13 days after planting, coneflowers appear to have faster early growth which slows later, while leadplants have a more linear growth rate which culminates in greater average heights 28 days after planting. Coneflowers also appear to have greater variability in initial height and growth rate, although there are more coneflowers with height data. Figure 10.2: Spaghetti plot by species with loess fit. Each “line” represents one plant. Exploratory analyses such as these confirm the suspicions of biology researchers that leadplants and coneflowers should be analyzed separately. Because of biological differences, it is expected that these two species will show different growth patterns and respond differently to treatments such as fertilization. Coneflowers are members of the aster family, growing up to 4 feet tall with their distinctive gray seed heads and drooping yellow petals. Leadplants, on the other hand, are members of the bean family, with purple flowers, a height of 1 to 3 feet, and compound greyish green leaves which look to be dusted with white lead. Leadplants have deep root systems and are symbiotic N-fixers, which means they might experience stifled growth in sterilized soil compared with other species. For the remainder of this chapter, we will focus on leadplants and how their growth patterns are affected by soil type and sterilization. You will have a chance to analyze coneflower data later in the Exercises section. Lattice plots, illustrating several observational units simultaneously, each with fitted lines where appropriate, are also valuable to examine during the exploratory analysis phase. Figure 10.3 shows height over time for 24 randomly selected leadplants that germinated in this study, with a fitted linear regression line. Linearity appears reasonable in most cases, although there is some variability in the intercepts and a good deal of variability in the slopes of the fitted lines. These intercepts and slopes by plant, of course, will be potential parameters in a multilevel model which we will fit to this data. Given the three-level nature of this data, it is also useful to examine a spaghetti plot by pot (Figure 10.4). While linearity appears to reasonably model the average trend over time within pot, we see differences in the plant-to-plant variability within pot, but some consistency in intercept and slope from pot to pot. Figure 10.3: Lattice plot of linear trends fit to 24 randomly selected leadplants. One plant with only a single height measurement has no associated regression line. Figure 10.4: Spaghetti plot for leadplants by pot with loess fit. Spaghetti plots can also be an effective tool for examining the potential effects of soil type and sterilization on growth patterns of leadplants. Figure 10.5 and Figure 10.6 illustrate how the growth patterns of leadplants depend on soil type and sterilization. In general, we observe slower growth in soil from remnant prairies and soil that has not been sterilized. Figure 10.5: Spaghetti plot for leadplants by soil type with loess fit. Figure 10.6: Spaghetti plot for leadplants by sterilization with loess fit. We can further explore the variability in linear growth among plants and among pots by fitting regression lines and examining the estimated intercepts and slopes, as well as the corresponding R-squared values. Figures 10.7 and 10.8 provide just such an analysis, where Figure 10.7 shows results of fitting lines by plant, and Figure 10.8 shows results of fitting lines by pot. Certain caveats accompany these summaries. In the case of fitted lines by plant, each plant is given equal weight regardless of the number of observations (2-4) for a given plant, and in the case of fitted lines by pot, a line is estimated by simply pooling all observations from a given pot, ignoring the plant from which the observations came, and equally weighting pots regardless of how many plants germinated and survived to Day 28. Nevertheless, the summaries of fitted lines provide useful information. When fitting regression lines by plant, we see a mean intercept of 1.52 (SD=0.66), indicating an estimated average height at 13 days of 1.5 mm, and a mean slope of 0.114 mm per day of growth from Days 13 to 28 (SD=0.059). Most R-squared values were strong (e.g., 84% were above 0.8). Summaries of fitted regression lines by pot show similar mean intercepts (1.50) and slopes (0.107), but somewhat less variability pot-to-pot than we observed plant-to-plant (SD=0.46 for intercepts and SD=0.050 for slopes). Figure 10.7: Histograms of (a) intercepts, (b) slopes, and (c) R-squared values for linear fits across all leadplants. Figure 10.8: Histograms of (a) intercepts, (b) slopes, and (c) R-squared values for linear fits across all pots with leadplants. Another way to examine variability due to plant vs. variability due to pot is through summary statistics. Plant-to-plant variability can be estimated by averaging standard deviations from each pot (.489 for intercepts and .039 for slopes), while pot-to-pot variability can be estimated by finding the standard deviation of average intercept (.478) or slope (.051) within pot. Based on these rough measurements, variability due to plants and pots is comparable. Fitted lines by plant and pot are modeled using a centered time variable (time13), adjusted so that the first day of height measurements (13 days after planting) corresponds to time13=0. This centering has two primary advantages. First, the estimated intercept becomes more interpretable. Rather than representing height on the day of planting (which should be 0 mm, but which represents a hefty extrapolation from our observed range of days 13 to 28), the intercept now represents height on Day 13. Second, the intercept and slope are much less correlated (r=-0.16) than when uncentered time is used, which improves the stability of future models. Fitted intercepts and slopes by plant can be used for an additional exploratory examination of factor effects to complement those from the earlier spaghetti plots. Figure 10.9 complements Figure 10.2, again showing differences between species—coneflowers tend to start smaller and have slower growth rates, although they have much more variability in growth patterns than leadplants. Returning to our focus on leadplants, Figure 10.10 shows that plants grown in soil from cultivated fields tends to be taller at Day 13, and plants grown in soil from remnant prairies tend to grow more slowly than plants grown in other soil types. Figure 10.11 shows the strong tendency for plants grown in sterilized soil to grow faster than plants grown in non-sterilized soil. We will soon see if our fitted multilevel models support these observed trends. Figure 10.9: Boxplots of (a) intercepts and (b) slopes for all plants by species, based on a linear fit to height data from each plant. Figure 10.10: Boxplots of (a) intercepts and (b) slopes for all leadplants by soil type, based on a linear fit to height data from each plant. Figure 10.11: Boxplots of (a) intercepts and (b) slopes for all leadplants by sterilization, based on a linear fit to height data from each plant. Since we have time at Level One, any exploratory analysis of Case Study 10.2 should contain an investigation of the variance-covariance structure within plant. Figure 10.12 shows the potential for an autocorrelation structure in which the correlation between observations from the same plant diminishes as the time between measurements increases. Residuals five days apart have correlations ranging from .77 to .91, while measurements ten days apart have correlations of .62 and .70, and measurements fifteen days apart have correlation of .58. Figure 10.12: Correlation structure within plant. The upper right contains correlation coefficients between residuals at pairs of time points, the lower left contains scatterplots of the residuals at time point pairs, and the diagonal contains histograms of residuals at each of the four time points. 10.4 Initial models: unconditional means and unconditional growth The structure and notation for three level models will closely resemble the structure and notation for two level models, just with extra subscripts. Therein lies some of the power of multilevel models—extensions are relatively easy and allow you to control for many sources of variability, obtaining more precise estimates of important parameters. However, the number of variance component parameters to estimate can quickly mushroom as covariates are added at lower levels, so implementing simplifying restrictions will often become necessary (see Section 10.7). We once again begin with the unconditional means model, in which there are no predictors at any level, in order to assess the amount of variation at each level. Here, Level Three is pot, Level Two is plant within pot, and Level One is time within plant. Using model formulations at each of the three levels, the unconditional means three-level model can be expressed as: Level One (timepoint within plant): \\[\\begin{equation} Y_{ijk} = a_{ij}+\\epsilon_{ijk} \\textrm{ where } \\epsilon_{ijk}\\sim N(0,\\sigma^2) \\tag{10.1} \\end{equation}\\] Level Two (plant within pot): \\[\\begin{equation} a_{ij} = a_{i}+u_{ij} \\textrm{ where } u_{ij}\\sim N(0,\\sigma_{u}^{2}) \\tag{10.2} \\end{equation}\\] Level Three (pot): \\[\\begin{equation} a_{i} = \\alpha_{0}+\\tilde{u}_{i} \\textrm{ where } \\tilde{u}_{i} \\sim N(0,\\sigma_{\\tilde{u}}^{2}) \\tag{10.3} \\end{equation}\\] where the heights of plants from different pots are considered independent, but plants from the same pot are correlated as well as measurements at different times from the same plant. Keeping track of all the model terms, especially with three subscripts, is not a trivial task, but it’s worth spending time thinking through. Here is a quick guide to the meaning of terms found in our three-level model: \\(Y_{ijk}\\) is the height (in mm) of plant \\(j\\) from pot \\(i\\) at time \\(k\\) \\(a_{ij}\\) is the true mean height for plant \\(j\\) from pot \\(i\\) across all time points. This is not considered a model parameter, since we further model \\(a_{ij}\\) at Level Two. \\(a_{i}\\) is the true mean height for pot \\(i\\) across all plants from that pot and all time points. This is also not considered a model parameter, since we further model \\(a_{i}\\) at Level Three. \\(\\alpha_{0}\\) is a fixed effects model parameter representing the true mean height across all pots, plants, and time points \\(\\epsilon_{ijk}\\) describes how far an observed height \\(Y_{ijk}\\) is from the mean height for plant \\(j\\) from pot \\(i\\) \\(u_{ij}\\) describe how far the mean height of plant \\(j\\) from pot \\(i\\) is from the mean height of all plants from pot \\(i\\) \\(\\tilde{u}_{i}\\) describes how far the mean height of all observations from pot \\(i\\) is from the overall mean height across all pots, plants, and time points. None of the error terms (\\(\\epsilon, u, \\tilde{u}\\)) are considered model parameters; they simply account for differences between the observed data and expected values under our model. \\(\\sigma^2\\) is a variance component (random effects model parameter) that describes within-plant variability over time \\(\\sigma_{u}^{2}\\) is the variance component describing plant-to-plant variability within pot \\(\\sigma_{\\tilde{u}}^{2}\\) is the variance component describing pot-to-pot variability. The three-level unconditional means model can also be expressed as a composite model: \\[\\begin{equation} Y_{ijk}=\\alpha_{0}+\\tilde{u}_{i}+u_{ij}+\\epsilon_{ijk} \\tag{10.4} \\end{equation}\\] and this composite model can be fit using statistical software: Formula: hgt ~ 1 + (1 | plant) + (1 | pot) Random effects: Groups Name Variance Std.Dev. plant (Intercept) 0.27817 0.5274 pot (Intercept) 0.04873 0.2207 Residual 0.72782 0.8531 Number of obs: 413, groups: plant, 107; pot, 32 Fixed effects: Estimate Std. Error t value (Intercept) 2.38808 0.07887 30.28 From this output, we obtain estimates of our four model parameters: \\(\\hat{\\alpha}_{0}=2.39=\\) the mean height (in mm) across all time points, plants, and pots. \\(\\hat{\\sigma}^2=0.728=\\) the variance over time within plants. \\(\\hat{\\sigma}_{u}^{2}=0.278=\\) the variance between plants from the same pot. \\(\\hat{\\sigma}_{\\tilde{u}}^{2}=0.049=\\) the variance between pots. From the estimates of variance components, 69.0% of total variability in height measurements is due to differences over time for each plant, 26.4% of total variability is due to differences between plants from the same pot, and only 4.6% of total variability is due to difference between pots. Accordingly, we will next explore whether the incorporation of time as a linear predictor at Level One can reduce the unexplained variability within plant. The unconditional growth model introduces time as a predictor at Level One, but there are still no predictors at Levels Two or Three. The unconditional growth model allows us to assess how much of the within-plant variability (the variability among height measurements from the same plant at different time points) can be attributed to linear changes over time, while also determining how much variability we see in the intercept (Day 13 height) and slope (daily growth rate) from plant-to-plant and pot-to-pot. Later, we can model plant-to-plant and pot-to-pot differences in intercepts and slopes with Level Two and Three covariates. The three-level unconditional growth model (Model B) can be specified either using formulations at each level: Level One (timepoint within plant): \\[\\begin{equation} Y_{ijk} = a_{ij}+b_{ij}\\textstyle{time}_{ijk}+\\epsilon_{ijk} \\tag{10.5} \\end{equation}\\] Level Two (plant within pot): \\[\\begin{eqnarray*} a_{ij} &amp; = &amp; a_{i}+u_{ij} \\\\ b_{ij} &amp; = &amp; b_{i}+v_{ij} \\end{eqnarray*}\\] Level Three (pot): \\[\\begin{eqnarray*} a_{i} &amp; = &amp; \\alpha_{0}+\\tilde{u}_{i} \\\\ b_{i} &amp; = &amp; \\beta_{0}+\\tilde{v}_{i} \\end{eqnarray*}\\] or as a composite model: \\[\\begin{equation} Y_{ijk}=[\\alpha_{0}+\\beta_{0}\\textstyle{time}_{ijk}]+ [\\tilde{u}_{i}+{v}_{ij}+\\epsilon_{ijk}+(\\tilde{v}_{i}+{v}_{ij})\\textstyle{time}_{ijk}] \\tag{10.6} \\end{equation}\\] where \\(\\epsilon_{ijk}\\sim N(0,\\sigma^2)\\), \\[ \\left[ \\begin{array}{c} u_{ij} \\\\ v_{ij} \\end{array} \\right] \\sim N \\left( \\left[ \\begin{array}{c} 0 \\\\ 0 \\end{array} \\right], \\left[ \\begin{array}{cc} \\sigma_{u}^{2} &amp; \\\\ \\sigma_{uv} &amp; \\sigma_{v}^{2} \\end{array} \\right] \\right), \\] and \\[ \\left[ \\begin{array}{c} \\tilde{u}_{i} \\\\ \\tilde{v}_{i} \\end{array} \\right] \\sim N \\left( \\left[ \\begin{array}{c} 0 \\\\ 0 \\end{array} \\right], \\left[ \\begin{array}{cc} \\sigma_{\\tilde{u}}^{2} &amp; \\\\ \\sigma_{\\tilde{u}\\tilde{v}} &amp; \\sigma_{\\tilde{v}}^{2} \\end{array} \\right] \\right). \\] In this model, at Level One the trajectory for plant \\(j\\) from pot \\(i\\) is assumed to be linear, with intercept \\(a_{ij}\\) (height on Day 13) and slope \\(b_{ij}\\) (daily growth rate between Days 13 and 28); the \\(\\epsilon_{ijk}\\) terms capture the deviation between the true growth trajectory of plant \\(j\\) from pot \\(i\\) and its observed heights. At Level Two, \\(a_{i}\\) represents the true mean intercept and \\(b_{i}\\) represents the true mean slope for all plants from pot \\(i\\), while \\(u_{ij}\\) and \\(v_{ij}\\) capture the deviation between plant \\(j\\)’s true growth trajectory and the mean intercept and slope for pot \\(i\\). The deviations in intercept and slope at Level Two are allowed to be correlated through the covariance parameter \\(\\sigma_{uv}\\). Finally, \\(\\alpha_{0}\\) is the true mean intercept and \\(\\beta_{0}\\) is the true mean daily growth rate over the entire population of leadplants, while \\(\\tilde{u}_{i}\\) and \\(\\tilde{v}_{i}\\) capture the deviation between pot \\(i\\)’s true overall growth trajectory and the population mean intercept and slope. Note that between-plant and between-pot variability are both partitioned now into variability in initial status (\\(\\sigma_{u}^{2}\\) and \\(\\sigma_{\\tilde{u}}^{2}\\)) and variability in rates of change (\\(\\sigma_{v}^{2}\\) and \\(\\sigma_{\\tilde{v}}^{2}\\)). Using the composite model specification, the unconditional growth model can be fit to the seed germination data: Formula: hgt ~ time13 + (time13 | plant) + (time13 | pot) Random effects: Groups Name Variance Std.Dev. Corr plant (Intercept) 0.299163 0.54696 time13 0.001194 0.03456 0.28 pot (Intercept) 0.044219 0.21028 time13 0.001261 0.03551 -0.61 Residual 0.082157 0.28663 Number of obs: 413, groups: plant, 107; pot, 32 Fixed effects: Estimate Std. Error t value (Intercept) 1.537696 0.070304 21.87 time13 0.112117 0.007924 14.15 From this output, we obtain estimates of our nine model parameters (two fixed effects and seven variance components): \\(\\hat{\\alpha}_{0}=1.538=\\) the mean height of leadplants 13 days after planting. \\(\\hat{\\beta}_{0}=0.112=\\) the mean daily change in height of leadplants from 13 to 28 days after planting. \\(\\hat{\\sigma}=.287=\\) the standard deviation in within-plant residuals after accounting for time. \\(\\hat{\\sigma}_{u}=.547=\\) the standard deviation in Day 13 heights between plants from the same pot. \\(\\hat{\\sigma}_{v}=.0346=\\) the standard deviation in rates of change in height between plants from the same pot. \\(\\hat{\\rho}_{uv}=.280=\\) the correlation in plants’ Day 13 height and their rate of change in height. \\(\\hat{\\sigma}_{\\tilde{u}}=.210=\\) the standard deviation in Day 13 heights between pots. \\(\\hat{\\sigma}_{\\tilde{v}}=.0355=\\) the standard deviation in rates of change in height between pots. \\(\\hat{\\rho}_{\\tilde{u}\\tilde{v}}=-.610=\\) the correlation in pots’ Day 13 height and their rate of change in height. We see that, on average, leadplants have a height of 1.54 mm thirteen days after planting (pooled across pots and treatment groups), and their heights tend to grow by 0.11 mm per day, producing an average height at the end of the study (Day 28) of 3.22 mm. According to the t-values listed in R, both the Day 13 height and the growth rate are statistically significant. The estimated within-plant variance \\(\\hat{\\sigma}^2\\) decreased by 88.7% from the unconditional means model (from 0.728 to 0.082), implying that 88.7% of within-plant variability in height can be explained by linear growth over time. 10.5 Encountering boundary constraints Typically, with models consisting of three or more levels, the next step after adding covariates at Level One (such as time) is considering covariates at Level Two. In the seed germination experiment, however, there are no Level Two covariates of interest, and the treatments being studied were applied to pots (Level Three). We are primarily interested in the effects of soil type and sterilization on the growth of leadplants. Since soil type is a categorical factor with three levels, we can represent soil type in our model with indicator variables for cultivated lands (cult) and remnant prairies (rem), using reconstructed prairies as the reference level. For sterilization, we create a single indicator variable (strl) which takes on the value 1 for sterilized soil. Our Level One and Level Two models will look identical to those from Model B; our Level Three models will contain the new covariates for soil type (cult and rem) and sterilization (strl): \\[\\begin{eqnarray*} a_{i} &amp; = &amp; \\alpha_{0}+\\alpha_{1}\\textstyle{strl}_{i}+\\alpha_{2}\\textstyle{cult}_{i}+\\alpha_{3}\\textstyle{rem}_{i}+\\tilde{u}_{i} \\\\ b_{i} &amp; = &amp; \\beta_{0}+\\beta_{1}\\textstyle{strl}_{i}+\\beta_{2}\\textstyle{cult}_{i}+\\beta_{3}\\textstyle{rem}_{i}+\\tilde{v}_{i} \\end{eqnarray*}\\] where the error terms at Level Three follow the same multivariate normal distribution as in Model B. In our case, the composite model can be written as: \\[\\begin{eqnarray*} Y_{ijk} &amp; = &amp; (\\alpha_{0}+\\alpha_{1}\\textstyle{strl}_{i}+\\alpha_{2}\\textstyle{cult}_{i}+\\alpha_{3}\\textstyle{rem}_{i}+\\tilde{u}_{i}+u_{ij}) + \\\\ &amp; &amp; (\\beta_{0}+\\beta_{1}\\textstyle{strl}_{i}+\\beta_{2}\\textstyle{cult}_{i}+\\beta_{3}\\textstyle{rem}_{i}+\\tilde{v}_{i}+ v_{ij})\\textstyle{time}_{ijk}+\\epsilon_{ijk} \\\\ \\end{eqnarray*}\\] which, after combining fixed effects and random effects, can be rewritten as: \\[\\begin{eqnarray*} Y_{ijk} &amp; = &amp; [\\alpha_{0}+\\alpha_{1}\\textstyle{strl}_{i}+\\alpha_{2}\\textstyle{cult}_{i}+\\alpha_{3}\\textstyle{rem}_{i} + \\beta_{0}\\textstyle{time}_{ijk} + \\\\ &amp; &amp; \\beta_{1}\\textstyle{strl}_{i}\\textstyle{time}_{ijk}+\\beta_{2}\\textstyle{cult}_{i}\\textstyle{time}_{ijk}+ \\beta_{3}\\textstyle{rem}_{i}\\textstyle{time}_{ijk}] + \\\\ &amp; &amp; [\\tilde{u}_{i}+u_{ij}+\\epsilon_{ijk}+\\tilde{v}_{i}\\textstyle{time}_{ijk}+v_{ij}\\textstyle{time}_{ijk}] \\end{eqnarray*}\\] From the output below, the addition of Level Three covariates in Model C (cult, rem, strl, and their interactions with time) appears to provide a significant improvement (likelihood ratio test statistic = 32.2 on 6 df, \\(p&lt;.001\\)) to the unconditional growth model (Model B). Formula: hgt ~ time13 + strl + cult + rem + time13:strl + time13:cult + time13:rem + (time13 | plant) + (time13 | pot) Random effects: Groups Name Variance Std.Dev. Corr plant (Intercept) 0.2980203 0.54591 time13 0.0012081 0.03476 0.28 pot (Intercept) 0.0531506 0.23054 time13 0.0001317 0.01148 -1.00 Residual 0.0820769 0.28649 Number of obs: 413, groups: plant, 107; pot, 32 Fixed effects: Estimate Std. Error t value (Intercept) 1.502893 0.127001 11.834 time13 0.101069 0.008292 12.189 strl -0.076546 0.151368 -0.506 cult 0.130013 0.182723 0.712 rem 0.138387 0.176196 0.785 time13:strl 0.058917 0.010282 5.730 time13:cult -0.029765 0.012263 -2.427 time13:rem -0.035860 0.011978 -2.994 ... Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) modelbl 9 603.76 639.97 -292.88 585.76 modelcl 15 583.55 643.90 -276.78 553.55 32.202 6 1.493e-05 *** ... However, Model C has encountered a boundary constraint with an estimated Level 3 correlation between the intercept and slope error terms of -1. “Allowable” values of correlation coefficients run from -1 to 1; by definition, it is impossible to have a correlation between two error terms below -1. Thus, our estimate of -1 is right on the boundary of the allowable values. But how did this happen, and why is it potentially problematic? Consider a model in which we have two parameters that must be estimated: \\(\\beta_0\\) and \\(\\sigma^2\\). As the intercept, \\(\\beta_0\\) can take on any value; any real number is “allowable”. But, by definition, variance terms such as \\(\\sigma^2\\) must be non-negative; that is, \\(\\sigma^2 \\geq 0\\). Under the Principle of Maximum Likelihood, maximum likelihood estimators for \\(\\beta_0\\) and \\(\\sigma^2\\) will be chosen to maximize the likelihood of observing our given data. The left plot in Figure 10.13 shows hypothetical contours of the likelihood function \\(L(\\beta_0, \\sigma^2)\\); the likelihood is clearly maximized at \\((\\hat{\\beta}_0 , \\hat{\\sigma}^2)=(4,-2)\\). However, variance terms cannot be negative! A more sensible approach would have been to perform a constrained search for MLEs, considering any potential values for \\(\\beta_0\\) but only non-negative values for \\(\\sigma^2\\). This constrained search is illustrated in the right plot in Figure 10.13. In this case, the likelihood is maximized at \\((\\hat{\\beta}_0 , \\hat{\\sigma}^2)=(4,0)\\). Note that the estimated intercept did not change, but the estimated variance is simply set at the smallest allowable value – at the boundary constraint. Figure 10.13: Left (a): hypothetical contours of the likelihood function \\(L(\\beta_0, \\sigma^2)\\) with no restrictions on \\(\\sigma^2\\); the likelihood function is maximized at \\((\\hat{\\beta}_0, \\hat{\\sigma}^2)=(4,-2)\\). Right (b): hypothetical contours of the likelihood function \\(L(\\beta_0, \\sigma^2)\\) with the restriction that \\(\\sigma^2 \\geq 0\\); the constrained likelihood function is maximized at \\((\\hat{\\beta}_0, \\hat{\\sigma}^2)=(4,0)\\) Graphically, in this simple illustration, the effect of the boundary constraint is to alter the likelihood function from a nice hill (in the left plot in Figure 10.13) with a single peak at \\((4,-2)\\), to a hill with a huge cliff face where \\(\\sigma^2=0\\). The highest point overlooking this cliff is at \\((4,0)\\), straight down the hill from the original peak. In general, then, boundary constraints occur when the maximum likelihood estimator of at least one model parameter occurs at the limits of allowable values (such as estimated correlation coefficients of -1 or 1, or estimated variances of 0). Maximum likelihood estimates at the boundary tend to indicate that the likelihood function would be maximized at non-allowable values of that parameter, if an unconstrained search for MLEs was conducted. Most software packages, however, will only report maximum likelihood estimates with allowable values. Therefore, boundary constraints would ideally be avoided, if possible. What should you do if you encounter boundary constraints? Often, boundary constraints signal that your model needs to be reparameterized – i.e., you should alter your model to feature different parameters or ones that are interpreted differently. This can be accomplished in several ways: remove parameters, especially those variance and correlation terms which are being estimated on their boundaries fix the values of certain parameters; for instance, you could set two variance terms equal to each other, thereby reducing the number of unknown parameters to estimate by one transform covariates. Centering variables, standardizing variables, or changing units can all help stabilize a model. Numerical procedures for searching for and finding maximum likelihood estimates can encounter difficulties when variables have very high or low values, extreme ranges, outliers, or are highly correlated. Although it is worthwhile attempting to reparameterize models to remove boundary constraints, sometimes they can be tolerated if (a) you are not interested in estimates of those parameters encountering boundary issues, and (b) removing those parameters does not affect conclusions about parameters of interest. For example, in the output below we explore the implications of simply removing the correlation between error terms at the pot level (i.e., assume \\(\\rho_{\\tilde{u}\\tilde{v}}=0\\) rather than accepting the (constrained) maximum likelihood estimate of \\(\\hat{\\rho}_{\\tilde{u}\\tilde{v}}=-1\\) that we saw in Model C). Formula: hgt ~ time13 + strl + cult + rem + time13:strl + time13:cult + time13:rem + (time13 | plant) + (1 | pot) + (0 + time13 | pot) Random effects: Groups Name Variance Std.Dev. Corr plant (Intercept) 0.2941010 0.54231 time13 0.0012062 0.03473 0.22 pot (Intercept) 0.0591621 0.24323 pot.1 time13 0.0001375 0.01173 Residual 0.0821742 0.28666 Number of obs: 413, groups: plant, 107; pot, 32 Fixed effects: Estimate Std. Error t value (Intercept) 1.51209 0.12965 11.663 time13 0.10158 0.00836 12.150 strl -0.08743 0.15407 -0.567 cult 0.13235 0.18563 0.713 rem 0.10661 0.17931 0.595 time13:strl 0.05869 0.01038 5.653 time13:cult -0.03065 0.01234 -2.484 time13:rem -0.03810 0.01209 -3.150 Note that the estimated variance components are all very similar to Model C, and the estimated fixed effects and their associated t-statistics are also very similar to Model C. Therefore, in this case we could consider simply reporting the results of Model C despite the boundary constraint. However, when removing boundary constraints through reasonable model reparameterizations is possible, that is typically the preferred route. In this case, one option we might consider is simplifying Model C by setting \\(\\sigma_{\\tilde{v}}^{2}=\\sigma_{\\tilde{u}\\tilde{v}}=0\\). We can then write our new model (Model C.1) in level-by-level formulation: Level One (timepoint within plant): \\[\\begin{equation} Y_{ijk} = a_{ij}+b_{ij}\\textstyle{time}_{ijk}+\\epsilon_{ijk} \\end{equation}\\] Level Two (plant within pot): \\[\\begin{eqnarray*} a_{ij} &amp; = &amp; a_{i}+u_{ij} \\\\ b_{ij} &amp; = &amp; b_{i}+v_{ij} \\end{eqnarray*}\\] Level Three (pot): \\[\\begin{eqnarray*} a_{i} &amp; = &amp; \\alpha_{0}+\\alpha_{1}\\textstyle{strl}_{i}+\\alpha_{2}\\textstyle{cult}_{i}+\\alpha_{3}\\textstyle{rem}_{i}+\\tilde{u}_{i} \\\\ b_{i} &amp; = &amp; \\beta_{0}+\\beta_{1}\\textstyle{strl}_{i}+\\beta_{2}\\textstyle{cult}_{i}+\\beta_{3}\\textstyle{rem}_{i} \\end{eqnarray*}\\] Note that there is no longer an error term associated with the model for mean growth rate \\(b_{i}\\) at the pot level. The growth rate for pot \\(i\\) is assumed to be fixed, after accounting for soil type and sterilization; all pots with the same soil type and sterilization are assumed to have the same growth rate. As a result, our error assumption at Level Three is no longer bivariate normal, but rather univariate normal: \\(\\tilde{u}_{i}\\sim N(0,\\sigma_{\\tilde{u}}^{2})\\). By removing one of our two Level Three error terms (\\(\\tilde{v}_{i}\\)), we effectively removed two parameters – the variance for \\(\\tilde{v}_{i}\\) and the correlation between \\(\\tilde{u}_{i}\\) and \\(\\tilde{v}_{i}\\). Fixed effects remain similar, as can be seen in the output below: Formula: hgt ~ time13 + strl + cult + rem + time13:strl + time13:cult + time13:rem + (time13 | plant) + (1 | pot) Random effects: Groups Name Variance Std.Dev. Corr plant (Intercept) 0.294718 0.54288 time13 0.001327 0.03642 0.19 pot (Intercept) 0.057654 0.24011 Residual 0.082221 0.28674 Number of obs: 413, groups: plant, 107; pot, 32 Fixed effects: Estimate Std. Error t value (Intercept) 1.512224 0.129007 11.722 time13 0.101091 0.007453 13.565 strl -0.087518 0.153441 -0.570 cult 0.132876 0.184887 0.719 rem 0.106536 0.178601 0.597 time13:strl 0.059264 0.009492 6.244 time13:cult -0.030824 0.011353 -2.715 time13:rem -0.036244 0.011102 -3.265 We now have a more stable model, free of boundary constraints. In fact, we can attempt to determine whether or not removing the two variance component parameters for Model C.1 provides a significant reduction in performance. Based on a likelihood ratio test (see below), we do not have significant evidence (chi-square test statistic=2.089 on 2 df, p=0.3519) that \\(\\sigma_{\\tilde{v}}^{2}\\) or \\(\\sigma_{\\tilde{u}\\tilde{v}}\\) is non-zero, so it is advisable to use the simpler Model C.1. However, Section 10.6 describes why this test may be misleading and prescribes a potentially better approach. ... Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) modelcl0 13 581.64 633.95 -277.82 555.64 modelcl 15 583.55 643.90 -276.78 553.55 2.089 2 0.3519 ... 10.6 Parametric bootstrap testing When testing random effects at the boundary (such as \\(\\sigma_{\\tilde{v}}^{2} = 0\\)), using a chi-square distribution to conduct a likelihood ratio test like we did in Section 10.5 is not appropriate. In fact, this will produce a conservative test, with p-values that are too large and not rejected enough (Raudenbush and Bryk, Singer and Willett, Faraway). For example, we should suspect that the p-value (.3519) produced by the likelihood ratio test comparing Models C and C.1 in Section 10.5 is too large, that the real probability of getting a likelihood ratio test statistic of 2.089 or greater when Model C.1 is true is smaller than .3519. Researchers often use the parametric bootstrap to better approximate the distribution of the likelihood test statistic and produce more accurate p-values by simulating data under the null hypothesis. Here are the basic steps for running a parametric bootstrap procedure to compare Model C.1 with Model C (see associated diagram below): Fit Model C.1 (the null model) to obtain estimated fixed and random effects (This is the “parametric” part.) Use the estimated fixed and random effects from the null model to regenerate a new set of plant heights with the same sample size (\\(n=413\\)) and associated covariates for each observation as the original data (This is the “bootstrap” part.) Fit both Model C.1 (the reduced model) and Model C (the full model) to the new data Compute a likelihood ratio statistic comparing Models C.1 and C Repeat the previous 3 steps many times (e.g., 1000) Produce a histogram of likelihood ratio statistics to illustrate its behavior when the null hypothesis is true Calculate a p-value by finding the proportion of times the bootstrapped test statistic is greater than our observed test statistic The steps in conducting a parametric bootstrap test comparing Models C and C.1. Let’s see how new plant heights are generated under the parametric bootstrap. Consider, for instance, \\(i=1\\) and \\(j=1,2\\). That is, consider Plants #11 and #12. These plants are found in Pot #1, which was randomly assigned to contain sterilized soil from a restored prairie (STP): pot plant soil sterile species germin hgt13 hgt18 hgt23 hgt28 1 1 11 STP Y L Y 2.3 2.9 4.5 5.1 2 1 12 STP Y L Y 1.9 2.0 2.6 3.5 Level Three One way to see the data generation process under the null model (Model C.1) is to start with Level Three and work backwards to Level One. Recall that our Level Three models for \\(a_{i}\\) and \\(b_{i}\\), the true intercept and slope from Pot \\(i\\), in Model C.1 are: \\[\\begin{eqnarray*} a_{i} &amp; = &amp; \\alpha_{0}+\\alpha_{1}\\textstyle{strl}_{i}+\\alpha_{2}\\textstyle{cult}_{i}+\\alpha_{3}\\textstyle{rem}_{i}+\\tilde{u}_{i} \\\\ b_{i} &amp; = &amp; \\beta_{0}+\\beta_{1}\\textstyle{strl}_{i}+\\beta_{2}\\textstyle{cult}_{i}+\\beta_{3}\\textstyle{rem}_{i} \\end{eqnarray*}\\] All the \\(\\alpha\\) and \\(\\beta\\) terms will be fixed at their estimated values, so the one term that will change for each bootstrapped data set is \\(\\tilde{u}_{i}\\). As we obtain a numeric value for \\(\\tilde{u}_{i}\\) for each pot, we will fix the subscript. For example, if \\(\\tilde{u}_{i}\\) is set to -.192 for Pot #1, then we would denote this by \\(\\tilde{u}_{1}=-.192\\). Similarly, in the context of Model C.1, \\(a_{1}\\) represents the mean height at Day 13 across all plants in Pot #1, where \\(\\tilde{u}_{1}\\) quantifies how Pot #1’s Day 13 height relates to other pots with the same sterilization and soil type. According to Model C.1, each \\(\\tilde{u}_{i}\\) is sampled from a normal distribution with mean 0 and standard deviation .240 (note that the standard deviation \\(\\sigma^2_{u}\\) is also fixed at its estimated value from Model C.1, given in Section 10.5). For example, a random component to the intercept for Pot #1 (\\(\\tilde{u}_{1}\\)) would be sampled from the just-mentioned normal distribution; say, for instance, \\(\\tilde{u}_{1}=-.192\\). Then we can produce a model-based intercept and slope for Pot #1: \\[\\begin{eqnarray*} a_{1} &amp; = &amp; 1.512-.088(1)+.133(0)+.107(0)-.192 = 1.232 \\\\ b_{1} &amp; = &amp; .101+.059(1)-.031(0)-.036(0) = .160 \\end{eqnarray*}\\] Notice a couple of features of the above derivations. First, all of the coefficients from the above equations (\\(\\alpha_{0}=1.512\\), \\(\\alpha_{1}=-.088\\), etc.) come from the estimated fixed effects from Model C.1 reported in Section 10.5. Second, “restored prairie” is the reference level for soil type, so that indicators for “cultivated land” and “remnant prairie” are both 0. Third, the mean intercept (Day 13 height) for observations from sterilized restored prairie soil is 1.512 - 0.088 = 1.424 mm across all pots, while the mean daily growth is .160 mm. Pot #1 therefore has mean Day 13 height that is .192 mm below the mean for all pots with sterilized restored prairie soil, but every such pot is assumed to have the same growth rate of .160 mm/day because of our assumption that there is no pot-to-pot variability in growth rate (i.e., \\(\\tilde{v}_{i}=0\\)). Level Two We next proceed to Level Two, where our equations for Model C.1 are: \\[\\begin{eqnarray*} a_{ij} &amp; = &amp; a_{i}+u_{ij} \\\\ b_{ij} &amp; = &amp; b_{i}+v_{ij} \\end{eqnarray*}\\] We will initially focus on Plant #11 from Pot #1. Notice that the intercept (Day 13 height = \\(a_{11}\\)) for Plant #11 has two components: the mean Day 13 height for Pot #1 (\\(a_{1}\\)) which we specified at Level Three, and an error term (\\(u_{11}\\)) which indicates how the Day 13 height for Plant #11 differs from the overall average for all plants from Pot #1. The slope (daily growth rate = \\(b_{11}\\)) for Plant #11 similarly has two components. Since both \\(a_{1}\\) and \\(b_{1}\\) were determined at Level Three, at this point we need to find the two error terms for Plant #11: \\(u_{11}\\) and \\(v_{11}\\). According to our multilevel model, we can sample \\(u_{11}\\) and \\(v_{11}\\) from a bivariate normal distribution with means both equal to 0, standard deviation for the intercept of .543, standard deviation for the slope of .036, and correlation between the intercept and slope of .194. For instance, suppose we sample \\(u_{11}=.336\\) and \\(v_{11}=.029\\). Then we can produce a model-based intercept and slope for Plant #11: \\[\\begin{eqnarray*} a_{11} &amp; = &amp; 1.232+.336 = 1.568 \\\\ b_{11} &amp; = &amp; .160+.029 = .189 \\end{eqnarray*}\\] Although plants from Pot #1 have a mean Day 13 height of 1.232 mm, Plant #11’s mean Day 13 height is .336 mm above that. Similarly, although plants from Pot #1 have a mean growth rate of .160 mm/day (just like every other pot with sterilized restored prairie soil), Plant #11’s growth rate is .029 mm/day faster. Level One Finally we proceed to Level One, where the height of Plant #11 is modeled as a linear function of time (\\(1.568 + .189\\textstyle{time}_{11k}\\)) with a normally distributed residual \\(\\epsilon_{11k}\\) at each time point \\(k\\). Four residuals (one for each time point) are sampled independently from a normal distribution with mean 0 and standard deviation .287 – the standard deviation again coming from parameter estimates from fitting Model C.1 to the actual data as reported in Section 10.5. Suppose we obtain residuals of \\(\\epsilon_{111}=-.311\\), \\(\\epsilon_{112}=.119\\), \\(\\epsilon_{113}=.241\\), and \\(\\epsilon_{114}=-.066\\). In that case, our parametrically generated data for Plant #11 from Pot #1 would look like: \\[ \\begin{array}{rcccl} Y_{111} &amp; = &amp; 1.568+.189(0)-.311 &amp; = &amp; 1.257 \\\\ Y_{112} &amp; = &amp; 1.568+.189(5)+.119 &amp; = &amp; 2.632 \\\\ Y_{113} &amp; = &amp; 1.568+.189(10)+.241 &amp; = &amp; 3.699 \\\\ Y_{114} &amp; = &amp; 1.568+.189(15)-.066 &amp; = &amp; 4.337 \\\\ \\end{array} \\] We would next turn to Plant #12 from Pot #1 (\\(i=1\\) and \\(j=2\\)). Fixed effects would remain the same, as would coefficients for Pot #1, \\(a_{1} = 1.232\\) and \\(b_{1} = .160\\), at Level Three. We would, however, sample new residuals \\(u_{12}\\) and \\(v_{12}\\) at Level Two, producing a different intercept \\(a_{12}\\) and slope \\(b_{12}\\) than those observed for Plant #11. Four new independent residuals \\(\\epsilon_{12k}\\) would also be selected at Level One, from the same normal distribution as before with mean 0 and standard deviation .287. Once an entire set of simulated heights for every pot, plant, and time point have been generated based on Model C.1, two models are fit to this data: Model C.1 – the correct (null) model that was actually used to generate the responses Model C – the incorrect (full) model that contains two extra variance components – \\(\\sigma_{\\tilde{v}}^{2}\\) and \\(\\sigma_{\\tilde{u}\\tilde{v}}\\) – that were not actually used when generating the responses # Generate 1 set of bootstrapped data and run chi-square test set.seed(3333) d &lt;- drop(simulate(modelcl0)) m2 &lt;-refit(modelcl, newresp=d) m1 &lt;-refit(modelcl0, newresp=d) anova(m2,m1) ## Data: leaddata ## Models: ## m1: hgt ~ time13 + strl + cult + rem + time13:strl + time13:cult + ## m1: time13:rem + (time13 | plant) + (1 | pot) ## m2: hgt ~ time13 + strl + cult + rem + time13:strl + time13:cult + ## m2: time13:rem + (time13 | plant) + (time13 | pot) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## m1 13 588.48 640.78 -281.24 562.48 ## m2 15 591.07 651.42 -280.54 561.07 1.4054 2 0.4953 A likelihood ratio test statistic is calculated comparing Model C.1 to Model C. For example, after continuing as above to generate new \\(Y_{ijk}\\) values corresponding to all 413 leadplant height measurements, we fit the “bootstrapped” data using both models. Since the data was generated using Model C.1, we would expect the two extra terms in Model C (\\(\\sigma^2_{\\tilde{v}}\\) and \\(\\sigma_{\\tilde{u}\\tilde{v}}\\)) to contribute very little to the quality of the fit; Model C will have a slightly larger likelihood and loglikelihood since it contains every parameter from Model C.1 plus two more, but the difference in the likelihoods should be due to chance. In fact, that is what the output above shows. Model C does have a larger loglikelihood than Model C.1 (-280.54 vs. -281.24), but this small difference is not statistically significant based on a chi-square test with 2 degrees of freedom (p=.4953). However, we are really only interested in saving the likelihood ratio test statistic from this bootstrapped sample (\\(2*(-280.54 - (-281.24) = 1.40\\)). By generating (“bootstrapping”) many sets of responses based on estimated parameters from Model C.1 and calculating many likelihood ratio test statistics, we can observe how this test statistic behaves under the null hypothesis of \\(\\sigma_{\\tilde{v}}^{2} = \\sigma_{\\tilde{u}\\tilde{v}} = 0\\), rather than making the (dubious) assumption that its behavior is described by a chi-square distribution with 2 degrees of freedom. Figure 10.14 illustrates the null distribution of the likelihood ratio test statistic derived by the parametric bootstrap procedure as compared to a chi-square distribution. A p-value for comparing our full and reduced models can be approximated by finding the proportion of likelihood ratio test statistics generated under the null model which exceed our observed likelihood ratio test (2.089). The parametric bootstrap provides a more reliable p-value in this case (.088 from table below); a chi-square distribution puts too much mass in the tail and not enough near 0, leading to overestimation of the p-value. Based on this test, we would still choose our simpler Model C.1, but we nearly had enough evidence to favor the more complex model. Data: leaddata Parametric bootstrap with 1000 samples. Models: m0: hgt ~ time13 + strl + cult + rem + time13:strl + time13:cult + m0: time13:rem + (time13 | plant) + (1 | pot) mA: hgt ~ time13 + strl + cult + rem + time13:strl + time13:cult + mA: time13:rem + (time13 | plant) + (time13 | pot) Df AIC BIC logLik deviance Chisq Chi Df Pr_boot(&gt;Chisq) m0 13 581.64 633.95 -277.82 555.64 mA 15 583.55 643.90 -276.78 553.55 2.089 2 0.088 Figure 10.14: Null distribution of likelihood ratio test statistic derived using parametric bootstrap (histogram) compared to a chi-square distribution with 2 degrees of freedom (smooth curve). The horizontal line represents the observed likelihood ratio test statistic. Another way of testing whether or not we should stick with the reduced model or reject it in favor of the larger model is by generating parametric bootstrap samples, and then using those samples to produce 95% confidence intervals for both \\(\\sigma_{\\tilde{u}\\tilde{v}} = 0\\) and \\(\\sigma_{\\tilde{v}}^{2} = 0\\). From the output below, the confidence intervals for \\(\\sigma_{\\tilde{u}\\tilde{v}}\\) (Index 10) and \\(\\sigma_{\\tilde{v}}^{2}\\) (Index 11) both contain 0, showing again that we do not have significant evidence to reject the reduced model in favor of the larger one. # Use lmeresampler to get bootstrapped CIs for var components ## running a parametric bootstrap (30 seconds or so if B=100) set.seed(333) boo1 &lt;- lmeresampler::bootstrap(model = modelcl, fn = varcomp.mer, type = &quot;parametric&quot;, B = 100) ## bootstrap confidence intervals are easily found using &#39;boot.ci&#39; ## - there are actually 7 variance components ## - varcomp.mer lists 11, but 4 are actually 0 boot.ci(boo1, index = 10, type=&quot;perc&quot;) ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 100 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = boo1, type = &quot;perc&quot;, index = 10) ## ## Intervals : ## Level Percentile ## 95% (-0.0043, 0.0012 ) ## Calculations and Intervals on Original Scale ## Some percentile intervals may be unstable boot.ci(boo1, index = 11, type=&quot;perc&quot;) ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 100 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = boo1, type = &quot;perc&quot;, index = 11) ## ## Intervals : ## Level Percentile ## 95% ( 0.0000, 0.0004 ) ## Calculations and Intervals on Original Scale ## Some percentile intervals may be unstable ## you can also examine the bootstrap samples graphically # plot(boo1, index = 10) # plot(boo1, index = 11) In this section, we have offered the parametric bootstrap as a noticeable improvement over the likelihood ratio test with an approximate chi-square distribution for testing random effects near the boundary. And typically when we conduct hypothesis tests involving variance terms we are testing at the boundary, since we are asking if the variance term is really necessary (i.e., \\(H_O: \\sigma^2=0\\) vs. \\(H_A: \\sigma^2 &gt; 0\\)). However, what if we are conducting a hypothesis test about a fixed effect? For the typical test of whether or not a fixed effect is significant – e.g., \\(H_O: \\alpha_i=0\\) vs. \\(H_A: \\alpha_i \\neq 0\\) – we are not testing at the boundary, since most fixed effects have no bounds on allowable values. We have often used a likelihood ratio test with an approximate chi-square distribution in these settings – does that provide accurate p-values? Although some research (e.g., Faraway, 2006) shows that p-values of fixed effects from likelihood ratio tests can tend to be anti-conservative (too high), in general the approximation is not bad. We will continue to use the likelihood ratio test with a chi-square distribution for fixed effects, but you could always check your p-values using a parametric bootstrap approach. 10.7 Exploding variance components Our modeling task in Section 10.5 was simplified by the absence of covariates at Level Two. As multilevel models grow to include three or more levels, the addition of just a few covariates at lower levels can lead to a huge increase in the number of parameters (fixed effects and variance components) that must be estimated throughout the model. In this section, we will examine when and why the number of model parameters might explode, and we will consider strategies for dealing with these potentially complex models. For instance, consider Model C, where we must estimate a total of 15 parameters: 8 fixed effects plus 7 variance components (1 at Level One, 3 at Level Two, and 3 at Level Three). By adding just a single covariate to the equations for \\(a_{ij}\\) and \\(b_{ij}\\) at Level Two in Model C (say, for instance, the size of each seed), we would now have a total of 30 parameters to estimate! The new multilevel model (Model Cplus) could be written as follows: Level One (timepoint within plant): \\[\\begin{equation} Y_{ijk} = a_{ij}+b_{ij}\\textstyle{time}_{ijk}+\\epsilon_{ijk} \\tag{10.7} \\end{equation}\\] Level Two (plant within pot): \\[\\begin{eqnarray*} a_{ij} &amp; = &amp; a_{i}+c_{i}\\textstyle{seedsize}_{ij}+u_{ij} \\\\ b_{ij} &amp; = &amp; b_{i}+d_{i}\\textstyle{seedsize}_{ij}+v_{ij} \\end{eqnarray*}\\] Level Three (pot): \\[\\begin{eqnarray*} a_{i} &amp; = &amp; \\alpha_{0}+\\alpha_{1}\\textstyle{strl}_{i}+\\alpha_{2}\\textstyle{cult}_{i}+\\alpha_{3}\\textstyle{rem}_{i}+ \\tilde{u}_{i}\\\\ b_{i} &amp; = &amp; \\beta_{0}+\\beta_{1}\\textstyle{strl}_{i}+\\beta_{2}\\textstyle{cult}_{i}+\\beta_{3}\\textstyle{rem}_{i}+ \\tilde{v}_{i} \\\\ c_{i} &amp; = &amp; \\gamma_{0}+\\gamma_{1}\\textstyle{strl}_{i}+\\gamma_{2}\\textstyle{cult}_{i}+\\gamma_{3}\\textstyle{rem}_{i}+ \\tilde{w}_{i} \\\\ d_{i} &amp; = &amp; \\delta_{0}+\\delta_{1}\\textstyle{strl}_{i}+\\delta_{2}\\textstyle{cult}_{i}+\\delta_{3}\\textstyle{rem}_{i}+ \\tilde{z}_{i} \\end{eqnarray*}\\] or as a composite model: \\[\\begin{eqnarray*} Y_{ijk} &amp; = &amp; [\\alpha_{0}+\\alpha_{1}\\textstyle{strl}_{i}+\\alpha_{2}\\textstyle{cult}_{i}+\\alpha_{3}\\textstyle{rem}_{i} + \\gamma_{0}\\textstyle{seedsize}_{ij} + \\\\ &amp; &amp; \\beta_{0}\\textstyle{time}_{ijk} + \\beta_{1}\\textstyle{strl}_{i}\\textstyle{time}_{ijk}+\\beta_{2}\\textstyle{cult}_{i}\\textstyle{time}_{ijk}+ \\beta_{3}\\textstyle{rem}_{i}\\textstyle{time}_{ijk} + \\\\ &amp; &amp; \\gamma_{1}\\textstyle{strl}_{i}\\textstyle{seedsize}_{ij}+\\gamma_{2}\\textstyle{cult}_{i}\\textstyle{seedsize}_{ij}+ \\gamma_{3}\\textstyle{rem}_{i}\\textstyle{seedsize}_{ij} + \\\\ &amp; &amp; \\delta_{0}\\textstyle{seedsize}_{ij}\\textstyle{time}_{ijk} + \\delta_{1}\\textstyle{strl}_{i}\\textstyle{seedsize}_{ij}\\textstyle{time}_{ijk} + \\delta_{2}\\textstyle{cult}_{i}\\textstyle{seedsize}_{ij}\\textstyle{time}_{ijk} + \\\\ &amp; &amp; \\delta_{3}\\textstyle{rem}_{i}\\textstyle{seedsize}_{ij}\\textstyle{time}_{ijk}] + \\\\ &amp; &amp; [\\tilde{u}_{i} + u_{ij} + \\epsilon_{ijk} + \\tilde{w}_{i}\\textstyle{seedsize}_{ij} + \\tilde{v}_{i}\\textstyle{time}_{ijk} + v_{ij}\\textstyle{time}_{ijk} + \\tilde{z}_{i}\\textstyle{seedsize}_{ij}\\textstyle{time}_{ijk} ] \\end{eqnarray*}\\] where \\(\\epsilon_{ijk}\\sim N(0,\\sigma^2)\\), \\[ \\left[ \\begin{array}{c} u_{ij} \\\\ v_{ij} \\end{array} \\right] \\sim N \\left( \\left[ \\begin{array}{c} 0 \\\\ 0 \\end{array} \\right], \\left[ \\begin{array}{cc} \\sigma_{u}^{2} &amp; \\\\ \\sigma_{uv} &amp; \\sigma_{v}^{2} \\end{array} \\right] \\right), \\] and \\[ \\left[ \\begin{array}{c} \\tilde{u}_{i} \\\\ \\tilde{v}_{i} \\\\ \\tilde{w}_{i} \\\\ \\tilde{z}_{i} \\end{array} \\right] \\sim N \\left( \\left[ \\begin{array}{c} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{array} \\right], \\left[ \\begin{array}{cccc} \\sigma_{\\tilde{u}}^{2} &amp; &amp; &amp; \\\\ \\sigma_{\\tilde{u}\\tilde{v}} &amp; \\sigma_{\\tilde{v}}^{2} &amp; &amp; \\\\ \\sigma_{\\tilde{u}\\tilde{w}} &amp; \\sigma_{\\tilde{v}\\tilde{w}} &amp; \\sigma_{\\tilde{w}}^{2} &amp; \\\\ \\sigma_{\\tilde{u}\\tilde{z}} &amp; \\sigma_{\\tilde{v}\\tilde{z}} &amp; \\sigma_{\\tilde{w}\\tilde{z}} &amp; \\sigma_{\\tilde{z}}^{2} \\end{array} \\right] \\right). \\] We would have 16 fixed effects from the four equations at Level Three, each with 4 fixed effects to estimate if we use the same 3 indicators for soil type and sterilization for each \\(a\\), \\(b\\), \\(c\\), and \\(d\\). And, with four equations at Level Three, the error covariance matrix at the pot level would be 4x4 with 10 variance components to estimate; each error term (4) has a variance associated with it, and each pair of error terms (6) has an associated correlation. The error structure at Levels One (1 variance term) and Two (2 variance terms and 1 correlation) would remain the same, for a total of 14 variance components. Now consider adding an extra Level One covariate to the model in the previous paragraph. How many model parameters would now need to be estimated? (Try writing out the multilevel models and counting parameters…) The correct answer is 52 total parameters! There are 24 fixed effects (from 6 Level Three equations) and 28 variance components (1 at Level One, 6 at Level Two, and 21 at Level Three). Estimating even 30 parameters as in Model Cplus from a single set of data is an ambitious task and computationally very challenging. Essentially we (or the statistics package we are using) must determine which combination of values for the 30 parameters would maximize the likelihood associated with our model and the observed data. Even in the absolute simplest case, with only two options for each parameter, there would be over one billion possible combinations to consider. But if our primary interest is in fixed effects, we really only have 5 covariates (and their associated interactions) in our model. What can be done to make fitting a 3-level model with 1 covariate at Level One, 1 at Level Two, and 3 at Level Three more manageable? Reasonable options include: Reduce the number of variance components by assuming all error terms to be independent; that is, set all correlation terms to 0. Reduce the number of variance components by removing error terms from certain Level Two and Three equations. Often, researchers will begin with a random intercepts model, in which only the first equation at Level Two and Three has an error term. With the leadplant data, we would account for variability in Day 13 height (intercept) among plants and pots, but assume that the effects of time and seed size are constant among pots and plants. Reduce the number of fixed effects by removing interaction terms that are not expected to be meaningful. Interaction terms between covariates at different levels can be eliminated simply by reducing the number of terms in certain equations at Levels Two and Three. There is no requirement that all equations at a certain level contain the same set of predictors. Often, researchers will not include covariates in equations beyond the intercept at a certain level unless there’s a compelling reason. By following the options above, our potential 30-parameter model can be simplified to this 9-parameter model: Level One: \\[\\begin{equation} Y_{ijk} = a_{ij}+b_{ij}\\textstyle{time}_{ijk}+\\epsilon_{ijk} \\end{equation}\\] Level Two: \\[\\begin{eqnarray*} a_{ij} &amp; = &amp; a_{i}+c_{i}\\textstyle{seedsize}_{ij}+u_{ij} \\\\ b_{ij} &amp; = &amp; b_{i} \\end{eqnarray*}\\] Level Three: \\[\\begin{eqnarray*} a_{i} &amp; = &amp; \\alpha_{0} + \\alpha_{1}\\textstyle{strl}_{i} + \\alpha_{2}\\textstyle{cult}_{i} + \\alpha_{3}\\textstyle{rem}_{i} + \\tilde{u}_{i} \\\\ b_{i} &amp; = &amp; \\beta_{0} \\\\ c_{i} &amp; = &amp; \\gamma_{0} \\end{eqnarray*}\\] where \\(\\epsilon_{ijk}\\sim N(0,\\sigma^2)\\), \\(u_{ij}\\sim N(0,\\sigma_{u}^{2})\\), and \\(\\tilde{u}_{i}\\sim N(0,\\sigma_{\\tilde{u}}^{2})\\). Or, in terms of a composite model: \\[\\begin{eqnarray*} Y_{ijk} &amp; = &amp; [\\alpha_{0}+\\alpha_{1}\\textstyle{strl}_{i}+\\alpha_{2}\\textstyle{cult}_{i}+\\alpha_{3}\\textstyle{rem}_{i} + \\gamma_{0}\\textstyle{seedsize}_{ij} + \\beta_{0}\\textstyle{time}_{ijk}] + \\\\ &amp; &amp; [\\tilde{u}_{i}+u_{ij}+\\epsilon_{ijk}] \\end{eqnarray*}\\] According to the second option, we have built a random intercepts model with error terms only at the first (intercept) equation at each level. Not only does this eliminate variance terms associated with the missing error terms, but it also eliminates correlation terms between errors (as suggested by Option 1) since there are no pairs of error terms that can be formed at any level. In addition, as suggested by Option 3, we have eliminated predictors (and their fixed effects coefficients) at every equation other than the intercept at each level. The simplified 9-parameter model essentially includes a random effect for pot (\\(\\sigma_{\\tilde{u}}^{2}\\)) after controlling for sterilization and soil type, a random effect for plant within pot (\\(\\sigma_{0}^{2}\\)) after controlling for seed size, and a random effect for error about the time trend for individual plants (\\(\\sigma^{2}\\)). We must assume that the effect of time is the same for all plants and all pots, and it does not depend on seed size, sterilization, or soil type. Similarly, we must assume that the effect of seed size is the same for each pot and does not depend on sterilization or soil type. While somewhat proscriptive, a random intercepts model such as this can be a sensible starting point, since the simple act of accounting for variability of observational units at Level Two or Three can produce better estimates of fixed effects of interest and their standard errors. 10.8 Building to a final model In Model C we considered the main effects of soil type and sterilization on leadplant initial height and growth rate, but we did not consider interactions—even though biology researchers expect that sterilization will aid growth in certain soil types more than others. Thus, in Model D we will build Level Three interaction terms into Model C.1: Level One: \\[\\begin{equation} Y_{ijk} = a_{ij}+b_{ij}\\textstyle{time}_{ijk}+\\epsilon_{ijk} \\end{equation}\\] Level Two: \\[\\begin{eqnarray*} a_{ij} &amp; = &amp; a_{i}+u_{ij} \\\\ b_{ij} &amp; = &amp; b_{i}+v_{ij} \\end{eqnarray*}\\] Level Three: \\[\\begin{eqnarray*} a_{i} &amp; = &amp; \\alpha_{0} + \\alpha_{1}\\textstyle{strl}_{i} + \\alpha_{2}\\textstyle{cult}_{i} + \\alpha_{3}\\textstyle{rem}_{i} + \\alpha_{4}\\textstyle{strl}_{i}\\textstyle{rem}_{i} + \\alpha_{5}\\textstyle{strl}_{i}\\textstyle{cult}_{i} + \\tilde{u}_{i} \\\\ b_{i} &amp; = &amp; \\beta_{0}+\\beta_{1}\\textstyle{strl}_{i}+\\beta_{2}\\textstyle{cult}_{i}+\\beta_{3}\\textstyle{rem}_{i} + \\beta_{4}\\textstyle{strl}_{i}\\textstyle{rem}_{i} + \\beta_{5}\\textstyle{strl}_{i}\\textstyle{cult}_{i} \\end{eqnarray*}\\] where error terms are defined as in Model C.1. From the output below, we see that the interaction terms were not especially helpful, except possibly for a differential effect of sterilization in remnant and reconstructed prairies on the growth rate of leadplants. But it’s clear that Model D can be simplified through the removal of certain fixed effects with low t-ratios. Formula: hgt ~ time13 + strl + cult + rem + time13:strl + time13:cult + time13:rem + strl:cult + strl:rem + time13:strl:cult + time13:strl:rem + (time13 | plant) + (1 | pot) Random effects: Groups Name Variance Std.Dev. Corr plant (Intercept) 0.292722 0.54104 time13 0.001276 0.03573 0.20 pot (Intercept) 0.070544 0.26560 Residual 0.082139 0.28660 Number of obs: 413, groups: plant, 107; pot, 32 Fixed effects: Estimate Std. Error t value (Intercept) 1.531250 0.153897 9.950 time13 0.094919 0.008119 11.692 strl -0.126511 0.222661 -0.568 cult -0.061835 0.329916 -0.187 rem 0.124258 0.239585 0.519 time13:strl 0.072790 0.012005 6.063 time13:cult -0.022624 0.020836 -1.086 time13:rem -0.020474 0.013165 -1.555 strl:cult 0.276162 0.407097 0.678 strl:rem -0.071397 0.378048 -0.189 time13:strl:cult -0.016082 0.024812 -0.648 time13:strl:rem -0.051990 0.023951 -2.171 To form Model F, we begin by removing all covariates describing the intercept (Day 13 height), since neither sterilization nor soil type nor their interaction appear to be significantly related to initial height. However, sterilization, remnant prairie soil, and their interaction appear to have significant influences on growth rate, although the effect of cultivated soil on growth rate did not appear significantly different from that of restored prairie soil (the reference level). A likelihood ratio test shows no significant difference between Models D and F (chi-square test statistic = 11.15 on 7 df, p=.1323), supporting the use of simplified Model F. Formula: hgt ~ time13 + time13:strl + time13:rem + time13:strl:rem + (time13 | -2.73299 -0.51784 0.04508 0.50400 2.91566 Random effects: Groups Name Variance Std.Dev. Corr plant (Intercept) 0.294044 0.54226 time13 0.001425 0.03775 0.16 pot (Intercept) 0.048710 0.22070 Residual 0.082045 0.28643 Number of obs: 413, groups: plant, 107; pot, 32 Fixed effects: Estimate Std. Error t value (Intercept) 1.529092 0.071336 21.435 time13 0.091371 0.007745 11.798 time13:strl 0.059670 0.010375 5.751 time13:rem -0.016489 0.013238 -1.246 Data: leaddata Models: modelfl0: hgt ~ time13 + time13:strl + time13:rem + time13:strl:rem + (time13 | modelfl0: plant) + (1 | pot) modeldl0: hgt ~ time13 + strl + cult + rem + time13:strl + time13:cult + modeldl0: time13:rem + strl:cult + strl:rem + time13:strl:cult + time13:strl:rem + modeldl0: (time13 | plant) + (1 | pot) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) modelfl0 10 581.20 621.43 -280.60 561.20 modeldl0 17 584.05 652.45 -275.02 550.05 11.148 7 0.1323 We mentioned in Section 10.6 that we could compare Model F and Model D, which differ only in their fixed effects terms, using the parametric bootstrap approach. In fact, in Section 10.6 we suggested that the p-value using the chi-square approximation (.1323) may be a slight under-estimate of the true p-value, but probably in the ballpark. In fact, when we generated 1000 bootstrapped samples of plant heights under Model F, and produced 1000 simulated likelihood ratios comparing Models D and F, we produced a p-value of .201. In Figure 10.15, we see that the chi-square distribution has too much area in the peak and too little area in the tails, although in general it approximates the parametric bootstrap distribution of the likelihood ratio pretty nicely. Figure 10.15: Null distribution of likelihood ratio test statistic derived using parametric bootstrap (histogram) compared to a chi-square distribution with 7 degrees of freedom (smooth curve). The horizontal line represents the observed likelihood ratio test statistic. The effects of remnant prairie soil and the interaction between remnant soil and sterilization appear to have marginal benefit in Model F, so we remove those two terms to create Model E. A likelihood ratio test comparing Models E and F, however, shows that Model F significantly outperforms Model E (chi-square test statistic = 9.40 on 2 df, p=.0090). Thus, we will use Model F as our “Final Model” for generating inference. Formula: hgt ~ time13 + time13:strl + (time13 | plant) + (1 | pot) Random effects: Groups Name Variance Std.Dev. Corr plant (Intercept) 0.29441 0.54260 time13 0.00159 0.03987 0.14 pot (Intercept) 0.04712 0.21707 Residual 0.08199 0.28634 Number of obs: 413, groups: plant, 107; pot, 32 Fixed effects: Estimate Std. Error t value (Intercept) 1.528770 0.070965 21.543 time13 0.085682 0.006524 13.134 time13:strl 0.058645 0.009364 6.263 refitting model(s) with ML (instead of REML) ... Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) modelel0 8 586.6 618.78 -285.3 570.6 modelfl0 10 581.2 621.43 -280.6 561.2 9.401 2 0.009091 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ... Our final model (Model F), with its constraints on Level Three error terms, can be expressed level-by-level as: Level One: \\[\\begin{equation} Y_{ijk} = a_{ij}+b_{ij}\\textstyle{time}_{ijk}+\\epsilon_{ijk} \\end{equation}\\] Level Two: \\[\\begin{eqnarray*} a_{ij} &amp; = &amp; a_{i}+u_{ij} \\\\ b_{ij} &amp; = &amp; b_{i}+v_{ij} \\end{eqnarray*}\\] Level Three: \\[\\begin{eqnarray*} a_{i} &amp; = &amp; \\alpha_{0} + \\tilde{u}_{i} \\\\ b_{i} &amp; = &amp; \\beta_{0}+\\beta_{1}\\textstyle{strl}_{i}+\\beta_{2}\\textstyle{rem}_{i} + \\beta_{3}\\textstyle{strl}_{i}\\textstyle{rem}_{i} \\end{eqnarray*}\\] where \\(\\epsilon_{ijk}\\sim N(0,\\sigma^2)\\), \\[ \\left[ \\begin{array}{c} u_{ij} \\\\ v_{ij} \\end{array} \\right] \\sim N \\left( \\left[ \\begin{array}{c} 0 \\\\ 0 \\end{array} \\right], \\left[ \\begin{array}{cc} \\sigma_{u}^{2} &amp; \\\\ \\sigma_{uv} &amp; \\sigma_{v}^{2} \\end{array} \\right] \\right), \\] and \\(\\tilde{{u}_{i}}\\sim N(0,\\sigma_{\\tilde{u}}^{2})\\). In composite form, we have: \\[\\begin{eqnarray*} Y_{ijk} &amp; = &amp; [\\alpha_{0}+ \\beta_{0}\\textstyle{time}_{ijk} + \\beta_{1}\\textstyle{strl}_{i}\\textstyle{time}_{ijk} + \\beta_{2}\\textstyle{rem}_{i}\\textstyle{time}_{ijk} + \\beta_{3}\\textstyle{strl}_{i}\\textstyle{rem}_{i}\\textstyle{time}_{ijk}] + \\\\ &amp; &amp; [\\tilde{u}_{i}+u_{ij}+\\epsilon_{ijk}+v_{ij}\\textstyle{time}_{ijk}] \\end{eqnarray*}\\] Estimates of model parameters can be interpreted in the following manner: \\(\\hat{\\sigma}=.287=\\) the standard deviation in within-plant residuals after accounting for time. \\(\\hat{\\sigma}_{u}=.543=\\) the standard deviation in Day 13 heights between plants from the same pot. \\(\\hat{\\sigma}_{v}=.037=\\) the standard deviation in rates of change in height between plants from the same pot. \\(\\hat{\\rho}_{uv}=.157=\\) the correlation in plants’ Day 13 height and their rate of change in height. \\(\\hat{\\sigma}_{\\tilde{u}}=.206=\\) the standard deviation in Day 13 heights between pots. \\(\\hat{\\alpha}_{0}=1.529=\\) the mean height for leadplants 13 days after planting. \\(\\hat{\\beta}_{0}=0.091=\\) the mean daily change in height from 13 to 28 days after planting for leadplants from reconstructed prairies or cultivated lands (rem=0) with no sterilization (strl=0) . \\(\\hat{\\beta}_{1}=0.060=\\) the increase in mean daily change in height for leadplants from using sterilized soil instead of unsterilized soil in reconstructed prairies or cultivated lands. Thus, leadplants grown in sterilized soil from reconstructed prairies or cultivated lands have an estimated daily increase in height of 0.151 mm. \\(\\hat{\\beta}_{2}=-0.017=\\) the decrease in mean daily change in height for leadplants from using unsterilized soil from remnant prairies, rather than unsterilized soil from reconstructed prairies or cultivated lands. Thus, leadplants grown in unsterilized soil from remnant prairies have an estimated daily increase in height of 0.074 mm. \\(\\hat{\\beta}_{3}=-0.039=\\) the decrease in mean daily change in height for leadplants from sterilized soil from remnant prairies, compared to the expected daily change based on \\(\\hat{\\beta}_{1}\\) and \\(\\hat{\\beta}_{2}\\). Three-way interactions show that the size of an interaction between two predictors differs depending on the level of a third predictor. Whew! In this case, we might focus on how the interaction between remnant prairies and time differs for unsterilized and sterilized soil. Specifically, the negative effect of remnant prairies on growth rate (compared to reconstructed prairies or cultivated lands) is larger in sterilized soil than unsterilized; in sterilized soil, plants from remnant prairie soil grow .056 mm/day slower on average than plants from other soil types (.095 vs. .151 mm/day), while in unsterilized soil, plants from remnant prairie soil grow just .017 mm/day slower than plants from other soil types (.074 vs. .091 mm/day). Note that the difference between .056 and .017 is our three-way interaction coefficient. Through this three-way interaction term, we also see that leadplants grown in sterilized soil from remnant prairies have an estimated daily increase in height of 0.095 mm. Based on t-values produced by Model F, sterilization has the most significant effect on leadplant growth, while there is some evidence that growth rate is somewhat slower in remnant prairies, and that the effect of sterilization is also somewhat muted in remnant prairies. Sterilization leads to an estimated 66% increase in growth rate of leadplants from Days 13 to 28 in soil from reconstructed prairies and cultivated lands, and an estimated 28% increase in soil from remnant prairies. In unsterilized soil, plants from remnant prairies grow an estimated 19% slower than plants from other soil types. 10.9 Covariance structure (Optional) As in Chapter 9, it is important to be aware of the covariance structure implied by our chosen models (focusing initially on Model B). Our three-level model, through error terms at each level, defines a specific covariance structure at both the plant level (Level Two) and the pot level (Level Three). For example, our standard model implies a certain level of correlation among measurements from the same plant and among plants from the same pot. Although three-level models are noticeably more complex than two-level models, it is still possible to systematically determine the implication of our standard model; by doing this, we can evaluate whether our fitted model agrees with our exploratory analyses, and we can also decide if it’s worth considering alternative covariance structures. We will first consider Model B with \\(\\tilde{v}_{i}\\) at Level Three, and then we will evaluate the resulting covariance structure that results from removing \\(\\tilde{v}_{i}\\), thereby restricting \\(\\sigma_{\\tilde{v}}^{2}=\\sigma_{\\tilde{u}\\tilde{v}}=0\\). The composite version of Model B has been previously expressed as: \\[\\begin{equation} Y_{ijk}=[\\alpha_{0}+\\beta_{0}\\textstyle{time}_{ijk}]+ [\\tilde{u}_{i}+u_{ij}+\\epsilon_{ijk}+(\\tilde{v}_{i}+v_{ij})\\textstyle{time}_{ijk}] \\tag{10.8} \\end{equation}\\] where \\(\\epsilon_{ijk}\\sim N(0,\\sigma^2)\\), \\[ \\left[ \\begin{array}{c} u_{ij} \\\\ v_{ij} \\end{array} \\right] \\sim N \\left( \\left[ \\begin{array}{c} 0 \\\\ 0 \\end{array} \\right], \\left[ \\begin{array}{cc} \\sigma_{u}^{2} &amp; \\\\ \\sigma_{uv} &amp; \\sigma_{v}^{2} \\end{array} \\right] \\right), \\] and \\[ \\left[ \\begin{array}{c} \\tilde{u}_{i} \\\\ \\tilde{v}_{i} \\end{array} \\right] \\sim N \\left( \\left[ \\begin{array}{c} 0 \\\\ 0 \\end{array} \\right], \\left[ \\begin{array}{cc} \\sigma_{\\tilde{u}}^{2} &amp; \\\\ \\sigma_{\\tilde{u}\\tilde{v}} &amp; \\sigma_{\\tilde{v}}^{2} \\end{array} \\right] \\right). \\] In order to assess the implied covariance structure from our standard model, we must first derive variance and covariance terms for related observations (i.e., same timepoint and same plant, different timepoints but same plant, different plants but same pot). Each derivation will rely on the random effects portion of the composite model, since there is no variability associated with fixed effects. For ease of notation, we will let \\(t_{k}=\\textstyle{time}_{ijk}\\), since all plants were planned to be observed on the same 4 days. The variance for an individual observation can be expressed as: \\[\\begin{equation} Var(Y_{ijk}) = (\\sigma^{2} + \\sigma_{u}^{2} + \\sigma_{\\tilde{u}}^{2}) + 2(\\sigma_{uv} + \\sigma_{\\tilde{u}\\tilde{v}})t_k + (\\sigma_{v}^{2} + \\sigma_{\\tilde{v}}^{2})t_{k}^{2}, \\tag{10.9} \\end{equation}\\] and the covariance between observations taken at different timepoints (\\(k\\) and \\(k^{&#39;}\\)) from the same plant (\\(j\\)) is: \\[\\begin{equation} Cov(Y_{ijk},Y_{ijk^{&#39;}}) = (\\sigma_{u}^{2} + \\sigma_{\\tilde{u}}^{2}) + (\\sigma_{uv} + \\sigma_{\\tilde{u}\\tilde{v}})(t_{k}+t_{k^{&#39;}}) + (\\sigma_{v}^{2} + \\sigma_{\\tilde{v}}^{2})t_{k}t_{k^{&#39;}}, \\tag{10.10} \\end{equation}\\] and the covariance between observations taken at potentially different times (\\(k\\) and \\(k&#39;\\)) from different plants (\\(j\\) and \\(j^{&#39;}\\)) from the same pot (\\(i\\)) is: \\[\\begin{equation} Cov(Y_{ijk},Y_{ij^{&#39;}k^{&#39;}}) = \\sigma_{\\tilde{u}}^{2} + \\sigma_{\\tilde{u}\\tilde{v}}(t_{k}+t_{k^{&#39;}}) + \\sigma_{\\tilde{v}}^{2}t_{k}t_{k^{&#39;}}. \\tag{10.11} \\end{equation}\\] Based on these variances and covariances, the covariance matrix for observations over time from the same plant (\\(j\\)) from pot \\(i\\) can be expressed as the following 4x4 matrix: \\[ Cov(\\textbf{Y}_{ij}) = \\left[ \\begin{array}{cccc} \\tau_{1}^{2} &amp; &amp; &amp; \\\\ \\tau_{12} &amp; \\tau_{2}^{2} &amp; &amp; \\\\ \\tau_{13} &amp; \\tau_{23} &amp; \\tau_{3}^{2} &amp; \\\\ \\tau_{14} &amp; \\tau_{24} &amp; \\tau_{34} &amp; \\tau_{4}^{2} \\end{array} \\right], \\] where \\(\\tau_{k}^{2}=Var(Y_{ijk})\\) and \\(\\tau_{kk&#39;}=Cov(Y_{ijk},Y_{ijk&#39;})\\). Note that \\(\\tau_{k}^{2}\\) and \\(\\tau_{kk&#39;}\\) are both independent of \\(i\\) and \\(j\\) so that \\(Cov(\\textbf{Y}_{ij})\\) will be constant for all plants from all pots. That is, every plant from every pot will have the same set of variances over the four timepoints and the same correlations between heights at different timepoints. But, the variances and correlations can change depending on the timepoint under consideration as suggested by the presence of \\(t_k\\) terms in Equations (10.9) through (10.11). Similarly, the covariance matrix between observations from plants \\(j\\) and \\(j&#39;\\) from pot \\(i\\) can be expressed as this 4x4 matrix: \\[ Cov(\\textbf{Y}_{ij},\\textbf{Y}_{ij&#39;}) = \\left[ \\begin{array}{cccc} \\tilde{\\tau}_{11} &amp; &amp; &amp; \\\\ \\tilde{\\tau}_{12} &amp; \\tilde{\\tau}_{22} &amp; &amp; \\\\ \\tilde{\\tau}_{13} &amp; \\tilde{\\tau}{23} &amp; \\tilde{\\tau}_{33} &amp; \\\\ \\tilde{\\tau}_{14} &amp; \\tilde{\\tau}_{24} &amp; \\tilde{\\tau}_{34} &amp; \\tilde{\\tau}_{44} \\end{array} \\right], \\] where \\(\\tilde{\\tau}_{kk}=Cov(Y_{ijk},Y_{ij&#39;k})=\\sigma_{\\tilde{u}}^{2}+2\\sigma_{\\tilde{u}\\tilde{v}}t_{k}+\\sigma_{\\tilde{v}}^{2}t_{k}^{2}\\) and \\(\\tilde{\\tau}_{kk&#39;}=Cov(Y_{ijk},Y_{ij&#39;k&#39;})\\) as derived above. As we saw with \\(Cov(\\textbf{Y}_{ij})\\), \\(\\tilde{\\tau}_{kk}\\) and \\(\\tilde{\\tau}_{kk&#39;}\\) are both independent of \\(i\\) and \\(j\\) so that \\(Cov(\\textbf{Y}_{ij},\\textbf{Y}_{ij&#39;})\\) will be constant for all pairs of plants from all pots. That is, any pair of plants from the same pot will have the same correlations between heights at any two timepoints. As with any covariance matrix, we can convert \\(Cov(\\textbf{Y}_{ij},\\textbf{Y}_{ij&#39;})\\) into a correlation matrix if desired. Now that we have the general covariance structure implied by the standard multilevel model in place, we can examine the specific structure suggested by the estimates of variance components in Model B. Restricted maximum likelihood (REML) in Section 10.4 produced the following estimates for variance components: \\(\\hat{\\sigma}^2=.0822\\), \\(\\hat{\\sigma}_{u}^{2}=.299\\), \\(\\hat{\\sigma}_{v}^{2}=.00119\\), \\(\\hat{\\sigma}_{uv}=\\hat{\\rho}_{uv}\\sqrt{\\hat{\\sigma}_{u}^{2}\\hat{\\sigma}_{v}^{2}}=.00528\\), \\(\\hat{\\sigma}_{\\tilde{u}}^{2}=.0442\\), \\(\\hat{\\sigma}_{\\tilde{v}}^{2}=.00126\\), \\(\\hat{\\sigma}_{\\tilde{u}\\tilde{v}}=\\hat{\\rho}_{\\tilde{u}\\tilde{v}}\\sqrt{\\hat{\\sigma}_{\\tilde{u}}^{2}\\hat{\\sigma}_{\\tilde{v}}^{2}}=-.00455\\). Based on these estimates and the derivations above, the within-plant correlation structure over time is estimated to be: \\[ Corr(\\textbf{Y}_{ij}) = \\left[ \\begin{array}{cccc} 1 &amp; &amp; &amp; \\\\ .76 &amp; 1 &amp; &amp; \\\\ .65 &amp; .82 &amp; 1 &amp; \\\\ .54 &amp; .77 &amp; .88 &amp; 1 \\end{array} \\right] \\] for all plants \\(j\\) and all pots \\(i\\), and the correlation structure between different plants from the same pot is estimated to be: \\[ Corr(\\textbf{Y}_{ij},\\textbf{Y}_{ij&#39;}) = \\left[ \\begin{array}{cccc} .104 &amp; &amp; &amp; \\\\ .047 &amp; .061 &amp; &amp; \\\\ -.002 &amp; .067 &amp; .116 &amp; \\\\ -.037 &amp; .068 &amp; .144 &amp; .191 \\end{array} \\right]. \\] The within-plant correlation structure suggests that measurements taken closer in time tend to be more highly correlated than those with more separation in time, and later measurements tend to be more highly correlated than earlier measurements. Examination of standard deviation terms by timepoint suggests that variability increases over time within a plant (estimated SDs of .652, .703, .828, and .999 for days 13, 18, 23, and 28, respectively). The correlation structure between plants from the same pot depicts a fairly low level of correlation; even for measurements taken at the same timepoint, the largest correlation between plants from the same pot occurs on Day 28 (r=.191) while the smallest correlation occurs on Day 18 (r=.061). We can use these results to estimate within-plant and within-pot correlation structure after imposing the same constraints on Model B that we did on Model F (i.e., \\(\\sigma_{\\tilde{v}}^{2}=\\sigma_{\\tilde{u}\\tilde{v}}=0\\)). Using the same REML variance components estimates as above except that \\(\\hat{\\sigma}_{\\tilde{v}}^{2}=0\\) rather than \\(.00126\\) and \\(\\hat{\\sigma}_{\\tilde{u}\\tilde{v}}=\\hat{\\rho}_{\\tilde{u}\\tilde{v}}\\sqrt{\\hat{\\sigma}_{\\tilde{u}}^{2}\\hat{\\sigma}_{\\tilde{v}}^{2}}=0\\) rather than \\(-.00455\\), the within-plant correlation structure is estimated to be: \\[ Corr(\\textbf{Y}_{ij}) = \\left[ \\begin{array}{cccc} 1 &amp; &amp; &amp; \\\\ .80 &amp; 1 &amp; &amp; \\\\ .75 &amp; .84 &amp; 1 &amp; \\\\ .70 &amp; .82 &amp; .88 &amp; 1 \\end{array} \\right] \\] for all plants \\(j\\) and all pots \\(i\\), and the correlation structure between different plants from the same pot is estimated to be: \\[ Corr(\\textbf{Y}_{ij},\\textbf{Y}_{ij&#39;}) = \\left[ \\begin{array}{cccc} .104 &amp; &amp; &amp; \\\\ .095 &amp; .087 &amp; &amp; \\\\ .084 &amp; .077 &amp; .068 &amp; \\\\ .073 &amp; .067 &amp; .059 &amp; .052 \\end{array} \\right]. \\] Our model restrictions produced slightly higher estimated within-plant correlations, especially for observations separated by longer periods of time. Standard deviation terms by timepoint are very similar (estimated SDs of .652, .713, .806, and .923 for days 13, 18, 23, and 28, respectively). In terms of the relationship between heights of different plants from the same pot, our model restrictions produced slightly higher correlation estimates with Day 13 height, but slightly lower correlation estimates associated with heights at Days 23 and 28. For measurements taken at the same timepoint, the largest correlation between plants from the same pot now occurs on Day 13 (r=.104) while the smallest correlation now occurs on Day 28 (r=.052). None of the differences in the covariance structure, however, should have a large impact on final conclusions, especially regarding fixed effects, so our strategy for dealing with boundary constraints appears very reasonable. In addition, the covariance structure implied by our standard 3-level model appears to model the correlation structure we observed in Figure 10.12 during our exploratory analyses very nicely. Even the variability over time implied by the standard model matches well with the raw observed variability in height by time period (respective standard deviations of .64, .64, .86, and .91). Thus, we feel well justified in fitting models based on the standard covariance structure. 10.9.1 Details of covariance structures In this section, we present additional details regarding implications of our standard covariance structure for 3-level models. We will focus on Model B; derivations for Model F would proceed in a similar fashion. The variance for an individual observation can be derived as: \\[\\begin{eqnarray*} Var(Y_{ijk}) &amp; = &amp; Var(\\epsilon_{ijk}+u_{ij}+\\tilde{u}_{i}+(v_{ij}+\\tilde{v}_{i})\\textstyle{time}_{ijk}) \\\\ &amp; = &amp; (\\sigma^{2} + \\sigma_{u}^{2} + \\sigma_{\\tilde{u}}^{2}) + 2(\\sigma_{uv} + \\sigma_{\\tilde{u}\\tilde{v}})t_k + (\\sigma_{v}^{2} + \\sigma_{\\tilde{v}}^{2})t_{k}^{2} \\end{eqnarray*}\\] The covariance between observations taken at different timepoints from the same plant is: \\[\\begin{eqnarray*} Cov(Y_{ijk},Y_{ijk&#39;}) &amp; = &amp; Cov(\\epsilon_{ijk}+u_{ij}+\\tilde{u}_{i}+(v_{ij}+\\tilde{v}_{i})t_{k}, \\epsilon_{ijk&#39;}+u_{ij}+\\tilde{u}_{i}+(v_{ij}+\\tilde{v}_{i})t_{k&#39;}) \\\\ &amp; = &amp; (\\sigma_{u}^{2} + \\sigma_{\\tilde{u}}^{2}) + (\\sigma_{uv} + \\sigma_{\\tilde{u}\\tilde{v}})(t_{k}+t_{k&#39;}) + (\\sigma_{v}^{2} + \\sigma_{\\tilde{v}}^{2})t_{k}t_{k&#39;} \\end{eqnarray*}\\] The covariance between observations taken from different plants from the same pot is: \\[\\begin{eqnarray*} Cov(Y_{ijk},Y_{ij&#39;k&#39;}) &amp; = &amp; Cov(\\epsilon_{ijk}+u_{ij}+\\tilde{u}_{i}+(v_{ij}+\\tilde{v}_{i})t_{k}, \\epsilon_{ij&#39;k&#39;}+u_{ij&#39;}+\\tilde{u}_{i}+(v_{ij&#39;}+\\tilde{v}_{i})t_{k&#39;}) \\\\ &amp; = &amp; \\sigma_{\\tilde{u}}^{2} + \\sigma_{\\tilde{u}\\tilde{v}}(t_{k}+t_{k&#39;}) + \\sigma_{\\tilde{v}}^{2}t_{k}t_{k&#39;} \\end{eqnarray*}\\] Based on these variances and covariances and the expressions for \\(Cov(\\textbf{Y}_{ij})\\) and \\(Cov(\\textbf{Y}_{ij},\\textbf{Y}_{ij&#39;})\\) in Section 10.9, the complete covariance matrix for observations from pot \\(i\\) can be expressed as the following 24x24 matrix (assuming 4 observations over time for each of 6 plants): \\[ Cov(\\textbf{Y}_{i}) = \\left[ \\begin{array}{cccccc} Cov(\\textbf{Y}_{i1}) &amp; &amp; &amp; &amp; &amp; \\\\ Cov(\\textbf{Y}_{i1},\\textbf{Y}_{i2}) &amp; Cov(\\textbf{Y}_{i2}) &amp; &amp; &amp; &amp; \\\\ Cov(\\textbf{Y}_{i1},\\textbf{Y}_{i3}) &amp; Cov(\\textbf{Y}_{i2},\\textbf{Y}_{i3}) &amp; Cov(\\textbf{Y}_{i3}) &amp; &amp; &amp; \\\\ Cov(\\textbf{Y}_{i1},\\textbf{Y}_{i4}) &amp; Cov(\\textbf{Y}_{i2},\\textbf{Y}_{i4}) &amp; Cov(\\textbf{Y}_{i3},\\textbf{Y}_{i4}) &amp; Cov(\\textbf{Y}_{i4}) &amp; &amp; \\\\ Cov(\\textbf{Y}_{i1},\\textbf{Y}_{i5}) &amp; Cov(\\textbf{Y}_{i2},\\textbf{Y}_{i5}) &amp; Cov(\\textbf{Y}_{i3},\\textbf{Y}_{i5}) &amp; Cov(\\textbf{Y}_{i4},\\textbf{Y}_{i5}) &amp; Cov(\\textbf{Y}_{i5}) &amp; \\\\ Cov(\\textbf{Y}_{i1},\\textbf{Y}_{i6}) &amp; Cov(\\textbf{Y}_{i2},\\textbf{Y}_{i6}) &amp; Cov(\\textbf{Y}_{i3},\\textbf{Y}_{i6}) &amp; Cov(\\textbf{Y}_{i4},\\textbf{Y}_{i6}) &amp; Cov(\\textbf{Y}_{i5},\\textbf{Y}_{i6}) &amp; Cov(\\textbf{Y}_{i6}) \\end{array} \\right]. \\] A covariance matrix for our entire data set, therefore, would be block diagonal, with \\(Cov(\\textbf{Y}_{i})\\) matrices along the diagonal reflecting within pot correlation and 0’s off-diagonal reflecting the assumed independence of observations from plants from different pots. As with any covariance matrix, we can convert the \\(Cov(\\textbf{Y}_{ij},\\textbf{Y}_{ij&#39;})\\) blocks for two different plants from the same pot into correlation matrices by dividing covariance terms by the product of corresponding standard deviations. Specifically, for \\(Cov(\\textbf{Y}_{ij},\\textbf{Y}_{ij&#39;})\\), the diagonal terms in a correlation matrix are formed by \\(Corr(Y_{ijk},Y_{ij&#39;k})=\\frac{\\tilde{\\tau}_{kk}}{\\sqrt{Var(Y_{ijk})Var(Y_{ij&#39;k})}}=\\frac{\\tilde{\\tau}_{kk}}{\\tau_{k}^{2}}\\) and the off-diagonal terms are formed by \\(Corr(Y_{ijk},Y_{ij&#39;k&#39;})=\\frac{\\tilde{\\tau}_{kk&#39;}}{\\sqrt{Var(Y_{ijk})Var(Y_{ij&#39;k&#39;})}}=\\frac{\\tilde{\\tau}_{kk&#39;}}{\\tau_{k}\\tau_{k&#39;}}\\). We calculated estimated covariance and correlation matrices within plant and between plants in Section 10.9 based on the standard covariance structure for three-level models. However, it can sometimes be helpful to consider alternative covariance structures and evaluate the robustness of results to changes in assumptions. A couple of natural covariance structures to fit in the Seed Germination case study, given the observed structure in our data, are the heterogeneous compound symmetry and heterogeneous AR(1) models. We fit both structures, along with the toeplitz structure, and compared the resulting models with our standard 3-level model. In all cases, the AIC and BIC from the standard model (615.2 and 651.4, respectively) are considerably lower than the corresponding performance measures from the models with alternative covariance structures. Thus, we feel justified in fitting models based on the standard covariance structure. 10.10 Notes on Using R (Optional) The R code below fits Models A-C.1 from Sections 10.4 and 10.5. Note that, in lmer(), an estimated variance at Level One (\\(\\sigma^{2}\\)) comes for “free”, but variance terms at Level Two (\\(\\sigma_{u}^{2}\\)) and Level Three (\\(\\sigma_{\\tilde{u}}^{2}\\)) must be specified separately in Model A through “(1\\(|\\)plant)” and “(1\\(|\\)pot)”. Specifying “(time13\\(|\\)plant)” in Model B is equivalent to specifying “(1+time13\\(|\\)plant)” and produces a total of 3 variance components to estimate: variances for error terms associated with the intercept (\\(\\sigma_{u}^{2}\\) comes for “free”) and slope (\\(\\sigma_{v}^{2}\\) comes from the time13 term) at the plant level, along with a covariance or correlation (\\(\\sigma_{uv}\\) or \\(\\rho_{uv}\\)) between those two error terms. To restrict \\(\\sigma_{uv} = \\rho_{uv} = 0\\), you could specify each error term separately: “(1\\(|\\)plant) + (time13\\(|\\)plant)”. Also note that to fit Model C.1 in R, the random error components are written as “(time13\\(|\\)plant) + (1\\(|\\)pot)”, indicating you’d like error terms associated with the intercept and slope at the plant level, but only for the intercept term at the pot level. The fixed effects in Model C.1 just reflect all fixed effects in the composite model. library(lme4) # Model A - unconditional means modelal = lmer(hgt ~ 1 + (1|plant) + (1|pot), REML=T, data=leaddata) summary(modelal) # Model B - unconditional growth modelbl = lmer(hgt ~ time13 + (time13|plant) + (time13|pot), REML=T, data=leaddata) summary(modelbl) # Model C - add covariates at pot level modelcl = lmer(hgt ~ time13 + strl + cult + rem + time13:strl + time13:cult + time13:rem + (time13|plant) + (time13|pot), REML=T, data=leaddata) summary(modelcl) # get corr=-1 at pot level # Model C.1 - remove two variance components at Level Two from Model C modelcl0 = lmer(hgt ~ time13 + strl + cult + rem + time13:strl + time13:cult + time13:rem + (time13|plant) + (1|pot), REML=T, data=leaddata) summary(modelcl0) In Section 10.6 we sought to perform a significance test comparing Models C and C.1, where Model C.1 restricted two variance components from Model C to be 0. Our initial attempt used the anova() function in R, which created two problems: (a) the anova() function uses full maximum likelihood estimates rather than REML estimates of model parameters and performance, which is fine when two models differ in fixed effects but not, as in this case, when two models differ only in random effects; and, (b) the likelihood ratio test statistic is often not well approximated by a chi-square distribution. Therefore, we implemented the parametric bootstrap method to simulate the distribution of the likelihood ratio test statistic and obtain a more reliable p-value, also illustrating that the chi-square distribution would produce an artificially large p-value. anova(modelcl0,modelcl) # go with Model C.0 (fewer varcomps) # run bootstrapAnova function = parametric bootstrap code for lme4-models # http://stats.stackexchange.com/questions/4858/ # longitudinal-data-analysis-by-multilevel-modeling-in-r/4870#4870 # Posted on Nov 24, 2010, by Fabian Scheipl #m0 is the lmer model under the null hypothesis (i.e. the smaller model) #mA is the lmer model under the alternative bootstrapAnova &lt;- function(mA, m0, B=100){ oneBootstrap &lt;- function(m0, mA){ d &lt;- drop(simulate(m0)) m2 &lt;-refit(mA, newresp=d) m1 &lt;-refit(m0, newresp=d) return(anova(m2,m1)$Chisq[2]) } nulldist &lt;- replicate(B, oneBootstrap(m0, mA)) ret &lt;- anova(mA, m0) ret$&quot;Pr(&gt;Chisq)&quot;[2] &lt;- mean(ret$Chisq[2] &lt; nulldist) names(ret)[7] &lt;- &quot;Pr_boot(&gt;Chisq)&quot; attr(ret, &quot;heading&quot;) &lt;- c(attr(ret, &quot;heading&quot;)[1], paste(&quot;Parametric bootstrap with&quot;, B,&quot;samples.&quot;), attr(ret, &quot;heading&quot;)[-1]) attr(ret, &quot;nulldist&quot;) &lt;- nulldist return(ret) } bRLRT = bootstrapAnova(mA=modelcl, m0=modelcl0) nullLRT = attr(bRLRT,&quot;nulldist&quot;) hist(nullLRT,prob=T) abline(v=2.1471,col=2) # 2.1471 is the observed LRT statistic x=seq(0,max(nullLRT),length=1000) y=dchisq(x,2) lines(x,y) 10.11 Exercises 10.11.1 Conceptual Exercises Seed Germination. In Sections 10.3.1 and 10.3.2, why must we be careful excluding plants with no height data? Why can’t we simply leave those plants in our 3-level analysis with all missing heights set to 0 mm? Give an example of a Level Two covariate that might have been recorded in this study. In Figure 10.1, would using mean heights by pot be reasonable as well? How about for Figure 10.2? Explain how “plant-to-plant variability can be estimated by averaging standard deviations from each pot … while pot-to-pot variability can be estimated by finding the standard deviation of average intercept or slope within pot.” Shouldn’t we subtract the mean number of days rather than 13 to calculate centered time? Why does the lower correlation between intercepts and slopes produced by centered time result in a more stable model? Explain why an autoregressive error structure is suggested for leadplant data at the end of Section 10.3.2. The experimental factors of interest in the seed germination study are Level Three covariates, yet the unconditional means model shows only 4.6% of total variability in plant heights to be at Level Three. Does that mean a multilevel model will not be helpful? Would it be better to find the mean height for each pot and just use those 72 values to examine the effects of experimental factors? Explain why a likelihood ratio test is appropriate for comparing Models B and C. Should we be concerned that \\(\\hat{\\sigma}_{0}^{2}\\) increased from Model A to B? Why or why not? Explain the idea of boundary constraints in your own words. Why can it be a problem in multilevel models? In Model C, we initially addressed boundary constraints by removing the Level Three correlation between error terms from our multilevel model. What other model adjustments might we have considered? Recall Model C from Chapter 9. Describe the steps for performing a parametric bootstrap test on \\(\\sigma_{01} = \\sigma_{1}^{2} = 0\\). How does Figure 10.14 show that a likelihood ratio test using a chi-square distribution would be biased? In Section 10.7, a model with 52 parameters is described: (a) illustrate that the model does indeed contain 52 parameters; (b) explain how to minimize the total number of parameters using ideas from Section 10.7; (c) what assumptions have you made in your simplification in (b)? In Section 10.8, Model F (the null model) is compared to Model D using a parametric bootstrap test. As in Section 10.6, show in detail how bootstrapped data would be generated under Model F for, say, Plant # 1 from Pot # 1. For the random parts, tell what distribution the random pieces are coming from and then select a random value from that distribution. Finally, explain how the parametric bootstrap test would be carried out. Section 10.8 contains an interpretation for the coefficient of a three-way interaction term, \\(\\hat{\\beta}_{3}\\). Provide an alternative interpretation for \\(\\hat{\\beta}_{3}\\) by focusing on how the sterilization-by-soil type interaction differs over time. Collective Efficacy and Violent Crime. In a 1997 Science article, Sampson, Raudenbush, and Earls studied the effects on violent crime of a neighborhood’s collective efficacy, defined as “social cohesion among neighbors combined with their willingness to intervene on behalf of the common good.” Multiple items related to collective efficacy were collected from 8782 Chicago residents from 343 neighborhood clusters. For this study, give the observational units at Levels One, Two, and Three. Table 10.3 shows Table 3 from Sampson et al. (1997). Provide interpretations of the bolded coefficients in context. Table 10.3: Table 3: Correlates of collective efficacy from Sampson et al. (1997). Variable Coefficient SE t ratio Intercept 3.523 0.013 263.20 Person-Level predictors Female -0.012 0.015 -0.76 Married -0.005 0.021 -0.25 Separated or divorced -0.045 0.026 -1.72 Single -0.026 0.024 -1.05 Homeowner 0.122 0.020 6.04 Latino 0.042 0.028 1.52 Black -0.029 0.030 -0.98 Mobility -0.025 0.007 -3.71 Age 0.0021 0.0006 3.47 Years in neighborhood 6e-04 0.0008 0.78 SES 0.035 0.008 4.64 Neighborhood-level predictors Concentrated disadvantage -0.172 0.016 -10.74 Immigrant concentration -0.037 0.014 -2.66 Residential stability 0.074 0.013 5.61 Variance Components Within neighborhoods 0.32 Between neighborhoods 0.026 Percent of variance explained Within neighborhoods 3.2 Between neighborhoods 70.3 Based on Table 10.3, let \\(Y_{ijk}\\) be the response of person \\(j\\) from neighborhood \\(i\\) to item \\(k\\) regarding collective efficacy; these are (difficulty-adjusted) responses to 10 items per person about collective efficacy. Then (a) write out the three-level model that likely produced this table, and (b) write out the corresponding composite model. Assume there was also an unreported variance component estimating item-to-item variability within person. Suggest valuable exploratory data analysis plots to complement Table 10.3. If the model suggested by Table 10.3 were expanded to include all potential variance components, how many total parameters would need to be estimated? If it were expanded further to include the 3 neighborhood-level covariates as predictors in all Level Three equations, how many total parameters would need to be estimated? At the bottom of Table 10.3, the percent of variance explained is given within and between neighborhoods. Explain what these values likely represent and how they were calculated. Table 10.4 shows a portion of Table 4 from Sampson et al. (1997). Describe the multilevel model that likely produced this table. State the primary result from this table in context. [Note that collective efficacy is a Level Three covariate in this table, summarized over an entire neighborhood.] Table 10.4: A portion of Table 4: Neighborhood correlates of perceived neighborhood violence from Sampson et al. (1997). Model 1:social composition Model 2:social composition and collective efficacy Variable Coefficient SE t Coefficient SE t Concentrated disadvantage 0.277 0.021 13.30 0.171 0.024 7.24 Immigrant concentration 0.041 0.017 2.44 0.018 0.016 1.12 Residential stability -0.102 0.015 -6.95 -0.056 0.016 -3.49 Collective efficacy -0.618 0.104 -5.95 Estimates of neighborhood-level coefficients control for gender, marital status, homeownership, ethnicity, mobility, age, years in neighborhood, and SES of those interviewed. Model 1 accounts for 70.5% of the variation between neighborhoods in perceived violence, whereas model 2 accounts for 77.8% of the variation. 10.11.2 Guided Exercise Kentucky Math Scores. Data was collected from 48,058 eighth graders from Kentucky who took the California Basic Educational Skills Test (Bickel 2007). These students attended 235 different middle schools from 132 different districts, and the following variables were collected: dis_id = District Identifier sch_id = School Identifier stud_nm = Student Identifier female = Coded 1 if Female and 0 if Male nonwhite = Coded 0 if White and 1 otherwise readn = California Test of Basic Skills Reading Score mathn = California Test of Basic Skills Math Score sch_size = School-Level Size (centered natural log) sch_ses = School-Level SES (socio-economic status - centered) dis_size = District-Level Size (centered natural log) dis_ses = District-Level SES (socio-economic status - centered) The primary research questions are whether or not math scores of eighth graders in Kentucky differ based on gender or ethnicity, and if any gender gap differs by ethnicity, or if any ethnicity gap differs by gender. Researchers wanted to be sure to adjust for important covariates at the school and district levels (e.g. school/district size and socio-economic status). Conduct a short exploratory data analysis. Which variables are most strongly related to mathn? Illustrate with summary statistics and plots. [Hints: your plots should accommodate the large number of observations in the data, and your summary statistics will have to handle occasional missing values.] (Model A) Run an unconditional means model with 3 levels and report the percentage of variability in math scores that can be explained at each level. (Model B) Add female, nonwhite, and their interaction at Level One. Write out the complete three-level model. How many parameters must be estimated? Run the model (be patient – it may take a few minutes!). Report and interpret a relevant pseudo-Rsquare value. Is there evidence (based on the t-value) of a significant interaction? In layman’s terms, what can you conclude based on the test for interaction? (Model C) Subtract the female-by-nonwhite interaction from Model B and add sch_ses, where sch_ses is a predictor in all Level Two equations. How many parameters must be estimated? Break your count down into fixed effects + variance components at Level 1 + varcomps at Level 2 + varcomps at Level 3. (No need to write out the model or run the model unless you want to…) (Model D) Subtract nonwhite from Model C and add dis_size, where dis_size is a predictor in all Level Three equations. Write out the complete three-level model. Also write out the composite model. How many parameters must be estimated? Run the model (be patient!), and then re-run it with an error term only on the intercept equation at Level Three. What are the implications of using this error structure? Does it change any conclusions about fixed effects? Which of the two models would you choose and why? (Optional) Explore the nature of the 3-way interaction between female, sch_ses, and dis_size by finding the predicted math score based on Model D with an error term only on the intercept equation at Level Three in 5 cases. Comment on trends you observe. Female vs. male with average sch_ses and dis_size Female vs. male with sch_ses at Q1 and dis_size at Q1 Female vs. male with sch_ses at Q1 and dis_size at Q3 Female vs. male with sch_ses at Q3 and dis_size at Q1 Female vs. male with sch_ses at Q3 and dis_size at Q3 (Optional) Create two scatterplots to illustrate the 3-way interaction between female, sch_ses, and dis_size. 10.11.3 Open-ended Exercises Seed Germination: Coneflowers Repeat the exploratory data analyses and model fitting from this chapter using coneflowers rather than leadplants. Mudamalai Leaf Growth Plant growth is influenced by many environmental factors, among them sunlight and water availability. A study conducted in the Mudamalai Wildlife Sanctuary in India in October of 2008 had as its purpose to “broaden the specific knowledge of certain tree species and climate zones located in the Nilgiri Hills of South India, and to enhance the overall knowledge of the dynamic relationship between plant growth and its environment” (Pray, 2009). Study researchers collected 1,960 leaves from 45 different trees (5 trees from each of 3 species within each of 3 climate zone). Within each tree, 3 branches were randomly chosen from each of 3 strata (high, medium, and low), and 5 leaves were randomly selected from each branch. Three different descriptive climatic zones were chosen for analysis—dry thorn, dry deciduous, and moist deciduous—and three different species of trees were analyzed—Cassia fistula (golden shower tree), Anogeissus latifolia (axlewood tree), and Diospyros montana (mountain ebony). Height and girth were measured for each tree, and length was assessed for each branch. Measurements taken on each leaf included length, width, surface area (determined carefully for 25 leaves per species and then by linear regression using length and width as predictors for the rest), pedial length (the length of the stem), pedial width, and percent herbivory (an eyeball estimate of the percent of each leaf that had been removed or eaten by herbivores). In addition, stomata density was measured using a compound scope to examine nail polish impressions of leaves for each strata of each tree (135 measurements). Here is a description of available variables: Species = tree species (Cassia fistula, Anogeissus latifolia, or Diospyros montana) Zone = climate zone (dry thorn, dry deciduous, or moist deciduous) Tree = tree number (1-5) within climate zone Tree.height = tree height (m) Tree.girth = tree girth (cm) Strata = height of branch from ground (high, medium, or low) Branch = branch number (1-3) within tree and strata Branch.length = length of branch (cm) Length = length of leaf (cm) Width = width of leaf (cm) Area = surface area of leaf (sq cm) Pedial.length = length of the leaf stem (mm) Pedial.width = width of the leaf stem (mm) Herbivory = percent of leaf removed or eaten by herbivores Stomata = density of specialized openings that allow for gas exchange on leaves Biology researchers were interested in determining “optimal physical characteristics for growth and survival of these trees in the various areas” to further conservation efforts. Construct a multilevel model to address the researchers’ questions, focusing on leaf area as the response of interest. Defend your final model, and interpret fixed effect and variance component estimates produced by your model. "],
["ch-GLMM.html", "Chapter 11 Generalized Linear Multilevel Models 11.1 Learning Objectives 11.2 Case Study: College Basketball Referees 11.3 Initial Exploratory Analyses 11.4 Two level Modeling with a Generalized Response 11.5 Crossed Random Effects 11.6 Model Comparisons Using the Parametric Bootstrap 11.7 A Potential Final Model for Examining Referee Bias 11.8 Estimated Random Effects 11.9 Notes on Using R (Optional) 11.10 Exercises 11.11 Conceptual Exercises", " Chapter 11 Generalized Linear Multilevel Models 11.1 Learning Objectives After finishing this chapter, students should be able to: Recognize when generalized linear multilevel models (GLMMs) are appropriate. Understand how GLMMs essentially combine ideas from earlier chapters. Apply exploratory data analysis techniques to multilevel data with non-normal responses. Recognize when crossed random effects are appropriate and how they differ from nested random effects. Write out a generalized linear multilevel statistical model, including assumptions about variance components. Interpret model parameters (including fixed effects and variance components) from a GLMM. Generate and interpret random effect estimates. 11.2 Case Study: College Basketball Referees An article by Anderson and Pierce (2009) describes empirical evidence that officials in NCAA men’s college basketball tend to “even out” foul calls over the course of a game, based on data collected in 2004-2005. Using logistic regression to model the effect of foul differential on the probability that the next foul called would be on the home team (controlling for score differential, conference, and whether or not the home team had the lead), Anderson and Pierce found that “the probability of the next foul being called on the visiting team can reach as high as 0.70.” More recently, Moskowitz and Wertheim, in their book Scorecasting (2012), argue that the number one reason for the home field advantage in sports is referee bias. Specifically, in basketball, they demonstrate that calls over which referees have greater control—offensive fouls, loose ball fouls, ambiguous turnovers such as palming and traveling—were more likely to benefit the home team than more clearcut calls, especially in crucial situations. Data have been gathered from the 2009-2010 college basketball season for three major conferences to investigate the following questions (Noecker and Roback 2013): Does evidence that college basketball referees tend to “even out” calls still exist in 2010 as it did in 2005? How do results change if our analysis accounts for the correlation in calls from the same game and the same teams? Is the tendency to even out calls stronger for fouls over which the referee generally has greater control? Fouls are divided into offensive, personal, and shooting fouls, and one could argue that referees have the most control over offensive fouls (typically where the player with the ball knocks over a stationary defender) and the least control over shooting fouls (where an offensive player is fouled in the act of shooting). Are the actions of referees associated with the score of the game? 11.3 Initial Exploratory Analyses 11.3.1 Data organization Examination of data for Case Study 11.2 reveals the following key variables: game = unique game identification number date = date game was played (YYYYMMDD) visitor = visiting team abbreviation hometeam = home team abbreviation foul.num = cumulative foul number within game foul.home = indicator if foul was called on the home team foul.vis = indicator if foul was called on the visiting team foul.diff = the difference in fouls before the current foul was called (home - visitor) score.diff = the score differential before the current foul was called (home - visitor) lead.vis = indicator if visiting team has the lead lead.home = indicator if home team has the lead previous.foul.home = indicator if previous foul was called on the home team previous.foul.vis = indicator if previous foul was called on the visiting team foul.type = categorical variable if current foul was offensive, personal, or shooting shooting = indicator if foul was a shooting foul personal = indicator if foul was a personal foul offensive = indicator if foul was an offensive foul time = number of minutes left in the first half when foul called Data was collected for 4972 fouls over 340 games from the Big Ten, ACC, and Big East conference seasons during 2009-2010. We focus on fouls called during the first half to avoid the issue of intentional fouls by the trailing team at the end of games. Table 11.1 illustrates key variables from the first 10 rows of the data set. Table 11.1: Key variables from the first 10 rows of data from the College Basketball Referees case study. Each row represents a different foul called; we see all 8 first-half fouls from Game 1 followed by the first 2 fouls called in Game 2. Table continues below game visitor hometeam foul.num foul.home foul.diff score.diff 1 IA MN 1 0 0 7 1 IA MN 2 1 -1 10 1 IA MN 3 1 0 11 1 IA MN 4 0 1 11 1 IA MN 5 0 0 14 1 IA MN 6 0 -1 22 1 IA MN 7 1 -2 25 1 IA MN 8 1 -1 23 2 MI MIST 1 0 0 2 2 MI MIST 2 1 -1 2 lead.home foul.type time 1 Personal 14.17 1 Personal 11.43 1 Personal 10.23 1 Personal 9.733 1 Shooting 7.767 1 Shooting 5.567 1 Shooting 2.433 1 Offensive 1 1 Shooting 18.98 1 Personal 17.2 For our initial analysis, our primary response variable is foul.home, and our primary hypothesis concerns evening out foul calls. We hypothesize that the probability a foul is called on the home team is inversely related to the foul differential; that is, if more fouls have been called on the home team than the visiting team, the next foul is less likely to be on the home team. The structure of this data suggests a couple of familiar attributes combined in an unfamiliar way. With a binary response variable, a generalized linear model is typically applied, especially one with a logit link function (indicating logistic regression). But, with covariates at multiple levels—some at the individual foul level and others at the game level—a multilevel model would also be sensible. So what we need is a multilevel model with a non-normal response; in other words, a generalized linear multilevel model (GLMM). We will investigate what such a model might look like in the next section, but we will still begin by exploring the data with initial graphical and numerical summaries. As with other multilevel situations, we will begin with broad summaries across all 4972 foul calls from all 340 games. Most of the variables we have collected can vary with each foul called; these Level One variables include: whether or not the foul was called on the home team (our response variable), the game situation at the time the foul was called (the time remaining in the first half, who is leading and by how many points, the foul differential between the home and visiting team, and who the previous foul was called on), and the type of foul called (offensive, personal, or shooting). Level Two variables, those that remain unchanged for a particular game, then include only the home and visiting teams, although we might consider attributes such as attendance, team rankings, etc. 11.3.2 Exploratory analyses In Figure 11.1, we see histograms for the continuous Level One covariates (time remaining, foul differential, and score differential). These plots treat each foul within a game as independent even though we expect them to be correlated, but they provide a sense for the overall patterns. We see that time remaining is reasonably uniform, except during the first five minutes of the first half when relatively fewer fouls are called. Score differential and foul differential are both bell-shaped, with a mean slightly favoring the home team in both cases – on average, the home team leads by 2.04 points (SD 7.24) and has 0.36 fewer previous fouls (SD 2.05) at the time a foul is called. Figure 11.1: Histograms showing distributions of the 3 continuous Level One covariates: (a) time remaining, (b) score difference, and (c) foul difference. Summaries of the categorical response (whether the foul was called on the home team) and categorical Level One covariates (whether the home team has the lead and what type of foul was called) can be provided through tables of proportions. More fouls are called on visiting teams (52.1%) than home teams, the home team is more likely to hold a lead (57.1%), and personal fouls are most likely to be called (51.6%), followed by shooting fouls (38.7%) and then offensive fouls (9.7%). For an initial examination of Level Two covariates (the home and visiting teams), we can take the number of times, for instance, Minnesota (MN) appears in the long data set (with one row per foul called as illustrated in Table 11.1) as the home team and divide by the number of unique games in which Minnesota is the home team. This ratio (12.1), found in Table 11.2, shows that Minnesota is among the bottom three teams in the average total number of fouls in the first halves of games in which it is the home team. That is, games at Minnesota have few total fouls relative to games played elsewhere. Accounting for the effect of home and visiting team will likely be an important part of our model, since some teams tend to play in games with twice as many fouls called as others, and other teams see a noticeable disparity in the total number of fouls depending on if they are home or away. Table 11.2: Average total number of fouls in the first half over all games in which a particular team is home or visitor. The left columns show the top 3 and bottom 3 teams according to total number of fouls (on both teams) in first halves of games in which they are the home team. The middle columns correspond to games in which the listed teams are the visitors, and the right columns show the largest differences (in both directions) between total fouls in games in which a team is home or visitor. Home Visitor Difference Duke 20.0 WVa 21.4 Duke 4.0 Top 3 VaTech 19.4 Nova 19.0 Wisc 2.6 Nova 19.1 Wake 18.6 Pitt 2.3 Mich 10.6 Wisc 10.4 WVa -6.9 Bottom 3 Ill 11.6 Mich 11.1 Mia -2.7 MN 12.1 PSU 11.3 Clem -2.6 Next, we inspect numerical and graphical summaries of relationships between Level One model covariates and our binary model response. As with other multilevel analyses, we will begin by observing broad trends involving all 4972 fouls called, even though fouls from the same game may be correlated. The conditional density plots in the first row of Figure 11.2 examine continuous Level One covariates. Figure 11.2a provides support for our primary hypothesis about evening out foul calls, indicating a very strong trend for fouls to be more often called on the home team at points in the game when more fouls had previously been called on the visiting team. Figures 11.2b and 11.2c then show that fouls were somewhat more likely to be called on the home team when the home team’s lead was greater and (very slightly) later in the half. Conclusions from the conditional density plots in Figure 11.2a-c are supported with associated empirical logit plots in Figure 11.2d-f. If a logistic link function is appropriate, these plots should be linear, and the stronger the linear association, the more promising the predictor. We see in Figure 11.2d further confirmation of our primary hypothesis, with lower log-odds of a foul called on the home team the greater number of previous fouls the home team had accumulated compared to the visiting team. Figure 11.2e shows that game score may play a role in foul trends, as the log-odds of a foul on the home team grows as the home team accumulates a bigger lead on the scoreboard, and Figure 11.2f shows a very slight tendency for greater log-odds of a foul called on the home team as the half proceeds (since points on the right are closer to the beginning of the game). Conclusions about continuous Level One covariates are further supported by summary statistics calculated separately for fouls called on the home team and those called on the visiting team. For instance, when a foul is called on the home team, there is an average of 0.64 additional fouls on the visitors at that point in the game, compared to an average of 0.10 additional fouls on the visitors when a foul is called on the visiting team. Similarly, when a foul is called on the home team, they are in the lead by an average of 2.7 points, compared to an average home lead of 1.4 points when a foul is called on the visiting team. As expected, the average time remaining the half is very similar for home and visitor fouls (9.2 vs. 9.5 minutes, respectively). The segmented barcharts in Figure 11.3 examine categorical Level One covariates, indicating that fouls were more likely to be called on the home team when the home team was leading, when the previous foul was on the visiting team, and when the foul was a personal foul rather than a shooting foul or an offensive foul. 51.8% of calls go against the home team when it is leading the game, compared to only 42.9% of calls when the it is behind; 51.3% of calls go against the home team when the previous foul went against the visitors, compared to only 43.8% of calls when the previous foul went against the home team; and, 49.2% of personal fouls are called against the home team, compared to only 46.9% of shooting fouls and 45.7% of offensive fouls. Eventually we will want to examine the relationship between foul type (personal, shooting, or offensive) and foul differential, examining our hypothesis that the tendency to even out calls will be even stronger for calls over which the referees have greater control (personal fouls and especially offensive fouls). Figure 11.2: Conditional density and empirical logit plots of the binary model response (foul called on home or visitor) vs. the three continuous Level One covariates (foul differential, score differential, and time remaining). The dark shading in a conditional density plot shows the proportion of fouls called on the home team for a fixed value of (a) foul differential, (b) score differential, and (c) time remaining. In empirical logit plots, estimated log odds of a home team foul are calculated for each distinct foul (d) and score (e) differential, except for differentials at the high and low extremes with insufficient data; for time (f), estimated log odds are calculated for two-minute time intervals and plotted against the midpoints of those interval. Figure 11.3: Segmented barcharts of the binary model response (foul called on home or visitor) vs. the three categorical Level One covariates (foul type (a), team in the lead (b), and team called for the previous foul (c) ). Each bar shows the percentage of fouls called on the home team vs. the percentage of fouls called on the visiting team for a particular category of the covariate. The exploratory analyses presented above are an essential first step in understanding our data, seeing univariate trends, and noting bivariate relationships between variable pairs. However, our important research questions (a) involve the effect of foul differential after adjusting for other significant predictors of which team is called for a foul, (b) account for potential correlation between foul calls within a game (or within a particular home or visiting team), and (c) determine if the effect of foul differential is constant across game conditions. In order to address research questions such as these, we need to consider multilevel, multivariate statistical models for a binary response variable. 11.4 Two level Modeling with a Generalized Response 11.4.1 A multiple generalized linear model approach (correlation not accounted for) One quick and dirty approach to analysis might be to run a multiple logistic regression model on the entire long data set of 4972 fouls. In fact, Anderson and Pierce ran such a model in their 2009 paper, using the results of their multiple logistic regression model to support their primary conclusions, while justifying their approach by confirming a low level of correlation within games and the minimal impact on fixed effect estimates that accounting for clustering would have. Output from one potential multiple logistic regression model is shown below; this initial modeling attempt shows significant evidence that referees tend to even out calls (i.e., that the probability of a foul called on the home team decreases as total home fouls increase compared to total visiting team fouls—that is, as foul.diff increases) after accounting for score differential and time remaining (Z=-3.078, p=.002). The extent of the effect of foul differential also appears to grow (in a negative direction) as the first half goes on, based on an interaction between time remaining and foul differential (Z=-2.485, p=.013). We will compare this model with others that formally account for clustering and correlation patterns in our data. glm(formula = foul.home ~ foul.diff + score.diff + lead.home + time + foul.diff:time + lead.home:time, family = binomial, data = refdata) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -0.103416 0.101056 -1.023 0.30614 foul.diff -0.076599 0.024888 -3.078 0.00209 ** score.diff 0.020062 0.006660 3.012 0.00259 ** lead.home -0.093227 0.160082 -0.582 0.56032 time -0.013141 0.007948 -1.653 0.09827 . foul.diff:time -0.007492 0.003014 -2.485 0.01294 * lead.home:time 0.021837 0.011161 1.957 0.05040 . --- Null deviance: 6884.3 on 4971 degrees of freedom Residual deviance: 6747.6 on 4965 degrees of freedom AIC: 6761.6 11.4.2 A two-stage modeling approach (provides the basic idea for multilevel modeling) As we saw in Section 8.4.2, to avoid clustering we could consider fitting a separate regression model to each unit of observation at Level Two (each game in this case). Since our primary response variable is binary (Was the foul called on the home team or not?), we would fit a logistic regression model to the data from each game. For example, consider the 14 fouls called during the first half of the March 3, 2010, game featuring Virginia at Boston College (Game 110). Table 11.3: Key variables from the March 3, 2010, game featuring Virginia at Boston College (Game 110). visitor hometeam foul.num foul.home foul.diff score.diff foul.type time VA BC 1 0 0 0 Shooting 19.783333 VA BC 2 0 -1 4 Personal 18.950000 VA BC 3 0 -2 7 Shooting 16.916667 VA BC 4 0 -3 10 Personal 14.883333 VA BC 5 1 -4 10 Personal 14.600000 VA BC 6 1 -3 6 Offensive 9.750000 VA BC 7 1 -2 4 Shooting 9.366667 VA BC 8 0 -1 4 Personal 9.200000 VA BC 9 0 -2 6 Shooting 7.666667 VA BC 10 0 -3 12 Shooting 2.500000 VA BC 11 1 -4 13 Personal 2.083333 VA BC 12 1 -3 13 Personal 1.966667 VA BC 13 0 -2 12 Shooting 1.733333 VA BC 14 1 -3 14 Personal 0.700000 Is there evidence from this game that referees tend to “even out” foul calls when one team starts to accumulate more fouls? Is the score differential associated with the probability of a foul on the home team? Is the effect of foul differential constant across all foul types during this game? We can address these questions through multiple logistic regression applied to only data from Game 110. First, notation must be defined. Let \\(Y_{ij}\\) be an indicator variable recording if the \\(j^{th}\\) foul from Game \\(i\\) was called on the home team (1) or the visiting team (0). We can consider \\(Y_{ij}\\) to be a Bernoulli random variable with parameter \\(p_{ij}\\), where \\(p_{ij}\\) is the true probability that the \\(j^{th}\\) foul from Game \\(i\\) was called on the home team. As in Chapter ??, we will begin by modeling the logit function of \\(p_{ij}\\) for Game \\(i\\) as a linear function of Level One covariates. For instance, if we were to consider a simple model with foul differential as the sole predictor, we could model the probability of a foul on the home team in Game 110 with the model: \\[\\begin{equation} log(\\frac{p_{110j}}{1-p_{110j}})=a_{110}+b_{110}\\mathrm{foul.diff}_{110j} \\tag{11.1} \\end{equation}\\] where \\(i\\) is fixed at 110. Note that there is no separate error term or variance parameter, since the variance is a function of \\(p_{ij}\\) with a Bernoulli random variable. Maximum likelihood estimators for the parameters in this model (\\(a_{110}\\) and \\(b_{110}\\)) can be obtained through statistical software. \\(e^{a_{110}}\\) represents the odds that a foul is called on the home team when the foul totals in Game 110 are even, and \\(e^{b_{110}}\\) represents the multiplicative change in the odds that a foul is called on the home team for each additional foul for the home team relative to the visiting team during the first half of Game 110. For example, if we compared the situation when the home team has been called for 3 more fouls than the visiting team to when the home team has been called for 4 more fouls than the visitors, we’d estimate \\(e^{b_{110}}\\) to be the multiplicative difference in odds of a foul called on the home team (the odds should decrease from 3 to 4 additional home fouls). For Game 110, we estimate \\(\\hat{a}_{110}=-5.67\\) and \\(\\hat{b}_{110}=-2.11\\) (see output below). Thus, according to our simple logistic regression model, the odds that a foul is called on the home team when both teams have an equal number of fouls in Game 110 is \\(e^{-5.67}=0.0035\\); that is, the probability that a foul is called on the visiting team (0.9966) is \\(1/0.0035 = 289\\) times higher than the probability a foul is called on the home team (0.0034) in that situation. While these parameter estimates seem quite extreme, reliable estimates are difficult to obtain with 14 observations and a binary response variable, especially in a case like this where the fouls were only even at the start of the game. Also, as the gap between home and visiting fouls increases by 1, the odds that a foul is called on the visiting team increases by a multiplicative factor of more than 8 (since \\(1/e^{-2.11}=8.25\\)). In Game 110, this trend toward referees evening out foul calls is statistically significant at the 0.10 level (Z=-1.851, p=.0642). glm(formula = foul.home ~ foul.diff, family = binomial, data = game110) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -5.668 3.131 -1.810 0.0703 . foul.diff -2.110 1.140 -1.851 0.0642 . --- Null deviance: 19.121 on 13 degrees of freedom Residual deviance: 11.754 on 12 degrees of freedom AIC: 15.754 As in Section 8.4.2, we can proceed to fit similar logistic regression models for each of the 339 other games in our data set. Each model will yield a new estimate of the intercept and the slope from Equation (11.1). These estimates are summarized graphically in Figure 11.4. There is noticeable variability among the 340 games in the fitted intercepts and slopes, with an IQR for intercepts ranging from -1.45 to 0.70, and an IQR for slopes ranging from -1.81 to -0.51. The majority of intercepts are below 0 (median = -0.41), so that in most games a foul is less likely to be called on the home team when foul totals are even. In addition, almost all of the estimated slopes are below 0 (median = -0.89), indicating that as the home team’s foul total grows in relation to the visiting team’s foul total, the odds of a foul on the home team continues to decrease. Figure 11.4: Histograms of (a) intercepts and (b) slopes from fitting simple logistic regression models by game. Several extreme outliers have been cut off in these plots for illustration purposes. At this point, you might imagine expanding model building efforts in a couple of directions: (a) continue to improve the Level One model in Equation (11.1) by controlling for covariates and adding potential interaction terms, or (b) build Level Two models to explain the game-to-game variability in intercepts or slopes using covariates which remain constant from foul to foul within a game (like the teams playing). While we could pursue these two directions independently, we can accomplish our modeling goals in a much cleaner and more powerful way by proceeding as in Chapter 8 and building a unified multilevel framework under which all parameters are estimated simultaneously and we remain faithful to the correlation structure inherent in our data. 11.4.3 A unified multilevel approach (the framework we’ll use) As in Chapters 8 and 9, we will write out a composite model after first expressing Level One and Level Two models. That is, we will create Level One and Level Two models as in Section 11.4.2, but we will then combine those models into a composite model and estimate all model parameters simultaneously. Once again \\(Y_{ij}\\) is an indicator variable recording if the \\(j^{th}\\) foul from Game \\(i\\) was called on the home team (1) or the visiting team (0), and \\(p_{ij}\\) is the true probability that the \\(j^{th}\\) foul from Game \\(i\\) was called on the home team. Our Level One model with foul differential as the sole predictor is given by Equation (11.1): \\[ log(\\frac{p_{ij}}{1-p_{ij}})=a_i+b_i\\mathrm{foul.diff}_{ij} \\] Then we include no fixed covariates at Level Two, but we include error terms to allow the intercept and slope from Level One to vary by game, and we allow these errors to be correlated: \\[\\begin{eqnarray*} a_i &amp; = &amp; \\alpha_{0}+u_i \\\\ b_i &amp; = &amp; \\beta_{0}+v_i, \\end{eqnarray*}\\] where the error terms at Level Two can be assumed to follow a multivariate normal distribution: \\[ \\left[ \\begin{array}{c} a_i \\\\ b_i \\end{array} \\right] \\sim N \\left( \\left[ \\begin{array}{c} 0 \\\\ 0 \\end{array} \\right], \\left[ \\begin{array}{cc} \\sigma_{u}^{2} &amp; \\\\ \\sigma_{uv} &amp; \\sigma_{v}^{2} \\end{array} \\right] \\right) \\] Our composite model then looks like: \\[\\begin{eqnarray*} log(\\frac{p_{ij}}{1-p_{ij}}) &amp; = &amp; a_i+b_i\\mathrm{foul.diff}_{ij} \\\\ &amp; = &amp; (\\alpha_{0}+u_i) + (\\beta_{0}+v_i)\\mathrm{foul.diff}_{ij} \\\\ &amp; = &amp; [\\alpha_{0}+\\beta_{0}\\mathrm{foul.diff}_{ij}]+[u_i+v_i\\mathrm{foul.diff}_{ij}] \\end{eqnarray*}\\] The major changes when moving from a normally distributed response to a binary response are the form of the response variable (a logit function) and the absence of an error term at Level One. Again, we can use statistical software to obtain parameter estimates for this unified multilevel model using all 4972 fouls recorded from the 340 games. For example, the glmer() function from the lme4 package in R extends the lmer() function to handle generalized responses and to account for the fact that fouls are not independent within games. Results are given below for the two-level model with foul differential as the sole covariate and Game as the Level Two observational unit. Formula: foul.home ~ foul.diff + (foul.diff | game) AIC BIC logLik deviance df.resid 6791.1 6823.6 -3390.5 6781.1 4967 Random effects: Groups Name Variance Std.Dev. Corr game (Intercept) 0.294145 0.54235 foul.diff 0.001235 0.03514 -1.00 Number of obs: 4972, groups: game, 340 Fixed effects: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -0.15684 0.04637 -3.382 0.000719 *** foul.diff -0.28533 0.03835 -7.440 1.01e-13 *** When parameter estimates from the multilevel model above are compared with those from the naive logistic regression model assuming independence of all observations (below), there are noticeable differences. For instance, each additional foul for the visiting team is associated with a 33% increase (\\(1/e^{-.285}\\)) in the odds of a foul called on the home team under the multilevel model, but the single level model estimates the same increase as only 14% (\\(1/e^{-.130}\\)). Also, estimated standard errors for fixed effects are greater under generalized linear multilevel modeling, which is not unusual after accounting for correlated observations, which effectively reduces the sample size. glm(formula = foul.home ~ foul.diff, family = binomial, data = refdata) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -0.13005 0.02912 -4.466 7.98e-06 *** foul.diff -0.13047 0.01426 -9.148 &lt; 2e-16 *** --- Null deviance: 6884.3 on 4971 degrees of freedom Residual deviance: 6798.1 on 4970 degrees of freedom AIC: 6802.1 11.5 Crossed Random Effects In the College Basketball Referees case study, our two primary Level Two covariates are home team and visiting team. In Section 11.3.2 we showed evidence that the probability a foul is called on the home team changes if we know precisely who the home and visiting teams are. However, if we were to include an indicator variable for each distinct team, we would need 38 indicator variables for home teams and 38 more for visiting teams. That’s a lot of degrees of freedom to spend! And adding those indicator variables would complicate our model considerably, despite the fact that we’re not very interested in specific coefficient estimates for each team—we just want to control for teams playing to draw stronger conclusions about referee bias (focusing on the foul.diff variable). One way, then, of accounting for teams is by treating them as random effects rather than fixed effects. For instance, we can assume the effect that Minnesota being the home team has on the log odds of foul call on the home team is drawn from a normal distribution with mean 0 and variance \\(\\sigma^{2}_{v}\\). Maybe the estimated random effect for Minnesota as the home team is -0.5, so that Minnesota is slightly less likely that the typical home team to have fouls called on it, and the odds of a foul on the home team are somewhat correlated whenever Minnesota is the home team. In this way, we account for the complete range of team effects, while only having to estimate a single parameter (\\(\\sigma^{2}_{v}\\)) rather than coefficients for 38 indicator variables. The effect of visiting teams can be similarly modeled. How will treating home and visiting teams as random effects change our multilevel model? Another way we might view this situation is by considering that Game is not the only Level Two observational unit we might have selected. What if we instead decided to focus on Home Team as the Level Two observational unit? That is, what if we assumed that fouls called on the same home team across all games must be correlated? In this case, we could redefine our Level One model from Equation (11.1). Let \\(Y_{hj}\\) be an indicator variable recording if the \\(j^{th}\\) foul from Home Team \\(h\\) was called on the home team (1) or the visiting team (0), and \\(p_{hj}\\) be the true probability that the \\(j^{th}\\) foul from Home Team \\(h\\) was called on the home team. Now, if we were to consider a simple model with foul differential as the sole predictor, we could model the probability of a foul on the home team for Home Team \\(h\\) with the model: \\[\\begin{equation} log(\\frac{p_{hj}}{1-p_{hj}})=a_h+b_h\\mathrm{foul.diff}_{hj} \\tag{11.2} \\end{equation}\\] In this case, \\(e^{a_{h}}\\) represents the odds that a foul is called on the home team when total fouls are equal between both teams in a game involving Home Team \\(h\\), and \\(e^{b_{h}}\\) represents the multiplicative change in the odds that a foul is called on the home team for every extra foul on the home team compared to the visitors in a game involving Home Team \\(h\\). After fitting logistic regression models for each of the 39 teams in our data set, we see in Figure 11.5 variability in fitted intercepts (mean=-0.15, sd=0.33) and slopes (mean=-0.22, sd=0.12) among the 39 teams, although much less variability than we observed from game-to-game. Of course, each logistic regression model for a home team was based on about 10 times more foul calls than each model for a game, so observing less variability from team-to-team was not unexpected. Figure 11.5: Histograms of (a) intercepts and (b) slopes from fitting simple logistic regression models by home team. From a modeling perspective, accounting for clustering by game and by home team (not to mention by visiting team) brings up an interesting issue we have not yet considered—can we handle random effects that are not nested? Since each foul called is associated with only one game (or only one home team and one visiting team), foul is considered nested in game (or home or visiting team). However, a specific home team is not associated with a single game; that home team appears in several games. Therefore, any effects of game, home team, and visiting team are considered crossed random effects. A two-level model which accounts for variability among games, home teams, and visiting teams would take on a slightly new look. First, the full subscripting would change a bit. Our primary response variable would now be written as \\(Y_{i[gh]j}\\), an indicator variable recording if the \\(j^{th}\\) foul from Game \\(i\\) was called on the home team (1) or the visiting team (0), where Game \\(i\\) pitted Visiting Team \\(g\\) against Home Team \\(h\\). Square brackets are introduced since \\(g\\) and \\(h\\) are essentially at the same level as \\(i\\), whereas we have assumed (without stating so) throughout this book that subscripting without square brackets implies a movement to lower levels as the subscripts move left to right (e.g., \\(ij\\) indicates \\(i\\) units are at Level Two, while \\(j\\) units are at Level One, nested inside Level Two units). We can then consider \\(Y_{i[gh]j}\\) to be a Bernoulli random variable with parameter \\(p_{i[gh]j}\\), where \\(p_{i[gh]j}\\) is the true probability that the \\(j^{th}\\) foul from Game \\(i\\) was called on Home Team \\(h\\) rather than Visiting Team \\(g\\). We will include the crossed subscripting only where necessary. Typically, with the addition of crossed effects, the Level One model will remain familiar and changes will be seen at Level Two, especially in the equation for the intercept term. In the model formulation below we allow, as before, the slope and intercept to vary by game: Level One: \\[\\begin{equation} log(\\frac{p_{i[gh]j}}{1-p_{i[gh]j}})=a_{i}+b_{i}\\mathrm{foul.diff}_{ij} \\tag{11.3} \\end{equation}\\] Level Two: \\[\\begin{eqnarray*} a_{i} &amp; = &amp; \\alpha_{0}+u_{i}+v_{h}+w_{g} \\\\ b_{i} &amp; = &amp; \\beta_{0}, \\end{eqnarray*}\\] Therefore, at Level Two, we assume that \\(a_{i}\\), the log odds of a foul on the home team when the home and visiting teams in Game \\(i\\) have an equal number of fouls, depends on four components: \\(\\alpha_{0}\\) is the population average log odds across all games and fouls (fixed) \\(u_{i}\\) is the effect of Game \\(i\\) (random) \\(v_{h}\\) is the effect of Home Team \\(h\\) (random) \\(w_{g}\\) is the effect of Visiting Team \\(g\\) (random) where error terms (random effects) at Level Two can be assumed to follow independent normal distributions: \\[\\begin{eqnarray*} u_{i} &amp; \\sim &amp; N \\left( 0 , \\sigma_{u}^{2} \\right) \\\\ v_{h} &amp; \\sim &amp; N \\left( 0 , \\sigma_{v}^{2} \\right) \\\\ w_{g} &amp; \\sim &amp; N \\left( 0 , \\sigma_{w}^{2} \\right). \\end{eqnarray*}\\] We could include terms that vary by home or visiting team in other Level Two equations, but often adjusting for these random effects on the intercept is sufficient. The advantages to including additional random effects are three-fold. First, by accounting for additional sources of variability, we should obtain more precise estimate of other model parameters, including key fixed effects. Second, we obtain estimates of variance components, allowing us to compare the relative sizes of game-to-game and team-to-team variability. Third, as outlined in Section 11.8, we can obtain estimated random effects which allow us to compare the effects on the log-odds of a home foul of individual home and visiting teams. Our composite model then looks like: \\[\\begin{equation} log(\\frac{p_{i[gh]j}}{1-p_{i[gh]j}}) = [\\alpha_{0}+\\beta_{0}\\mathrm{foul.diff}_{ij}]+[u_{i}+v_{h}+w_{g}]. \\tag{11.4} \\end{equation}\\] We will refer to this as Model A, where we look at the effect of foul differential on the odds a foul is called on the home team, while accounting for three crossed random effects at Level Two (game, home team, and visiting team). Parameter estimates for Model A are given below: Formula: Data: refdata AIC BIC logLik deviance df.resid 6780.5 6813.0 -3385.2 6770.5 4967 Random effects: Groups Name Variance Std.Dev. game (Intercept) 0.17165 0.4143 hometeam (Intercept) 0.06809 0.2609 visitor (Intercept) 0.02323 0.1524 Number of obs: 4972, groups: game, 340; hometeam, 39; visitor, 39 Fixed effects: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -0.18780 0.06331 -2.967 0.00301 ** From this output, we obtain estimates of our five model parameters: \\(\\hat{\\alpha}_{0}=-0.188=\\) the mean log odds of a home foul at the point where total fouls are equal between teams. In other words, when fouls are balanced between teams, the odds that a foul is called on the visiting team is 20.7% higher than the odds a foul is called on the home team (\\(1/e^{-.188}=1.207\\)). \\(\\hat{\\beta}_{0}=-0.264=\\) the decrease in mean log odds of a home foul for each 1 foul increase in the foul differential. More specifically, the odds the next foul is called on the visiting team rather than the home team increases by 30.2% with each additional foul called on the home team (\\(1/e^{-.264}=1.302\\)). \\(\\hat{\\sigma}_{u}^{2}=0.172=\\) the variance in intercepts from game-to-game. \\(\\hat{\\sigma}_{v}^{2}=0.068=\\) the variance in intercepts among different home teams. \\(\\hat{\\sigma}_{w}^{2}=0.023=\\) the variance in intercepts among different visiting teams. Based on the t-value (-6.80) and p-value (\\(p&lt;.001\\)) associated with foul differential in this model, we have significant evidence of a negative association between foul differential and the odds of a home team foul. That is, we have significant evidence that the odds that a foul is called on the home team shrinks as the home team has more total fouls compared with the visiting team. Thus, there seems to be preliminary evidence in the 2009-10 data that college basketball referees tend to even out foul calls over the course of the first half. Of course, we have yet to adjust for other significant covariates. 11.6 Model Comparisons Using the Parametric Bootstrap Our estimates of variance components provide evidence of the relative variability in the different Level Two random effects. For example, an estimated 65.4% of variability in the intercepts is due to differences from game-to-game, while 25.9% is due to differences among home teams, and 8.7% is due to differences among visiting teams. At this point, we could reasonably ask: if we use a random effect to account for differences among games, does it really pay off to also account for which team is home and which is the visitor? To answer this, we could compare models with and without the random effects for home and visiting teams (i.e., Model A, the full model, vs. Model A.0, the reduced model) using a likelihood ratio test. In Model A.0, Level Two now looks like: \\[\\begin{eqnarray*} a_{i} &amp; = &amp; \\alpha_{0}+u_{i} \\\\ b_{i} &amp; = &amp; \\beta_{0}, \\end{eqnarray*}\\] The likelihood ratio test (see below) provides significant evidence (LRT=16.074, df=2, p=.0003) that accounting for variability among home teams and among visiting teams improves our model. Data: refdata Models: model.0a: foul.home ~ foul.diff + (1 | game) model.a: foul.home ~ foul.diff + (1 | game) + (1 | hometeam) + (1 | visitor) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) model.0a 3 6792.5 6812.1 -3393.3 6786.5 model.a 5 6780.5 6813.0 -3385.2 6770.5 16.074 2 0.0003233 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 In Section 8.5.4 we noted that REML estimation is typically used when comparing models that differ only in random effects for multilevel models with normal responses, but with generalized linear multilevel models, full maximum likelihood (ML) estimation procedures are typically used regardless of the situation (including in the function glmer() in R). So the likelihood ratio test above is based on ML methods—can we trust its accuracy? In Section 10.6 we introduced the parametric bootstrap as a more reliable method in many cases for conducting hypothesis tests compared to the likelihood ratio test, especially when comparing full and reduced models that differ in their variance components. What would a parametric bootstrap test say about testing \\(H_{O}: \\sigma_{v}^{2}=\\sigma_{w}^{2}=0\\) vs. \\(H_{A}:\\) at least one of \\(\\sigma_{v}^{2}\\) or \\(\\sigma_{w}^{2}\\) is not equal to 0? Under the null hypothesis, since the two variance terms are being set to a value (0) on the boundary constraint, we would not expect the chi-square distribution to adequately approximate the behavior of the likelihood ratio test statistic. Figure 11.6 illustrates the null distribution of the likelihood ratio test statistic derived by the parametric bootstrap procedure with 100 samples as compared to a chi-square distribution. As we observed in Section 10.6, the parametric bootstrap provides a more reliable p-value in this case (\\(p&lt;.001\\) from output below) because a chi-square distribution puts too much mass in the tail and not enough near 0. However, the parametric bootstrap is computationally intensive, and it can take a long time to run even with moderately complex models. With this data, we would select our full Model A based on a parametric bootstrap test. ## Data: refdata ## Parametric bootstrap with 100 samples. ## Models: ## m0: foul.home ~ foul.diff + (1 | game) ## mA: foul.home ~ foul.diff + (1 | game) + (1 | hometeam) + (1 | visitor) ## Df AIC BIC logLik deviance Chisq Chi Df Pr_boot(&gt;Chisq) ## m0 3 6792.5 6812.1 -3393.3 6786.5 ## mA 5 6780.5 6813.0 -3385.2 6770.5 16.074 2 0 Figure 11.6: Null distribution of likelihood ratio test statistic comparing Models A and A.0 derived using parametric bootstrap with 100 samples (histogram) compared to a chi-square distribution with 2 degrees of freedom (smooth curve). We might also reasonably ask: is it helpful to allow slopes (coefficients for foul differential) to vary by game, home team, and visiting team as well? Again, since we are comparing models that differ in random effects, and since the null hypothesis involves setting random effects at their boundaries, we use the parametric bootstrap. Formally, we are comparing Model A to Model A.1, which has the same Level One equation as Model A: \\[ log(\\frac{p_{i[gh]j}}{1-p_{i[gh]j}})=a_{i}+b_{i}\\mathrm{foul.diff}_{ij} \\] but more variance components to estimate at Level Two: \\[\\begin{eqnarray*} a_{i} &amp; = &amp; \\alpha_{0}+u_{i}+v_{h}+w_{g} \\\\ b_{i} &amp; = &amp; \\beta_{0}+z_{i}+r_{h}+s_{g}, \\end{eqnarray*}\\] where error terms (random effects) at Level Two can be assumed to follow independent normal distributions: \\[\\begin{eqnarray*} u_{i} &amp; \\sim &amp; N \\left( 0 , \\sigma_{u}^{2} \\right) \\\\ z_{i} &amp; \\sim &amp; N \\left( 0 , \\sigma_{z}^{2} \\right) \\\\ v_{h} &amp; \\sim &amp; N \\left( 0 , \\sigma_{v}^{2} \\right) \\\\ r_{h} &amp; \\sim &amp; N \\left( 0 , \\sigma_{r}^{2} \\right) \\\\ w_{g} &amp; \\sim &amp; N \\left( 0 , \\sigma_{w}^{2} \\right) \\\\ s_{g} &amp; \\sim &amp; N \\left( 0 , \\sigma_{s}^{2} \\right). \\end{eqnarray*}\\] Thus our null hypothesis for comparing Model A vs. Model A.1 is \\(H_{O}: \\sigma_{z}^{2}=\\sigma_{r}^{2}=\\sigma_{s}^{2}=0\\). We do not have significant evidence (LRT=0.349, df=3, p=.46 by parametric bootstrap) of variability among slopes, so we will only include random effects for game, home team, and visiting team for the intercept going forward. Figure 11.7 illustrates the null distribution of the likelihood ratio test statistic derived by the parametric bootstrap procedure as compared to a chi-square distribution, again showing that the tails are too heavy in the chi-square distribution. ## Data: refdata ## Parametric bootstrap with 100 samples. ## Models: ## m0: foul.home ~ foul.diff + (1 | game) + (1 | hometeam) + (1 | visitor) ## mA: foul.home ~ foul.diff + (1 | game) + (1 | hometeam) + (1 | visitor) + ## mA: (0 + foul.diff | game) + (0 + foul.diff | hometeam) + (0 + ## mA: foul.diff | visitor) ## Df AIC BIC logLik deviance Chisq Chi Df Pr_boot(&gt;Chisq) ## m0 5 6780.5 6813.0 -3385.2 6770.5 ## mA 8 6786.1 6838.2 -3385.1 6770.1 0.349 3 0.46 Figure 11.7: Null distribution of likelihood ratio test statistic comparing Models A and A.1 derived using parametric bootstrap with 100 samples (histogram) compared to a chi-square distribution with 3 degrees of freedom (smooth curve). Note that we could have also allowed for a correlation between the error terms for the intercept and slope by game, home team, or visiting team – i.e., assume, for example: \\[ \\left[ \\begin{array}{c} u_{i} \\\\ z_{i} \\end{array} \\right] \\sim N \\left( \\left[ \\begin{array}{c} 0 \\\\ 0 \\end{array} \\right], \\left[ \\begin{array}{cc} \\sigma_{u}^{2} &amp; \\\\ \\sigma_{uz} &amp; \\sigma_{z}^{2} \\end{array} \\right] \\right) \\] while error terms by game, home team, or visiting team are still independent. Here, the new model would have 6 additional parameters when compared with Model A (3 variance terms and 3 covariance terms). By the parametric bootstrap, there is no significant evidence that the model with 6 additional parameters is necessary (LRT=6.49, df=6, p=.06 by parametric bootstrap). The associated p-value based on a likelihood ratio test with approximate chi-square distribution (and restricted maximum likelihood estimation) is .370, reflecting once again the overly heavy tails of the chi-square distribution. 11.7 A Potential Final Model for Examining Referee Bias In constructing a final model for this college basketball case study, we are guided by several considerations. First, we want to estimate the effect of foul differential on the odds of a home foul, after adjusting for important covariates. Second, we wish to find important interactions with foul differential, recognizing that the effect of foul differential might depend on the game situation and other covariates. Third, we want to account for random effects associated with game, home team, and visiting team. What follows is one potential final model that follows these guidelines: Level One: \\[\\begin{eqnarray*} log(\\frac{p_{i[gh]j}}{1-p_{i[gh]j}}) &amp; = &amp; a_{i} + b_{i}\\mathrm{foul.diff}_{ij} + c_{i}\\mathrm{score.diff}_{ij} + d_{i}\\mathrm{lead.home}_{ij} + f_{i}\\mathrm{time}_{ij} + \\\\ &amp; &amp; k_{i}\\mathrm{offensive}_{ij} + l_{i}\\mathrm{personal}_{ij} + m_{i}\\mathrm{foul.diff:offensive}_{ij} + \\\\ &amp; &amp; n_{i}\\mathrm{foul.diff:personal}_{ij} + o_{i}\\mathrm{foul.diff:time}_{ij} + \\\\ &amp; &amp; q_{i}\\mathrm{lead.home:time}_{ij} \\end{eqnarray*}\\] Level Two: \\[\\begin{eqnarray*} a_{i} &amp; = &amp; \\alpha_{0}+u_{i}+v_{h}+w_{g} \\\\ b_{i} &amp; = &amp; \\beta_{0} \\\\ c_{i} &amp; = &amp; \\gamma_{0} \\\\ d_{i} &amp; = &amp; \\delta_{0} \\\\ f_{i} &amp; = &amp; \\phi_{0} \\\\ k_{i} &amp; = &amp; \\kappa_{0} \\\\ l_{i} &amp; = &amp; \\lambda_{0} \\\\ m_{i} &amp; = &amp; \\mu_{0} \\\\ n_{i} &amp; = &amp; \\nu_{0} \\\\ o_{i} &amp; = &amp; \\omega_{0} \\\\ q_{i} &amp; = &amp; \\xi_{0}, \\end{eqnarray*}\\] where error terms at Level Two can be assumed to follow independent normal distributions: \\[\\begin{eqnarray*} u_{i} &amp; \\sim &amp; N \\left( 0 , \\sigma_{u}^{2} \\right) \\\\ v_{h} &amp; \\sim &amp; N \\left( 0 , \\sigma_{v}^{2} \\right) \\\\ w_{g} &amp; \\sim &amp; N \\left( 0 , \\sigma_{w}^{2} \\right). \\end{eqnarray*}\\] Our composite model then looks like: \\[\\begin{eqnarray*} log(\\frac{p_{i[gh]j}}{1-p_{i[gh]j}}) &amp; = &amp; [\\alpha_{0} + \\beta_{0}\\mathrm{foul.diff}_{ij} + \\gamma_{0}\\mathrm{score.diff}_{ij} + \\delta_{0}\\mathrm{lead.home}_{ij} + \\phi_{0}\\mathrm{time}_{ij} + \\\\ &amp; &amp; \\kappa_{0}\\mathrm{offensive}_{ij} + \\lambda_{0}\\mathrm{personal}_{ij} + \\mu_{0}\\mathrm{foul.diff:offensive}_{ij} + \\\\ &amp; &amp; \\nu_{0}\\mathrm{foul.diff:personal}_{ij} + \\omega_{0}\\mathrm{foul.diff:time}_{ij} + \\\\ &amp; &amp; \\xi_{0}\\mathrm{lead.home:time}_{ij}] + [u_{i}+v_{h}+w_{g}]. \\end{eqnarray*}\\] Using the composite form of this generalized linear multilevel model, the parameter estimates for our 11 fixed effects and 3 variance components are given in the output below: Formula: foul.home ~ foul.diff + score.diff + lead.home + time + offensive + personal + foul.diff:offensive + foul.diff:personal + foul.diff:time + Data: refdata AIC BIC logLik deviance df.resid 6731.0 6822.2 -3351.5 6703.0 4958 Random effects: Groups Name Variance Std.Dev. game (Intercept) 0.18461 0.4297 hometeam (Intercept) 0.07831 0.2798 visitor (Intercept) 0.04313 0.2077 Number of obs: 4972, groups: game, 340; hometeam, 39; visitor, 39 Fixed effects: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -0.246475 0.133956 -1.840 0.065772 . foul.diff -0.171469 0.045363 -3.780 0.000157 *** score.diff 0.033522 0.008236 4.070 4.7e-05 *** lead.home -0.150619 0.177204 -0.850 0.395339 time -0.008747 0.008560 -1.022 0.306868 offensive -0.080795 0.111232 -0.726 0.467616 personal 0.067200 0.065397 1.028 0.304148 foul.diff:offensive -0.103574 0.053869 -1.923 0.054516 . foul.diff:personal -0.055634 0.031948 -1.741 0.081614 . foul.diff:time -0.008689 0.003274 -2.654 0.007955 ** In general, we see a highly significant negative effect of foul differential—a strong tendency for referees to even out foul calls when one team starts amassing more fouls than the other. Important covariates to control for (because of their effects on the odds of a home foul) include score differential, whether the home team held the lead, time left in the first half, and the type of foul called. Furthermore, we see that the effect of foul differential depends on type of foul called and time left in the half—the tendency for evening out foul calls is stronger earlier in the half, and when offensive and personal fouls are called instead of shooting fouls. The effect of foul type supports the hypothesis that if referees are consciously or subconsciously evening out foul calls, the behavior will be more noticeable for calls over which they have more control, especially offensive fouls (which are notorious judgment calls) and then personal fouls (which don’t affect a player’s shot, and thus a referee can choose to let them go uncalled). Evidence like this can be considered dose response, since higher “doses” of referee control are associated with a greater effect of foul differential on their calls. A dose response effect provides even stronger indication of referee bias. Analyses of data from 2004-05 (Noecker and Roback, 2013) showed that the tendency to even out foul calls was stronger when one team had a large lead, but we found no evidence of a foul differential by score differential interaction in the 2009-10 data, although home team fouls are more likely when the home team has a large lead, regardless of the foul differential. Here are specific interpretations of key model parameters: \\(\\exp(\\hat{\\alpha}_{0})=\\exp(-0.247)=0.781\\). The odds of a foul on the home team is 0.781 at the end of the first half when the score is tied, the fouls are even, and the referee has just called a shooting foul. In other words, only 43.9% of shooting fouls in those situations will be called on the home team. \\(\\exp(\\hat{\\beta}_{0})=\\exp(-0.172)=0.842\\). Also, \\(0.842^{-1}=1.188\\). As the foul differential decreases by 1 (the visiting team accumulates another foul relative to the home team), the odds of a home foul increase by 18.8%. This interpretation applies to shooting fouls at the end of the half, after controlling for the effects of score differential and whether the home team has the lead. \\(\\exp(\\hat{\\gamma}_{0})=\\exp(0.034)=1.034\\). As the score differential increases by 1 (the home team accumulates another point relative to the visiting team), the odds of a home foul increase by 3.4%, after controlling for foul differential, type of foul, whether or not the home team has the lead, and time remaining in the half. Referees are more likely to call fouls on the home team when the home team is leading, and vice versa. Note that a change in the score differential could result in the home team gaining the lead, so that the effect of score differential experiences a non-linear “bump” at 0, where the size of the bump depends on the time remaining (this would involve the interpretation for \\(\\hat{\\xi}_{0}\\)). \\(\\exp(\\hat{\\mu}_{0})=\\exp(-0.103)=0.902\\). Also, \\(0.902^{-1}=1.109\\). The effect of foul differential increases by 10.9% if a foul is an offensive foul rather than a shooting foul, after controlling for score differential, whether the home team has the lead, and time remaining. As hypothesized, the effect of foul differential is greater for offensive fouls, over which referees have more control when compared with shooting fouls. For example, midway through the half (time=10), the odds that a shooting foul is on the home team increase by 29.6% for each extra foul on the visiting team, while the odds that an offensive foul is on the home team increase by 43.6%. \\(\\exp(\\hat{\\nu}_{0})=\\exp(-0.056)=0.946\\). Also, \\(0.946^{-1}=1.057\\). The effect of foul differential increases by 5.7% if a foul is a personal foul rather than a shooting foul, after controlling for score differential, whether the home team has the lead, and time remaining. \\(\\exp(\\hat{\\omega}_{0})=\\exp(-0.0087)=0.991\\). Also, \\(0.991^{-1}=1.009\\). The effect of foul differential increases by 0.9% for each extra minute that is remaining in the half, after controlling for foul differential, score differential, whether the home team has the lead, and type of foul. Thus, the tendency to even out foul calls is strongest earlier in the game. For example, midway through the half (time=10), the odds that a shooting foul is on the home team increase by 29.6% for each extra foul on the visiting team, while at the end of the half (time=0) the odds increase by 18.8% for each extra visiting foul. \\(\\hat{\\sigma}_{u}^{2}=0.185=\\) the variance in log-odds intercepts from game-to-game after controlling for all other covariates in the model. 11.8 Estimated Random Effects Our final model includes random effects for game, home team, and visiting team, and thus our model accounts for variability due to these three factors without estimating fixed effects to represent specific teams or games. However, we may be interested in examining the relative level of the random effects used for different teams or games. The presence of random effects allows different teams or games to begin with different baseline odds of a home foul. In other words, the intercept—the log odds of a home foul at the end of the first half when the score is tied, the fouls are even, and a shooting foul has been called—is adjusted by a random effect specific to the two teams playing and to the game itself. Together, the estimated random effects for, say, home team, should follow a normal distribution as we assumed: centered at 0 and with variance given by \\(\\sigma_{v}^{2}\\). It is sometimes interesting to know, then, which teams are typical (with estimated random effects near 0) and which fall in the upper and lower tails of the normal distribution. Figure 11.8 shows the 39 estimated random effects associated with each home team (see the discussion on Empirical Bayes estimates in Chapter 8). As expected, the distribution is normal with standard deviation near \\(\\hat{\\sigma}_{v}=0.28\\). To compare individual home teams, Figure 11.9 shows the estimated random effect and associated prediction interval for each of the 39 home teams. Although there is a great deal of uncertainty surrounding each estimate, we see that, for instance, DePaul and Seton Hall have higher baseline odds of home fouls than Purdue or Syracuse. Similar histograms and prediction intervals plots can be generated for random effects due to visiting teams and specific games. Figure 11.8: Histogram of estimated random effects for 39 home teams in Model F. Figure 11.9: Estimated random effects and associated prediction intervals for 39 home teams in Model F. 11.9 Notes on Using R (Optional) The R code below fits Model A.1 from Section 11.6. Note that, in glmer() or lmer(), if you have two equations at Level Two and want to fit a model with an error term for each equation, but you also want to assume that the two error terms are independent, the error terms must be requested separately. For example, “(1\\(|\\)hometeam)” allows the Level One intercept to vary by home team, while “(0+foul.dff\\(|\\)hometeam)” allows the Level One effect of foul.diff to vary by home team. Under this formulation, the correlation between those two error terms is assumed to be 0; a non-zero correlation could have been specified with “(1+foul.diff\\(|\\)hometeam)”. library(lme4) # Model A.1 (Multilevel model with only foul.diff and all 3 random effects # applied to both intercepts and slopes but no correlations) model.a3 &lt;- glmer(foul.home~ foul.diff + (1|game)+ (1|hometeam)+(1|visitor) + (0+foul.diff|game)+(0+foul.diff|hometeam)+ (0+foul.diff|visitor),family=binomial,data=refdata) The R code below shows how fixef() can be used to extract the estimated fixed effects from a multilevel model. Even more, it shows how ranef() can be used to illustrate estimated random effects by Game, Home Team, and Visiting Team, along with prediction intervals for those random effects. These estimated random effects are sometimes called Empirical Bayes estimators. In this case, random effects are placed only on the “[[“(Intercept)”]]” term; the phrase “Intercept” could be replaced with other Level One covariates whose values are allowed to vary by game, home team, or visiting team in our model. # Model F - final model? model.f=glmer(foul.home~foul.diff+score.diff+lead.home+time+offensive+ personal+foul.diff:offensive+foul.diff:personal+foul.diff:time+ lead.home:time+(1|game)+(1|hometeam)+(1|visitor),family=binomial, data=refdata) summary(model.f) exp(fixef(model.f)) # Get estimated random effects based on Model F re.int = ranef(model.f)$`game`[[&quot;(Intercept)&quot;]] hist(re.int,xlab=&quot;Random Effect&quot;,main=&quot;Random Effects for Game&quot;) Home.re=ranef(model.f)$`hometeam`[[&quot;(Intercept)&quot;]] hist(Home.re,xlab=&quot;Random Effect&quot;,main=&quot;Random Effects for Home Team&quot;) Visiting.re=ranef(model.f)$`visitor`[[&quot;(Intercept)&quot;]] hist(Visiting.re,xlab=&quot;Random Effect&quot;, main=&quot;Random Effects for the Visiting Team&quot;,xlim=c(-0.5,0.5)) cbind(Home.re,Visiting.re) # 39x2 matrix of REs by team # Prediction intervals for random effects based on Model F ranef1=dotplot(ranef(model.f, postVar = TRUE), strip = FALSE) print(ranef1[[3]], more = TRUE) ##HOME print(ranef1[[2]], more = TRUE) ##VIS print(ranef1[[1]], more = TRUE) 11.10 Exercises 11.11 Conceptual Exercises Give an example of a data set and an associated research question that would best be addressed with a multilevel model for a Poisson response. College Basketball Referees. Explain to someone unfamiliar with the plots in Figure 11.2 how to read both a conditional density plot and an empirical logit plot. For example, explain what the dark region in a conditional density plot represents, what each point in an empirical logit plot represents, etc. With the strength of the evidence found in Figure 11.2, plots (a) and (d), is there any need to run statistical models to convince someone that referee tendencies are related to the foul differential? In Section 11.4.2, why don’t we simply run a logistic regression model for each of the 340 games, collect the 340 intercepts and slopes, and fit a model to those intercepts and slopes? Explain in your own words the difference between crossed and nested random effects (Section 11.5). In the context of Case Study 9.2, describe a situation in which crossed random effects would be needed. Assume that we added \\(z_{i}\\), \\(r_{h}\\), and \\(s_{g}\\) to the Level Two equation for \\(b_{i}\\) in Equations (11.3). (a) Give interpretations for those 3 new random effects. (b) How many additional parameters would need to be estimated (and name the new model parameters)? In Section 11.6, could we use a likelihood ratio test to determine if it would be better to add either a random effect for home team or a random effect for visiting team to the model with a random effect for game (assuming we’re going to add one or the other)? Why or why not? Describe how we would obtain the 1000 values representing the parametric bootstrap distribution in the histogram in Figure 11.6. In Figure 11.6, why is it a problem that “a chi-square distribution puts too much mass in the tail” when using a likelihood ratio test to compare models? What would be the implications of using the R expression “(foul.diff \\(\\mid\\) game) + (foul.diff \\(\\mid\\) hometeam) + (foul.diff \\(\\mid\\) visitor)” in Model A.1 (Section 11.6) to obtain our variance components? Explain the implications of having no error terms in any Level Two equations in Section 11.7 except the one for \\(a_{i}\\). In the interpretation of \\(\\hat{\\beta}_{0}\\), explain why this applies to “shooting fouls at the end of the half”. Couldn’t we just say that we controlled for type of foul and time elapsed? In the interpretation of \\(\\hat{\\gamma}_{0}\\), explain the idea that “the effect of score differential experiences a non-linear ‘bump’ at 0, where the size of the bump depends on time remaining.” Consider, for example, the effect of a point scored by the home team with 10 minutes left in the first half, depending on whether the score is tied or the home team is ahead by 2. In the interpretation of \\(\\hat{\\mu}_{0}\\), verify the odds increases of 29.6% for shooting fouls and 43.6% for offensive fouls. Where does the stated 10.9% increase factor in? We could also interpret the interaction between two quantitative variables described by \\(\\hat{\\omega}_{0}\\) as “the effect of time remaining increases by 0.9% for each extra foul on the visiting team, after controlling for …” Numerically illustrate this interpretation by considering foul differentials of 2 (the home team has 2 more fouls than the visitors) and -2 (the visitors have 2 more fouls than the home team). Provide interpretations in context for \\(\\hat{\\phi}_{0}\\), \\(\\hat{\\kappa}_{0}\\), and \\(\\hat{\\xi}_{0}\\). In Section 11.8, why isn’t the baseline odds of a home foul for DePaul considered a model parameter? Heart attacks in Aboriginal Australians. Randall, et al., published a 2014 article in Health and Place entitled “Exploring disparities in acute myocardial infarction events between Aboriginal and non-Aboriginal Australians: Roles of age, gender, geography and area-level disadvantage.” They used multilevel Poisson models to compare rates of acute myocardial infarction (AMI) in the 199 Statistical Local Areas (SLAs) in New South Wales. Within SLA, AMI rates (number of events over population count) were summarized by subgroups determined by year (2002-2007), age (in 10-year groupings), sex, and Aboriginal status; these are then our Level One variables. Analyses also incorporated remoteness (classified by quartiles) and socio-economic status (classified by quintiles) assessed at the SLA level (Level Two). For this study, give the observational units at Levels One and Two. Table 11.4 shows Table 2 from Randall et al. (2014). Let \\(Y_{ij}\\) be the number of acute myocardial infarctions in subgroup \\(j\\) from SLA \\(i\\); write out the multilevel model that likely produced Table 11.4. How many fixed effects and variance components must be estimated? Table 11.4: Adjusted rate ratios for individual-level variables from the multilevel Poisson regression model with random intercept for area from Table 2 in Randall et al. (2014). RR 95% CI p-Value Aboriginal No(ref) 1 &lt;0.01 Yes 2.1 1.98-2.23 Age Group 25-34 (ref) 1 &lt;0.01 35-44 6.01 5.44-6.64 45-54 19.36 17.58-21.31 55-64 40.29 36.67-44.26 65-74 79.92 72.74-87.80 75-84 178.75 162.70-196.39 Sex Male (ref) 1 &lt;0.01 Female 0.45 0.44-0.45 Year 2002 (ref) 1 &lt;0.01 2003 1 0.98-1.03 2004 0.97 0.95-0.99 2005 0.91 0.89-0.94 2006 0.88 0.86-0.91 2007 0.88 0.86-0.91 Provide interpretations in context for the bolded rate ratios, confidence intervals, and p-values in Table 11.4. Given the rate ratio and 95% confidence interval reported for Aboriginal Australians in Table 11.4, find the estimated model fixed effect for Aboriginal Australians from the multilevel model along with its standard error. How might the p-value for Age have been produced? Randall et al. report that “we identified a significant interaction between Aboriginal status and age group (\\(p&lt;0.01\\)) and Aboriginal status and sex (\\(p&lt;0.01\\)), but there was no significant interaction between Aboriginal status and year (p=0.94).” How would the multilevel model associated with Table 11.4 need to have been adjusted to allow these interactions to be tested? Table 11.5 shows Table 3 from Randall et al. (2014). Describe the changes to the multilevel model for Table 11.4 that likely produced this new table. Provide interpretations in context for the bolded rate ratios, confidence intervals, and p-values in Table 11.5. Table 11.5: Adjusted rate ratios for area-level variables from the multilevel Poisson regression model with random intercept for area from Table 3 in Randall et al. (2014). RR 95% CI p-Value Remoteness of Residence* Major City 1 &lt;0.01 Inner Regional 1.16 1.04-1.28 Outer Regional 1.11 1.01-1.23 Remote/very remote 1.22 1.02-1.45 SES quintile* 1 least disadvantaged 1 &lt;0.01 2 1.26 1.11-1.43 3 1.4 1.24-1.58 4 1.46 1.30-1.64 5 most disadvantaged 1.7 1.52-1.91 Area-level factors added one at a time to the fully adjusted individual-level model (adjusted for Aboriginal status, age, sex and year) due to being highly associated. Randall et al. also report the results of a single-level Poisson regression model: “After adjusting for age, sex and year of event, the rate of AMI events in Aboriginal people was 2.3 times higher than in non-Aboriginal people (95% CI: 2.17-2.44).” Compare this to the results of the multilevel Poisson model; what might explain any observed differences? Randall et al. claim that “our application of multilevel modelling techniques allowed us to account for clustering by area of residence and produce ‘shrunken’ small-area estimates, which are not as prone to random fluctuations as crude or standardised rates.” Explain what they mean by this statement. 11.11.1 Guided Exercise 11.11.2 Open-ended Exercises Seed Germination We will return to the data from Case Study 10.2, but whether or not a seed germinated will be considered the response variable rather than the heights of germinated plants. We will use the wide data set (one row per plant) for this analysis as described in Section 10.3.1, although we will ignore plant heights over time and focus solely on if the plant germinated at any time. Use generalized linear multilevel models to determine the effects of soil type and sterilization on germination rates; perform separate analyses for coneflowers and leadplants, and describe differences between the two species. Support your conclusions with well-interpreted model coefficients and insightful graphical summaries. Book Banning The risk of literature censorship is constant, with provocative literature risking being challenged and subsequently being banned. Reasons for censorship have changed over time, with modern day challenges more likely to be for reasons relating to obscenity and child protection rather than overt political and religious objections (Jenkins, 2006). Many past studies have addressed chiefly the reasons listed by those who challenge a book, but few examine the overall context in which books are banned—for example, the characteristics of the states in which they occur. A team of students (Fast and Hegland, 2011) assembled a data set by starting with information on book challenges from the American Library Society. These book challenges—an initiation of the formal procedure for book censorship—occurred in US States between January of 2000 and November of 2010. In addition, state-level demographic information was obtained from the US Census Bureau and the Political Value Index (PVI) was obtained from the Cook Political Report. We will consider a data set with 931 challenges and 18 variables. All book challenges over the nearly 11 year period are included except those from the State of Texas; Texas featured 683 challenges over this timeframe, nearly 5 times the number in the next largest state. Thus, the challenges from Texas have been removed and could be analyzed separately. Here, then, is a description of available variables: book = unique book ID number booktitle = name of book author = name of author state = state where challenge made removed = 1 if book was removed (challenge was successful); 0 otherwise pvi2 = state score on the Political Value Index, where positive indicates a Democratic leaning, negative indicates a Republican leaning, and 0 is neutral cperhs = percentage of high school graduates in a state (grand mean centered) cmedin = median state income (grand median centered) cperba = percentage of college graduates in a state (grand mean centered) days2000 = date challenge was made, measured by number of days after January 1, 2000 obama = 1 if challenge was made while Barack Obama was President; 0 otherwise freqchal = 1 if book was written by a frequently challenged author (10 or more challenges across the country); 0 otherwise sexexp = 1 if reason for challenge was sexually explicit material; 0 otherwise antifamily = 1 if reason for challenge was antifamily material; 0 otherwise occult = 1 if reason for challenge was material about the occult; 0 otherwise language = 1 if reason for challenge was inappropriate language; 0 otherwise homosexuality = 1 if reason for challenge was material about homosexuality; 0 otherwise violence = 1 if reason for challenge was violent material; 0 otherwise The primary response variable is removed. Certain potential predictors are measured at the state level (e.g., pvi2 and cperhs), at least one is at the book level (freqchal), and several are specific to an individual challenge (e.g., obama and sexexp). In addition, note that the same book can be challenged for more than one reason, in different states, or even at different times in the same state. Perform exploratory analyses and then run multilevel models to examine significant determinants of successful challenges. Write a short report comparing specific reasons for the challenge to the greater context in which a challenge was made. "]
]
