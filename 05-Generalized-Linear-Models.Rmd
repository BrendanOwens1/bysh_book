---
title: "Chapter 5"
subtitle: "Generalized Linear Models (GLMs): A Unifying Theory"
output:
  pdf_document:
    number_sections: yes
  html_document: default
---
#Generalized Linear Models (GLMs): A Unifying Theory {#ch-glms}
##Learning Objectives

- Determine if a probability distribution can be expressed in one-parameter exponential family form.
- Identify canonical links for distributions of one parameter exponential family form.

##One parameter exponential families

Thus far, we have expanded our repertoire of models from OLS to include Poisson regression. But in the early 1970s Nelder and Wedderburn identified a broader class of models that generalizes the  multiple linear regression we considered in the introductory chapter and are referred to as __generalized linear models (GLMs)__ [@Nelder1972]. GLMs have similar forms for the likelihoods, MLEs, and variance. This makes it easier to find model estimates and their corresponding uncertainty. To determine whether a model is a GLM, we consider the following properties.
When a probability formula can be written in the form below
\begin{equation}
f(y;\theta)=exp^{[a(y)b(\theta)+c(\theta)+d(y)]}
(\#eq:1expForm)
\end{equation}
and if the __support__ (the set of possible input values) does not depend upon $\theta$, it is said to have a __one-parameter exponential family form__. We demonstrate that the Poisson distribution is a member of the one parameter exponential family by writing its probability mass function (pmf) in the form of Equation \@ref(eq:1expForm) and assessing its support.

###One Parameter Exponential Family: Possion

Recall we begin with
\[
P(Y=y)=\frac{e^{-\lambda}{\lambda}^y}{y!} \textrm{  where  } y=0,1,2\ldots\infty
\]
and consider the following useful identities for establish exponential form:
  \[a=e^{log(a)} \]
  \[a^x = e^{xlog(a)}\]
    \[log(ab)=log(a)+log(b)\]
  \[log\frac{a}{b}=log(a)-log(b)\]
  
Determining whether the Poisson model is a member of the one-parameter exponential family is a matter of writing the Poisson pmf in the form of Equation \@ref(eq:1expForm) and checking that the support does not depend upon $\lambda$. First, consider the condition concerning the support of the distribution. The set of possible values for any Poisson random variable is $y=0,1,2\ldots\infty$ which does not depend on $\lambda$. The support condition is met. Now we see if we can rewrite the probability mass function in one-parameter exponential family form.
 \begin{eqnarray}
 P(Y=y)={e^{-\lambda}e^{ylog \lambda}e^{-log (y!)}} \nonumber \\
 =e^{ylog \lambda-\lambda-log (y!)}
 (\#eq:opeff)
 \end{eqnarray}
The first term in the exponent for Equation \@ref(eq:1expForm) must be the product of two factors, one solely a function of y, $a(y)$ and another, $b(\lambda)$ a function of $\lambda$ only.  The middle term in the exponent must be a function of $\lambda$ only; no $y's$ should appear.  The last term has only $y's$ and no $\lambda$. Since this appears to be the case here, we can identify the different functions in this form:
\begin{eqnarray}
a(y)&=&y \\
b(\lambda)&=&log(\lambda) \\
c(\lambda)&=&-\lambda \nonumber \\
d(y)&=&-log (y!) \\
(\#eq:diffunc)
\end{eqnarray}
These functions have useful interpretations in statistical theory. We won't be going in to this in detail, but we will note that function $b(\lambda)$ or more generally $b(\theta)$ will be particularly helpful in GLMs. The function $b(\theta)$ is referred to as the __canonical link__. The canonical link is often a good choice to model as a linear function of the explanatory variables. That is, Poisson regression should be set up as $log(\lambda)=\beta_0+\beta_1x_1+\beta_2x_2+...$.  In fact, there is a distinct advantage modeling the canonical link as opposed to other functions of $\theta$, but it is also worth noting that other choices are possible, and at times preferred, depending upon the context of the application.  

There are other benefits of identifying a response as being from a one parameter exponential family. For example, by creating an unifying theory for regression modeling, Nelder and Wedderburn made possible a common and efficient method for finding estimates of model parameters using iteratively reweighted least squares (IWLS).  In addition, we can use the one parameter exponential family form to determine the expected value and standard deviation of $Y$. With statistical theory you can show that
\[E(Y) =-\frac{c'(\theta)}{b'(\theta)}\ \textrm{ and } Var(Y) =\frac{b''(\theta)c'(\theta)-c''(\theta)b'(\theta)}{[b'(\theta)]^3}
\]
where differentiation is with respect to $\theta$.  Verifying these results for the Poisson response:
\begin{equation*}
E(Y)=-\frac{-1}{\frac{1}{\lambda}}=\lambda \textrm{ and }  Var(Y)=\frac{1/{{\lambda}^2}}
{(1/{\lambda}^3)}=\lambda
\end{equation*}

We'll find that other distributions are members of the one parameter exponential family by writing their pdf or pmf in this manner and verifying the support condition. So for example, we'll see that the binomial distribution meets these conditions, so it is also a member of the one parameter exponential family. The normal distribution is a special case. We note that we have two parameters, a mean and standard deviation. If we assume, however, that one of the parameters is known, then we can show that a normal random variable is from a one parameter exponential family.  

###One parameter exponential family: Normal

Here we determine whether a normal distribution is a one parameter exponential family member. First we will need to assume that $\sigma$ is known. Next, possible values for a normal random variable range from $-\infty$ to $\infty$, so we have no problems with the support. Finally, we'll need to write the probability density function (pdf) in the one parameter exponential family form. We start with the familiar form:
\[
f(y)=\frac{1}{{\sigma \sqrt{2\pi}}}{e^{-\frac{(y-\mu)^2}{2\sigma^2}}}
\]
Even writing ${1/{\sigma \sqrt{2\pi}}}$ as $e^{-log{\sigma}-log(2\pi)/2}$ we still do not have the pdf written in one parameter exponential family form. We will first need to expand the exponent so that we have
\[
f(y)=e^{-log{\sigma}-log(2\pi)/2}{e^{-\frac{y^2-2y\mu +\mu^2}{2\sigma^2}}}
\]

Assuming $\sigma$ is known, we have
\[
f(y) \propto e^{-(-2y\mu + \mu^2 + y^2 )/2\sigma^2}
\]
From this result, we can see that the canonical link for a normal response is $\mu$ which is consistent with what we've been doing with OLS, since the simple linear regression model has the form:
\[ \mu_{Y|X} = \beta_0 + \beta_1X. \]

##Generalized Linear Modeling
GLM theory suggests that the canonical link can be modeled as a linear combination of the explanatory variable(s). This approach unifies a number of modeling results used throughout the text. One example is that likelihoods will be used to compare models in the same way for parameter exponential family members.

We have now __generalized__ our modeling to handle non-normal responses. For example, in addition to normally distributed responses, we are able to handle Poisson responses, binomial responses, and more. Writing a pmf or pdf for a response in one parameter exponential family form reveals the canonical link which can be modeled as a linear function of the predictors. This linear function of the predictors is the last piece of the puzzle for performing generalized linear modeling. But, in fact, it is really nothing new. We already use linear combinations when modeling normally distributed data.

<p align="center">  __Three Components of a GLM__ </p>
> 1. Distribution of Y (e.g., Poisson)
> 2. Link Function (a function of the parameter, e.g., log$(\lambda)$ for Poisson)
> 3. Linear Predictor (choice of predictors, e.g., $\beta_0 + \beta_1 x_1 + \beta_2 x_2 + ...$)

```{r, load_packages5, include=FALSE}
library(knitr) #for kable
```

```{r, table1chp5, echo=FALSE}
#{tab:1exp}
supersc <- "--2y\u03bc+ \u03bc^2^ + y^2^"
Dist <- c("Binary","Binomial","Poisson","Normal","Exponential","Gamma","Geometric")
Opeff <- c(" "," ","P(Y=y) = e^ylog&#955;-&#955;-y!^","f(y) &#8733; e^(-2y\u03bc+ \u03bc^2^ + y^2^)"," "," "," ")
canlink <- c(" ","logit(p)","log(&#955;)","\u03bc"," "," "," ")

table1chp5 <- data.frame(Dist, Opeff, canlink)
colnames(table1chp5) <- c("Distribution","One-parameter Exponential Family Form","Canonical Link")
kable(table1chp5, booktabs=T,caption="One parameter exponential family form and canonical links.")
```

Completing Table \@ref(tab:table1chp5) is left as an exercise.  

In the chapter on Poisson modeling, we provided heuristic rationale for using the log() function as our link. That is, counts would be non-negative but a linear function inevitably goes negative. By taking the logarithm of our parameter $\lambda$ we could use a linear predictor and not worry that it can take on negative values. Now we have theoretical justification for this choice, as the log is the canonical link for poisson data. In the next chapter we encounter yet another type of response, a binary response, which calls for a different link function. Our work here suggests that we will model logit(p)=$log(\frac{p}{1-p})$ using a linear predictor.

[Note that __generalized linear models (GLMs)__ differs from __General Linear Models__.  The *general* linear model is a statistical linear model with multivariate vectors as responses. For example, each subject in a study may have their height, weight, and shoe size recorded and modeled as a function of age and sex. The response is a vector, Y = (height, weight, shoe size), for each study participant. Age and sex are explanatory variables in the model. The residual is usually assumed to follow a multivariate normal distribution. If the residual is not a multivariate normal distribution, then generalized linear models may be used to relax assumptions about Y and the variance-covariance structure.]

##Exercises

1. For each distribution,

- Write the pdf in one parameter exponential form, if possible.
- Describe an example of a setting where this random variable might be used.
- Identify the canonical link function, and
- Compute $\mu = -\frac{c'(\theta)}{b'(\theta)}$ and $\sigma^2 = \frac{b''(\theta)c'(\theta)-c''(\theta)b'(\theta)}{[b'(\theta)]^3}$ and compare with known E(Y) and Var(Y).

a) Binary: Y = 1 for a success, 0 for a failure

  \[p(y)=p^{y}(1-p)^{(1-y)}
  \]
  
b) Binomial: Y = number of successes in n independent, identical trials

  \[p(y)=\left(\begin{array} {c}  n\\y  \end{array}\right) p^y(1-p)^{(n-y)}
  \]
  
c) Poisson: Y = number of events occurring in a given time (or space) when the average event rate is $\lambda$ per unit of time (or space)

  \[
  P(Y=y)=\frac{e^{-\lambda}\lambda^y}{y!}
  \]
  
d) Normal (with fixed $\sigma$)

  \[f(y; \mu)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(y-\mu)^2}{2\sigma^2}}\]

e) Exponential: Y = time spent waiting for the first event in a Poisson process with an average rate of $\lambda$ events per unit of time

  \[f(y)=\lambda e^{-\lambda y}\]

f) Gamma (for fixed $r$): Y = time spent waiting for the $r^{th}$ event in a Poisson process with an average rate of $\lambda$ events per unit of time

  \[f(y; \lambda)=\frac{\lambda^r y^{(r-1)}e^{-\lambda y}}{\Gamma(r)}\]

g) Geometric: Y = number of failures before the first success in a Bernoulli process

  \[p(y)=(1-p)^{y}p\]

h) Negative Binomial (for fixed $r$): Y = number of failures prior to the $r^{th}$ success in a Bernoulli process

\begin{eqnarray}
p(y; r) & = & \left(\begin{array} {c}  y+r-1\\r-1  \end{array}\right)(1-p)^{y}p^r \nonumber \\
 & = & \frac{\Gamma(y+r)}{\Gamma(r)y!} (1-p)^{y}p^r \textrm{ for non-integer } r \\
\end{eqnarray}

i) Pareto (for fixed $k$): 

  \[f(y; \theta)=\frac{\theta k^\theta}{y^{(\theta+1)}} \textrm{ for } y\geq k; \theta \geq 1\]

2. Complete Table \@ref(tab:table1chp5) containing your results of the preceding exercises.
